<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[错误整理]]></title>
    <url>%2F2018%2F05%2F25%2Finstall%20and%20config%2F%E9%94%99%E8%AF%AF%E6%95%B4%E7%90%86%2F</url>
    <content type="text"><![CDATA[1 解决python安装第三方包时候，报错 error: Unable to find vcvarsall.bat对应python版本安装下面程序：https://blogs.msdn.microsoft.com/pythonengineering/2016/04/11/unable-to-find-vcvarsall-bat/#comments]]></content>
      <categories>
        <category>install and config</category>
      </categories>
      <tags>
        <tag>error</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[实现多版本Python共存]]></title>
    <url>%2F2018%2F05%2F25%2Finstall%20and%20config%2F%E5%AE%9E%E7%8E%B0%E5%A4%9A%E7%89%88%E6%9C%ACPython%E5%85%B1%E5%AD%98%2F</url>
    <content type="text"><![CDATA[1 显示所有环境名字和路径1conda info --envs1 2 创建一个名为python3.5（名字随意，记住就好）的新环境：1conda create -n python3.5 python=3.5.2 3 运行Anaconda Navigator，选择新环境python3.5，安装jupyter notebook 4 使用环境和退出环境分别如下：123activate python3.5deactivate python3.5 用 activate python3.5 使用环境后就可以在新的环境里安装各种包，比如安装tensorflow-gpu 1pip install tensorflow-gpu 【注】如果只想在某个环境的Python下安装包，进入该环境对应的pip的Scripts目录使用pip install xxx即可 5 另附其他操作：克隆，把tensorflow克隆，新建一个my_th环境：1conda create -n my_th --clone tensorflow1 删除环境my_th1conda remove -n my_th --all]]></content>
      <categories>
        <category>install and config</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>anaconda navigator</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python屏蔽警告方法]]></title>
    <url>%2F2018%2F05%2F01%2Fpython%2FPython%E5%B1%8F%E8%94%BD%E8%AD%A6%E5%91%8A%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[只需要在相应的.py文件头这样写：123import osos.environ[&apos;TF_CPP_MIN_LOG_LEVEL&apos;]=&apos;2&apos;import tensorflow as tf 然后就没有问题啦～ 12345678910import osos.environ['TF_CPP_MIN_LOG_LEVEL']='1' # 这是默认的显示等级，显示所有信息# 2级import osos.environ['TF_CPP_MIN_LOG_LEVEL']='2' # 只显示 warning 和 Error# 3级import osos.environ['TF_CPP_MIN_LOG_LEVEL']='3' # 只显示 Error]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python技巧</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python专题：pickle序列化]]></title>
    <url>%2F2018%2F05%2F01%2Fpython%2FPython%E4%B8%93%E9%A2%98%EF%BC%9Apickle%E5%BA%8F%E5%88%97%E5%8C%96%2F</url>
    <content type="text"><![CDATA[pickle提供了一个简单的持久化功能。可以将对象以文件的形式存放在磁盘上。 pickle.dump(obj, file[, protocol]) 序列化对象，并将结果数据流写入到文件对象中。参数protocol是序列化模式，默认值为0，表示以文本的形式序列化。protocol的值还可以是1或2，表示以二进制的形式序列化。 pickle.load(file) 反序列化对象。将文件中的数据解析为一个Python对象。 其中要注意的是，在load(file)的时候，要让python能够找到类的定义，否则会报错： 比如下面的例子1234567891011121314151617import pickleclass Person: def __init__(self,n,a): self.name=n self.age=a def show(self): print self.name+"_"+str(self.age)aa = Person("JGood", 2)aa.show()f=open('d:\\p.txt','w')pickle.dump(aa,f,0)f.close()#del Personf=open('d:\\p.txt','r')bb=pickle.load(f)f.close()bb.show() 如果不注释掉del Person的话，那么会报错如下： 意思就是当前模块找不到类的定义了。 clear_memo() 清空pickler的“备忘”。使用Pickler实例在序列化对象的时候，它会“记住”已经被序列化的对象引用，所以对同一对象多次调用dump(obj)，pickler不会“傻傻”的去多次序列化。看下面的例子： 1234567891011121314151617181920import StringIO import pickle class Person: def __init__(self,n,a): self.name=n self.age=a def show(self): print self.name+"_"+str(self.age) aa = Person("JGood", 2) aa.show() fle = StringIO.StringIO() pick = pickle.Pickler(fle) pick.dump(aa) val1=fle.getvalue() print len(val1) pick.clear_memo() pick.dump(aa) val2=fle.getvalue() print len(val2) fle.close() 上面的代码运行如下： 如果不注释掉，则运行结果是第一个。如果注释掉，那么运行结果是第二个。 主要是因为，python的pickle如果不clear_memo，则不会多次去序列化对象。]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python技巧</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[欢迎光临，BLOG刚弄不久正在施工 文章陆续上传中...]]></title>
    <url>%2F2018%2F04%2F29%2FOTHERS%2F%E7%BD%AE%E9%A1%B6%E4%BF%A1%E6%81%AF%2F</url>
    <content type="text"></content>
      <categories>
        <category>其他</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[4线性方程求解]]></title>
    <url>%2F2018%2F04%2F29%2Fmath%2Flinear_algebra%2F4%E7%BA%BF%E6%80%A7%E6%96%B9%E7%A8%8B%E6%B1%82%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[线性方程求解方法Gauss消去法1转化为（同解）的三角形方程组2化阶梯形矩阵 且 要保证解不变，所以需要组成增广阵再进行 初等行变换（同解变换）实际过程就是一行一行消元，用上面行消去下面行第一个项，有利于解出最后一个回代 举例：适用性：系数矩阵A规模比较小的，否则很慢系数矩阵A是非奇异的，否则没有唯一解 Jacobi迭代法 雅克比迭代法矩阵描述 矩阵迭代公式DX = (L+U)X + bL和U都没更新，写在右边 Gauss-Seildel迭代法及时更新下半三角系数的迭代 矩阵迭代公式一(D-L)X = UX + b只有上半部的U没更新，写在右边 矩阵迭代公式二这样D的逆更方便求出来 迭代法的收敛严格对角占优矩阵 定义：对角线元素的绝对值和 &gt; 其所在 行/列 元素的绝对值和 的矩阵]]></content>
      <categories>
        <category>math</category>
        <category>linear_algebra</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Tensorflow学习10-2：验证码识别——训练和测试]]></title>
    <url>%2F2018%2F04%2F26%2Ftensorflow%2FTensorflow%E5%AD%A6%E4%B9%A010-2%EF%BC%9A%E9%AA%8C%E8%AF%81%E7%A0%81%E8%AF%86%E5%88%AB%E2%80%94%E2%80%94%E8%AE%AD%E7%BB%83%E5%92%8C%E6%B5%8B%E8%AF%95%2F</url>
    <content type="text"><![CDATA[简介如题，本篇介绍的是tensorflow实现验证码的识别，之前我们已经生成了数据集，并且转换成了tfrecord格式的文件，现在我们开始利用这个文件来进行训练及识别。 补充一点，我们可以有两种方法进行验证码识别，其一，把标签转为向量，向量长度为40，比如一个验证码为0782，它的标签可以转为长度为40的向量 1000000000 0000000100 0000000010 0010000000，接下来，训练方法和手写数字识别类似。其二，使用的是多任务的学习方法，拆分为4个标签 1 多任务学习 采用multi-task learning 多任务学习。以验证码识别为例： 多任务学习是一种联合学习，多个任务并行学习，结果相互影响。所谓多任务学习，就是同时求解多个问题。个性化问题就是一种典型的多任务学习问题，它同时学习多个用户的兴趣偏好。 多任务学习有交替训练和联合训练。由于数据集相同，我们采用的是多任务学习中的联合训练。 1）准备工作言归正传，我们下面用代码实现这个多任务学习。上篇已经按照之前的步骤生成好了tfrecord文件，我们使用alexnet_v2模型来完成。注意需要修改alexnet代码，该代码位于slim/nets文件夹下： 我们将nets拷贝到当前工程目录下，重命名为nets_multi，并修改alexnet.py代码，将最后一层分为4个输出（4个学习任务）。修改后其完整代码如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132from __future__ import absolute_importfrom __future__ import divisionfrom __future__ import print_functionimport tensorflow as tfslim = tf.contrib.slimtrunc_normal = lambda stddev: tf.truncated_normal_initializer(0.0, stddev)def alexnet_v2_arg_scope(weight_decay=0.0005): with slim.arg_scope([slim.conv2d, slim.fully_connected], activation_fn=tf.nn.relu, biases_initializer=tf.constant_initializer(0.1), weights_regularizer=slim.l2_regularizer(weight_decay)): with slim.arg_scope([slim.conv2d], padding='SAME'): with slim.arg_scope([slim.max_pool2d], padding='VALID') as arg_sc: return arg_scdef alexnet_v2(inputs, num_classes=1000, is_training=True, dropout_keep_prob=0.5, spatial_squeeze=True, scope='alexnet_v2', global_pool=False): """AlexNet version 2. Described in: http://arxiv.org/pdf/1404.5997v2.pdf Parameters from: github.com/akrizhevsky/cuda-convnet2/blob/master/layers/ layers-imagenet-1gpu.cfg Note: All the fully_connected layers have been transformed to conv2d layers. To use in classification mode, resize input to 224x224 or set global_pool=True. To use in fully convolutional mode, set spatial_squeeze to false. The LRN layers have been removed and change the initializers from random_normal_initializer to xavier_initializer. Args: inputs: a tensor of size [batch_size, height, width, channels]. num_classes: the number of predicted classes. If 0 or None, the logits layer is omitted and the input features to the logits layer are returned instead. is_training: whether or not the model is being trained. dropout_keep_prob: the probability that activations are kept in the dropout layers during training. spatial_squeeze: whether or not should squeeze the spatial dimensions of the logits. Useful to remove unnecessary dimensions for classification. scope: Optional scope for the variables. global_pool: Optional boolean flag. If True, the input to the classification layer is avgpooled to size 1x1, for any input size. (This is not part of the original AlexNet.) Returns: net: the output of the logits layer (if num_classes is a non-zero integer), or the non-dropped-out input to the logits layer (if num_classes is 0 or None). end_points: a dict of tensors with intermediate activations. """ with tf.variable_scope(scope, 'alexnet_v2', [inputs]) as sc: end_points_collection = sc.name + '_end_points' # Collect outputs for conv2d, fully_connected and max_pool2d. with slim.arg_scope([slim.conv2d, slim.fully_connected, slim.max_pool2d], outputs_collections=[end_points_collection]): net = slim.conv2d(inputs, 64, [11, 11], 4, padding='VALID', scope='conv1') net = slim.max_pool2d(net, [3, 3], 2, scope='pool1') net = slim.conv2d(net, 192, [5, 5], scope='conv2') net = slim.max_pool2d(net, [3, 3], 2, scope='pool2') net = slim.conv2d(net, 384, [3, 3], scope='conv3') net = slim.conv2d(net, 384, [3, 3], scope='conv4') net = slim.conv2d(net, 256, [3, 3], scope='conv5') net = slim.max_pool2d(net, [3, 3], 2, scope='pool5') # Use conv2d instead of fully_connected layers. with slim.arg_scope([slim.conv2d], weights_initializer=trunc_normal(0.005), biases_initializer=tf.constant_initializer(0.1)): net = slim.conv2d(net, 4096, [5, 5], padding='VALID', scope='fc6') net = slim.dropout(net, dropout_keep_prob, is_training=is_training, scope='dropout6') net = slim.conv2d(net, 4096, [1, 1], scope='fc7') net = slim.dropout(net, dropout_keep_prob, is_training=is_training, scope='dropout7') #分成4个输出 net0 = slim.conv2d(net, num_classes, [1, 1], activation_fn=None, normalizer_fn=None, biases_initializer=tf.zeros_initializer(), scope='fc8_0') net1 = slim.conv2d(net, num_classes, [1, 1], activation_fn=None, normalizer_fn=None, biases_initializer=tf.zeros_initializer(), scope='fc8_1') net2 = slim.conv2d(net, num_classes, [1, 1], activation_fn=None, normalizer_fn=None, biases_initializer=tf.zeros_initializer(), scope='fc8_2') net3 = slim.conv2d(net, num_classes, [1, 1], activation_fn=None, normalizer_fn=None, biases_initializer=tf.zeros_initializer(), scope='fc8_3') # Convert end_points_collection into a end_point dict. end_points = slim.utils.convert_collection_to_dict(end_points_collection) # if global_pool: # net = tf.reduce_mean(net, [1, 2], keep_dims=True, name='global_pool') # end_points['global_pool'] = net #if num_classes: #net = slim.conv2d(net, num_classes, [1, 1], #activation_fn=None, #normalizer_fn=None, #biases_initializer=tf.zeros_initializer(), #scope='fc8') if spatial_squeeze: net0 = tf.squeeze(net0, [1, 2], name='fc8_0/squeezed') end_points[sc.name + '/fc8_0'] = net0 net1 = tf.squeeze(net1, [1, 2], name='fc8_1/squeezed') end_points[sc.name + '/fc8_1'] = net1 net2 = tf.squeeze(net2, [1, 2], name='fc8_2/squeezed') end_points[sc.name + '/fc8_2'] = net2 net3 = tf.squeeze(net3, [1, 2], name='fc8_3/squeezed') end_points[sc.name + '/fc8_3'] = net3 return net0, net1, net2, net3, end_pointsalexnet_v2.default_image_size = 224 修改nets_factory.py文件，增加一条代码：1from nets import alexnet_multi 2）训练训练代码如下： 1234567891011121314151617import osimport tensorflow as tfimport numpy as npfrom PIL import Imagefrom nets import nets_factory#不同字符数量CHAR_SET_LEN = 10#图片高度IMAGE_HEIGHT = 60#图片宽度IMAGE_WIDTH = 160#批次BATCH_SIZE = 25MOD_DIR = "D:/Tensorflow/captcha/model/"#tfrecord存放路径TFRECORD_FILE = "D:/Tensorflow/captcha/train.tfrecord" 12345678910111213141516171819202122232425262728293031323334353637383940#placeholderx = tf.placeholder(tf.float32, [None, 224, 224])y0 = tf.placeholder(tf.float32, [None])y1 = tf.placeholder(tf.float32, [None])y2 = tf.placeholder(tf.float32, [None])y3 = tf.placeholder(tf.float32, [None])#学习率lr = tf.Variable(0.003, dtype=tf.float32)#读取tfrecorddef read_and_decode(filename): #根据文件名生成一个队列 filename_queue = tf.train.string_input_producer([filename]) reader = tf.TFRecordReader() #返回文件名和文件 _, serialized_example = reader.read(filename_queue) features = tf.parse_single_example(serialized_example, features= &#123; "image" : tf.FixedLenFeature([], tf.string), "label0": tf.FixedLenFeature([], tf.int64), "label1": tf.FixedLenFeature([], tf.int64), "label2": tf.FixedLenFeature([], tf.int64), "label3": tf.FixedLenFeature([], tf.int64), &#125;) #获取图片数据 image = tf.decode_raw(features["image"], tf.uint8) #tf.train.shuffle_batch必须确定shape image = tf.reshape(image, [224,224]) #图片预处理 image = tf.cast(image, tf.float32) / 255.0 image = tf.subtract(image, 0.5) image = tf.multiply(image, 2.0) #获取Label label0 = tf.cast(features["label0"], tf.int32) label1 = tf.cast(features["label1"], tf.int32) label2 = tf.cast(features["label2"], tf.int32) label3 = tf.cast(features["label3"], tf.int32) return image, label0, label1, label2, label3 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697#获取图片数据和标签image, label0, label1, label2, label3 = read_and_decode(TFRECORD_FILE)#使用shuffle_batch可以随机打乱 next_batch挨着往下取# shuffle_batch才能实现[img,label]的同步,也即特征和label的同步,不然可能输入的特征和label不匹配# 比如只有这样使用,才能使img和label一一对应,每次提取一个image和对应的label# shuffle_batch返回的值就是RandomShuffleQueue.dequeue_many()的结果# Shuffle_batch构建了一个RandomShuffleQueue，并不断地把单个的[img,label],送入队列中image_batch, label_batch0, label_batch1,label_batch2,label_batch3 = tf.train.shuffle_batch( [image, label0, label1, label2, label3], batch_size = BATCH_SIZE, capacity = 5000, min_after_dequeue=1000, num_threads=1)#定义网络结构train_network_fn = nets_factory.get_network_fn( "alexnet_v2", num_classes=CHAR_SET_LEN, weight_decay=0.0005, is_training=True)with tf.Session() as sess: #inputs: a tensor of size [batch_size, height, width, channels] X = tf.reshape(x, [BATCH_SIZE, 224, 224, 1]) #数据输入网络得到输出值 logits0,logits1,logits2,logits3,end_points = train_network_fn(X) #把标签转成one_hot形式 one_hot_labels0 = tf.one_hot(indices=tf.cast(y0, tf.int32), depth=CHAR_SET_LEN) one_hot_labels1 = tf.one_hot(indices=tf.cast(y1, tf.int32), depth=CHAR_SET_LEN) one_hot_labels2 = tf.one_hot(indices=tf.cast(y2, tf.int32), depth=CHAR_SET_LEN) one_hot_labels3 = tf.one_hot(indices=tf.cast(y3, tf.int32), depth=CHAR_SET_LEN) #计算loss loss0 = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits0, labels=one_hot_labels0)) loss1 = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits1, labels=one_hot_labels1)) loss2 = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits2, labels=one_hot_labels2)) loss3 = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits3, labels=one_hot_labels3)) #计算总loss total_loss = (loss0+loss1+loss2+loss3) / 4.0 #优化器 optimizer = tf.train.AdamOptimizer(learning_rate=lr).minimize(total_loss) #计算准确率 correct_prediction0 = tf.equal(tf.argmax(one_hot_labels0,1), tf.argmax(logits0,1)) accuracy0 = tf.reduce_mean(tf.cast(correct_prediction0, tf.float32)) correct_prediction1 = tf.equal(tf.argmax(one_hot_labels1,1), tf.argmax(logits1,1)) accuracy1 = tf.reduce_mean(tf.cast(correct_prediction1, tf.float32)) correct_prediction2 = tf.equal(tf.argmax(one_hot_labels2,1), tf.argmax(logits2,1)) accuracy2 = tf.reduce_mean(tf.cast(correct_prediction2, tf.float32)) correct_prediction3 = tf.equal(tf.argmax(one_hot_labels3,1), tf.argmax(logits3,1)) accuracy3 = tf.reduce_mean(tf.cast(correct_prediction3, tf.float32)) #用于保存模型 saver = tf.train.Saver() sess.run(tf.global_variables_initializer()) #创建一个协调器，管理线程 coord = tf.train.Coordinator() #启动Queue Runners，此时文件名队列已经进队 threads = tf.train.start_queue_runners(sess=sess, coord=coord) for i in range(3001): #获取一个批次是数据和标签 b_image,b_label0,b_label1,b_label2,b_label3 = sess.run([image_batch, label_batch0, label_batch1,label_batch2,label_batch3]) #优化模型 sess.run(optimizer, feed_dict=&#123;x:b_image, y0:b_label0, y1:b_label1, y2:b_label2, y3:b_label3&#125;) #每迭代20次计算一下loss 和 accuracy if i%20 == 0: #每迭代1000次降低一下学习率 if i%1000 == 0: sess.run(tf.assign(lr, lr/3)) # print("y0:",b_label0, "y1:",b_label1, "y2:",b_label2, "y3:",b_label3)# _logits0,_logits1,_logits2,_logits3 = sess.run([logits0,logits1,logits2,logits3], feed_dict=&#123;x:b_image&#125;)# print("logits0:",_logits0, "logits1:",_logits1,"logits2:",_logits2,"logits3:",_logits3) acc0,acc1,acc2,acc3,loss_ = sess.run([accuracy0,accuracy1,accuracy2,accuracy3,total_loss], feed_dict=&#123; x:b_image, y0:b_label0, y1:b_label1, y2:b_label2, y3:b_label3 &#125;) learning_rate = sess.run(lr) print("Iter:%d Loss:%.3f Accuracy:%.2f,%.2f,%.2f,%.2f Learning Rate:%.4f" % (i,loss_,acc0,acc1,acc2,acc3, learning_rate)) #满足设置条件，就停止训练保存模型 if i==3000: saver.save(sess, MOD_DIR + "captcha.model", global_step=i) #global_step——保存后缀为3000 break #通知其他线程关闭 coord.request_stop() #其他所有线程关闭后，这个函数才能返回 coord.join(threads) WARNING:tensorflow:From :34: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.Instructions for updating: Future major versions of TensorFlow will allow gradients to flowinto the labels input on backprop by default. See @{tf.nn.softmax_cross_entropy_with_logits_v2}. Iter:0 Loss:1566.136 Accuracy:0.32,0.12,0.16,0.20 Learning Rate:0.0010Iter:20 Loss:2.314 Accuracy:0.12,0.08,0.12,0.08 Learning Rate:0.0010Iter:40 Loss:2.297 Accuracy:0.24,0.04,0.12,0.04 Learning Rate:0.0010Iter:60 Loss:2.296 Accuracy:0.20,0.04,0.04,0.08 Learning Rate:0.0010Iter:80 Loss:2.310 Accuracy:0.04,0.04,0.04,0.20 Learning Rate:0.0010Iter:100 Loss:2.322 Accuracy:0.00,0.04,0.08,0.04 Learning Rate:0.0010Iter:120 Loss:2.277 Accuracy:0.08,0.32,0.08,0.16 Learning Rate:0.0010Iter:140 Loss:2.328 Accuracy:0.12,0.08,0.04,0.08 Learning Rate:0.0010Iter:160 Loss:2.294 Accuracy:0.08,0.16,0.12,0.08 Learning Rate:0.0010Iter:180 Loss:2.295 Accuracy:0.04,0.08,0.24,0.20 Learning Rate:0.0010Iter:200 Loss:2.314 Accuracy:0.16,0.04,0.04,0.04 Learning Rate:0.0010Iter:220 Loss:2.299 Accuracy:0.08,0.16,0.12,0.04 Learning Rate:0.0010Iter:240 Loss:2.310 Accuracy:0.04,0.00,0.12,0.12 Learning Rate:0.0010Iter:260 Loss:2.315 Accuracy:0.00,0.16,0.12,0.16 Learning Rate:0.0010Iter:280 Loss:2.305 Accuracy:0.12,0.28,0.08,0.04 Learning Rate:0.0010Iter:300 Loss:2.299 Accuracy:0.04,0.08,0.08,0.16 Learning Rate:0.0010Iter:320 Loss:2.293 Accuracy:0.12,0.08,0.16,0.20 Learning Rate:0.0010Iter:340 Loss:2.265 Accuracy:0.12,0.28,0.12,0.24 Learning Rate:0.0010Iter:360 Loss:2.307 Accuracy:0.16,0.16,0.08,0.12 Learning Rate:0.0010Iter:380 Loss:2.305 Accuracy:0.16,0.12,0.04,0.08 Learning Rate:0.0010Iter:400 Loss:2.312 Accuracy:0.16,0.20,0.00,0.04 Learning Rate:0.0010Iter:420 Loss:2.302 Accuracy:0.16,0.00,0.12,0.08 Learning Rate:0.0010Iter:440 Loss:2.278 Accuracy:0.08,0.24,0.36,0.08 Learning Rate:0.0010Iter:460 Loss:2.290 Accuracy:0.04,0.12,0.08,0.12 Learning Rate:0.0010Iter:480 Loss:2.294 Accuracy:0.20,0.16,0.08,0.12 Learning Rate:0.0010Iter:500 Loss:2.319 Accuracy:0.08,0.12,0.00,0.12 Learning Rate:0.0010Iter:520 Loss:2.294 Accuracy:0.12,0.04,0.20,0.20 Learning Rate:0.0010Iter:540 Loss:2.297 Accuracy:0.16,0.12,0.08,0.00 Learning Rate:0.0010Iter:560 Loss:2.309 Accuracy:0.08,0.08,0.04,0.04 Learning Rate:0.0010Iter:580 Loss:2.294 Accuracy:0.12,0.08,0.24,0.08 Learning Rate:0.0010Iter:600 Loss:2.284 Accuracy:0.16,0.08,0.08,0.24 Learning Rate:0.0010Iter:620 Loss:2.281 Accuracy:0.08,0.00,0.28,0.32 Learning Rate:0.0010Iter:640 Loss:2.318 Accuracy:0.04,0.16,0.08,0.00 Learning Rate:0.0010Iter:660 Loss:2.291 Accuracy:0.08,0.20,0.20,0.12 Learning Rate:0.0010Iter:680 Loss:2.311 Accuracy:0.04,0.04,0.00,0.12 Learning Rate:0.0010Iter:700 Loss:2.220 Accuracy:0.20,0.04,0.08,0.20 Learning Rate:0.0010Iter:720 Loss:2.196 Accuracy:0.48,0.16,0.00,0.24 Learning Rate:0.0010Iter:740 Loss:2.215 Accuracy:0.20,0.08,0.04,0.20 Learning Rate:0.0010Iter:760 Loss:2.084 Accuracy:0.36,0.12,0.08,0.12 Learning Rate:0.0010Iter:780 Loss:2.087 Accuracy:0.36,0.12,0.00,0.04 Learning Rate:0.0010Iter:800 Loss:2.121 Accuracy:0.36,0.08,0.08,0.08 Learning Rate:0.0010Iter:820 Loss:1.991 Accuracy:0.48,0.20,0.16,0.12 Learning Rate:0.0010Iter:840 Loss:1.926 Accuracy:0.60,0.12,0.32,0.16 Learning Rate:0.0010Iter:860 Loss:1.868 Accuracy:0.52,0.24,0.12,0.24 Learning Rate:0.0010Iter:880 Loss:1.876 Accuracy:0.48,0.12,0.16,0.20 Learning Rate:0.0010Iter:900 Loss:1.693 Accuracy:0.64,0.24,0.28,0.40 Learning Rate:0.0010Iter:920 Loss:1.768 Accuracy:0.72,0.28,0.20,0.24 Learning Rate:0.0010Iter:940 Loss:1.582 Accuracy:0.64,0.32,0.36,0.48 Learning Rate:0.0010Iter:960 Loss:1.673 Accuracy:0.60,0.24,0.24,0.32 Learning Rate:0.0010Iter:980 Loss:1.530 Accuracy:0.84,0.28,0.28,0.36 Learning Rate:0.0010Iter:1000 Loss:1.550 Accuracy:0.68,0.28,0.40,0.40 Learning Rate:0.0003Iter:1020 Loss:1.446 Accuracy:0.56,0.20,0.48,0.36 Learning Rate:0.0003Iter:1040 Loss:1.445 Accuracy:0.68,0.44,0.20,0.52 Learning Rate:0.0003Iter:1060 Loss:1.425 Accuracy:0.80,0.48,0.24,0.60 Learning Rate:0.0003Iter:1080 Loss:1.273 Accuracy:0.80,0.56,0.40,0.56 Learning Rate:0.0003Iter:1100 Loss:1.171 Accuracy:0.76,0.44,0.36,0.68 Learning Rate:0.0003Iter:1120 Loss:1.080 Accuracy:0.84,0.44,0.52,0.56 Learning Rate:0.0003Iter:1140 Loss:1.242 Accuracy:0.88,0.40,0.56,0.32 Learning Rate:0.0003Iter:1160 Loss:1.071 Accuracy:0.88,0.60,0.52,0.52 Learning Rate:0.0003Iter:1180 Loss:1.176 Accuracy:0.80,0.44,0.56,0.48 Learning Rate:0.0003Iter:1200 Loss:1.131 Accuracy:0.84,0.48,0.52,0.44 Learning Rate:0.0003Iter:1220 Loss:1.138 Accuracy:0.76,0.52,0.64,0.56 Learning Rate:0.0003Iter:1240 Loss:1.035 Accuracy:0.84,0.56,0.56,0.52 Learning Rate:0.0003Iter:1260 Loss:0.820 Accuracy:0.92,0.68,0.64,0.68 Learning Rate:0.0003Iter:1280 Loss:1.083 Accuracy:0.92,0.36,0.52,0.64 Learning Rate:0.0003Iter:1300 Loss:0.966 Accuracy:1.00,0.52,0.44,0.60 Learning Rate:0.0003Iter:1320 Loss:0.804 Accuracy:0.84,0.68,0.60,0.64 Learning Rate:0.0003Iter:1340 Loss:0.845 Accuracy:0.92,0.72,0.48,0.56 Learning Rate:0.0003Iter:1360 Loss:0.923 Accuracy:0.80,0.48,0.64,0.56 Learning Rate:0.0003Iter:1380 Loss:0.664 Accuracy:0.96,0.60,0.60,0.88 Learning Rate:0.0003Iter:1400 Loss:0.915 Accuracy:0.88,0.72,0.40,0.72 Learning Rate:0.0003Iter:1420 Loss:0.724 Accuracy:0.92,0.72,0.64,0.72 Learning Rate:0.0003Iter:1440 Loss:0.574 Accuracy:0.96,0.76,0.76,0.76 Learning Rate:0.0003Iter:1460 Loss:0.550 Accuracy:0.88,0.80,0.72,0.88 Learning Rate:0.0003Iter:1480 Loss:0.588 Accuracy:0.88,0.72,0.84,0.84 Learning Rate:0.0003Iter:1500 Loss:0.611 Accuracy:0.80,0.76,0.68,0.84 Learning Rate:0.0003Iter:1520 Loss:0.487 Accuracy:0.88,0.84,0.80,0.96 Learning Rate:0.0003Iter:1540 Loss:0.648 Accuracy:0.88,0.68,0.72,0.80 Learning Rate:0.0003Iter:1560 Loss:0.600 Accuracy:0.84,0.76,0.68,0.84 Learning Rate:0.0003Iter:1580 Loss:0.714 Accuracy:0.88,0.68,0.68,0.76 Learning Rate:0.0003Iter:1600 Loss:0.497 Accuracy:0.96,0.72,0.76,0.84 Learning Rate:0.0003Iter:1620 Loss:0.519 Accuracy:0.88,0.80,0.72,0.84 Learning Rate:0.0003Iter:1640 Loss:0.551 Accuracy:0.92,0.72,0.68,0.92 Learning Rate:0.0003Iter:1660 Loss:0.539 Accuracy:0.92,0.80,0.64,0.88 Learning Rate:0.0003Iter:1680 Loss:0.484 Accuracy:0.92,0.80,0.80,0.76 Learning Rate:0.0003Iter:1700 Loss:0.428 Accuracy:0.96,0.80,0.84,0.88 Learning Rate:0.0003Iter:1720 Loss:0.510 Accuracy:0.92,0.68,0.84,0.80 Learning Rate:0.0003Iter:1740 Loss:0.548 Accuracy:0.88,0.80,0.72,0.80 Learning Rate:0.0003Iter:1760 Loss:0.358 Accuracy:0.92,0.80,0.84,1.00 Learning Rate:0.0003Iter:1780 Loss:0.374 Accuracy:0.92,0.76,0.92,0.84 Learning Rate:0.0003Iter:1800 Loss:0.442 Accuracy:0.88,0.80,0.68,0.88 Learning Rate:0.0003Iter:1820 Loss:0.432 Accuracy:0.96,0.80,0.72,0.88 Learning Rate:0.0003Iter:1840 Loss:0.399 Accuracy:1.00,0.84,0.80,0.76 Learning Rate:0.0003Iter:1860 Loss:0.541 Accuracy:1.00,0.68,0.64,0.88 Learning Rate:0.0003Iter:1880 Loss:0.495 Accuracy:0.92,0.64,0.76,0.80 Learning Rate:0.0003Iter:1900 Loss:0.275 Accuracy:0.88,0.88,0.88,0.88 Learning Rate:0.0003Iter:1920 Loss:0.319 Accuracy:0.96,0.92,0.88,0.80 Learning Rate:0.0003Iter:1940 Loss:0.259 Accuracy:1.00,0.96,0.84,0.92 Learning Rate:0.0003Iter:1960 Loss:0.379 Accuracy:0.96,0.76,0.76,0.84 Learning Rate:0.0003Iter:1980 Loss:0.388 Accuracy:0.92,0.92,0.80,0.84 Learning Rate:0.0003Iter:2000 Loss:0.350 Accuracy:0.96,0.88,0.72,0.96 Learning Rate:0.0001Iter:2020 Loss:0.448 Accuracy:0.96,0.72,0.92,0.84 Learning Rate:0.0001Iter:2040 Loss:0.232 Accuracy:0.96,0.84,0.92,0.92 Learning Rate:0.0001Iter:2060 Loss:0.196 Accuracy:0.92,0.92,0.92,0.84 Learning Rate:0.0001Iter:2080 Loss:0.346 Accuracy:0.96,0.92,0.72,0.84 Learning Rate:0.0001Iter:2100 Loss:0.181 Accuracy:0.96,0.92,0.96,0.96 Learning Rate:0.0001Iter:2120 Loss:0.231 Accuracy:0.96,0.80,0.88,1.00 Learning Rate:0.0001Iter:2140 Loss:0.201 Accuracy:1.00,1.00,0.76,0.92 Learning Rate:0.0001Iter:2160 Loss:0.271 Accuracy:0.96,0.92,0.88,0.92 Learning Rate:0.0001Iter:2180 Loss:0.214 Accuracy:0.96,0.92,0.96,0.92 Learning Rate:0.0001Iter:2200 Loss:0.241 Accuracy:0.96,0.92,1.00,0.88 Learning Rate:0.0001Iter:2220 Loss:0.268 Accuracy:0.92,0.92,0.88,0.92 Learning Rate:0.0001Iter:2240 Loss:0.249 Accuracy:0.92,0.92,0.84,0.96 Learning Rate:0.0001Iter:2260 Loss:0.188 Accuracy:0.96,0.92,0.92,0.92 Learning Rate:0.0001Iter:2280 Loss:0.196 Accuracy:0.96,0.88,0.92,1.00 Learning Rate:0.0001Iter:2300 Loss:0.186 Accuracy:1.00,0.80,0.92,1.00 Learning Rate:0.0001Iter:2320 Loss:0.167 Accuracy:1.00,0.88,0.88,0.96 Learning Rate:0.0001Iter:2340 Loss:0.282 Accuracy:0.96,0.84,0.92,0.92 Learning Rate:0.0001Iter:2360 Loss:0.224 Accuracy:1.00,0.88,0.88,0.96 Learning Rate:0.0001Iter:2380 Loss:0.209 Accuracy:0.92,0.84,0.96,1.00 Learning Rate:0.0001Iter:2400 Loss:0.100 Accuracy:1.00,1.00,0.96,1.00 Learning Rate:0.0001Iter:2420 Loss:0.227 Accuracy:0.96,0.96,0.88,0.84 Learning Rate:0.0001Iter:2440 Loss:0.228 Accuracy:0.96,0.96,0.92,0.88 Learning Rate:0.0001Iter:2460 Loss:0.169 Accuracy:1.00,0.92,0.84,0.96 Learning Rate:0.0001Iter:2480 Loss:0.162 Accuracy:0.96,0.84,1.00,0.96 Learning Rate:0.0001Iter:2500 Loss:0.149 Accuracy:0.96,0.92,0.96,0.88 Learning Rate:0.0001Iter:2520 Loss:0.198 Accuracy:0.96,0.96,0.88,0.92 Learning Rate:0.0001Iter:2540 Loss:0.134 Accuracy:0.96,1.00,0.92,0.96 Learning Rate:0.0001Iter:2560 Loss:0.181 Accuracy:0.96,0.96,0.92,0.92 Learning Rate:0.0001Iter:2580 Loss:0.230 Accuracy:0.96,0.92,0.84,0.88 Learning Rate:0.0001Iter:2600 Loss:0.137 Accuracy:1.00,1.00,0.92,0.92 Learning Rate:0.0001Iter:2620 Loss:0.111 Accuracy:1.00,0.96,1.00,1.00 Learning Rate:0.0001Iter:2640 Loss:0.142 Accuracy:1.00,0.92,0.96,0.92 Learning Rate:0.0001Iter:2660 Loss:0.158 Accuracy:0.96,0.96,0.84,0.96 Learning Rate:0.0001Iter:2680 Loss:0.070 Accuracy:0.96,0.96,0.96,1.00 Learning Rate:0.0001Iter:2700 Loss:0.119 Accuracy:1.00,1.00,0.92,0.96 Learning Rate:0.0001Iter:2720 Loss:0.074 Accuracy:0.96,0.96,0.96,1.00 Learning Rate:0.0001Iter:2740 Loss:0.125 Accuracy:1.00,1.00,1.00,0.92 Learning Rate:0.0001Iter:2760 Loss:0.072 Accuracy:1.00,1.00,0.96,1.00 Learning Rate:0.0001Iter:2780 Loss:0.109 Accuracy:0.96,0.88,0.92,1.00 Learning Rate:0.0001Iter:2800 Loss:0.181 Accuracy:1.00,0.96,0.96,0.92 Learning Rate:0.0001Iter:2820 Loss:0.121 Accuracy:1.00,0.88,1.00,1.00 Learning Rate:0.0001Iter:2840 Loss:0.102 Accuracy:1.00,0.96,0.96,0.96 Learning Rate:0.0001Iter:2860 Loss:0.241 Accuracy:0.92,0.88,0.92,0.84 Learning Rate:0.0001Iter:2880 Loss:0.129 Accuracy:1.00,0.92,0.96,0.92 Learning Rate:0.0001Iter:2900 Loss:0.214 Accuracy:0.96,0.88,0.92,0.88 Learning Rate:0.0001Iter:2920 Loss:0.138 Accuracy:1.00,0.96,0.92,0.96 Learning Rate:0.0001Iter:2940 Loss:0.110 Accuracy:0.92,0.88,1.00,1.00 Learning Rate:0.0001Iter:2960 Loss:0.103 Accuracy:0.96,0.96,0.96,1.00 Learning Rate:0.0001Iter:2980 Loss:0.083 Accuracy:0.96,1.00,0.96,0.96 Learning Rate:0.0001Iter:3000 Loss:0.111 Accuracy:0.96,0.92,0.96,0.92 Learning Rate:0.0000 经过大约3000次迭代 accuracy 可以达到要求。 3）测试测试代码如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131# 验证码测试import osimport tensorflow as tffrom PIL import Imagefrom nets import nets_factoryimport numpy as npimport matplotlib.pyplot as plt# 不同字符数量CHAR_SET_LEN = 10# 图片高度IMAGE_HEIGHT = 60# 图片宽度IMAGE_WIDTH = 160# 批次BATCH_SIZE = 1MOD_DIR = "D:/Tensorflow/captcha/model/"# tfrecord文件存放路径TFRECORD_FILE = "D:/Tensorflow/captcha/validation.tfrecord"# placeholderx = tf.placeholder(tf.float32,[None,224,224])# 从tfrecord读出数据def read_and_decode(filename): # 根据文件名生成一个队列 filename_queue = tf.train.string_input_producer([filename]) # create a reader from file queue reader = tf.TFRecordReader() # reader从文件队列中读入一个序列化的样本,返回文件名和文件 _, serialized_example = reader.read(filename_queue) # get feature from serialized example # 解析符号化的样本 features = tf.parse_single_example( serialized_example, features=&#123; 'image': tf.FixedLenFeature([], tf.string), 'label0': tf.FixedLenFeature([], tf.int64), 'label1': tf.FixedLenFeature([], tf.int64), 'label2': tf.FixedLenFeature([], tf.int64), 'label3': tf.FixedLenFeature([], tf.int64), &#125;) #获取图片数据 image = tf.decode_raw(features["image"], tf.uint8) # 没有经过预处理的灰度图 image_raw = tf.reshape(image, [224,224]) #tf.train.shuffle_batch必须确定shape image = tf.reshape(image, [224,224]) # 图片预处理 image = tf.cast(image, tf.float32) /255.0 image = tf.subtract(image,0.5) image = tf.multiply(image,2.0) # 获取label label0 = tf.cast(features['label0'], tf.int32) label1 = tf.cast(features['label1'], tf.int32) label2 = tf.cast(features['label2'], tf.int32) label3 = tf.cast(features['label3'], tf.int32) return image, image_raw, label0, label1, label2, label3# 获取图片数据和标签image, image_raw, label0, label1, label2, label3 = read_and_decode(TFRECORD_FILE)# 使用shuffle_batch可以随机打乱输入 next_batch挨着往下取# shuffle_batch才能实现[img,label]的同步,也即特征和label的同步,不然可能输入的特征和label不匹配# 比如只有这样使用,才能使img和label一一对应,每次提取一个image和对应的label# shuffle_batch返回的值就是RandomShuffleQueue.dequeue_many()的结果# Shuffle_batch构建了一个RandomShuffleQueue，并不断地把单个的[img,label],送入队列中image_batch, image_raw_batch, label_batch0, label_batch1, label_batch2, label_batch3 = tf.train.shuffle_batch( [image,image_raw, label0,label1,label2,label3], batch_size=BATCH_SIZE, capacity=5000, min_after_dequeue=1000,num_threads=1)# 定义网络结构train_network_fn = nets_factory.get_network_fn( 'alexnet_v2', num_classes=CHAR_SET_LEN, weight_decay=0.0005, is_training=False)with tf.Session() as sess: X = tf.reshape(x,[BATCH_SIZE,224,224,1]) # 数据输入网络得到输出值 logits0,logits1,logits2,logits3,end_points = train_network_fn(X) # 预测值 predict0 = tf.reshape(logits0,[-1,CHAR_SET_LEN]) predict0 = tf.argmax(predict0,1) predict1 = tf.reshape(logits1, [-1, CHAR_SET_LEN]) predict1 = tf.argmax(predict1, 1) predict2 = tf.reshape(logits2, [-1, CHAR_SET_LEN]) predict2 = tf.argmax(predict2, 1) predict3 = tf.reshape(logits3, [-1, CHAR_SET_LEN]) predict3 = tf.argmax(predict3, 1) # 初始化 sess.run(tf.global_variables_initializer()) #载入训练好的模型 saver = tf.train.Saver() saver.restore(sess, MOD_DIR + "captcha.model-3000") # 创建一个协调器，管理线程 coord = tf.train.Coordinator() # 启动队列 threads = tf.train.start_queue_runners(sess=sess,coord=coord) for i in range(5): # 获取一个批次的数据和标签 b_image,b_image_raw, b_label0,b_label1,b_label2,b_label3 = sess.run([image_batch,image_raw_batch, label_batch0, label_batch1, label_batch2, label_batch3]) # 显示图片 img = Image.fromarray(b_image_raw[0], "L") plt.imshow(np.array(img)) plt.axis('off') plt.show() # 打印标签 print('label:',b_label0,b_label1,b_label2,b_label3) # 预测 label0,label1,label2,label3 = sess.run([predict0,predict1,predict2,predict3], feed_dict=&#123;x:b_image&#125;) # print print('predict:',label0,label1,label2,label3) # 通知其他线程关闭 coord.request_stop() # 其他所有线程关闭之后，这一函数才能返回 coord.join(threads) INFO:tensorflow:Restoring parameters from D:/Tensorflow/captcha/model/captcha.model-3000 label: [3] [1] [8] [3]predict: [3] [1] [8] [3] label: [8] [1] [1] [7]predict: [8] [1] [1] [7] label: [4] [7] [3] [4]predict: [4] [7] [3] [4] label: [5] [2] [9] [4]predict: [5] [6] [9] [8] label: [1] [6] [6] [8]predict: [1] [4] [8] [8] 总结：如何训练带字母字符的验证码呢？其实很简单，A-Z，一共26个字母，我们可以映射为11~35这26个数字，A：10，B：11，，，Z :35,那么，这种数字+字母的组合一共有10+26=36个字符，同样采用one-hot编码，label是一个36维的向量，只有1个值为1，其余为0，A：000000000010000…..000； 2 普通的单任务模式1）准备工作将net文件夹拷贝到当前目录下，不做改动。 2）训练3）测试]]></content>
      <categories>
        <category>tensorflow</category>
      </categories>
      <tags>
        <tag>alexnet</tag>
        <tag>slim</tag>
        <tag>Multi-task Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tensorflow学习10-1：验证码识别——生成验证码和tfrecord文件]]></title>
    <url>%2F2018%2F04%2F25%2Ftensorflow%2FTensorflow%E5%AD%A6%E4%B9%A010-1%EF%BC%9A%E9%AA%8C%E8%AF%81%E7%A0%81%E8%AF%86%E5%88%AB%E2%80%94%E2%80%94%E7%94%9F%E6%88%90%E9%AA%8C%E8%AF%81%E7%A0%81%E5%92%8Ctfrecord%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[1 生成验证码图片这些生成的图片位于同一个文件夹下，而且图片名就是 label 值。 生成代码如下：1234567891011# 验证码生成库from captcha.image import ImageCaptcha # pip install captchaimport numpy as npfrom PIL import Imageimport randomimport sysnumber = ['0','1','2','3','4','5','6','7','8','9']# letter = ['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z']# LETTER = ['A','B','C','D','E','F','G','H','I','J','K','L','M','N','O','P','Q','R','S','T','U','V','W','X','Y','Z']CAPTCHA_SAVE_DIR = "D:/Tensorflow/captcha/images/" 12345678910111213141516171819202122232425262728293031323334353637'''随机生成4个数字的字符串成。char_set：用于生成的字符listcaptcha_size：生成的验证码位数'''def random_captcha_text(char_set=number, captcha_size=4): # 验证码列表 captcha_text = [] for i in range(captcha_size): #随机选择 c = random.choice(char_set) #加入验证码列表 captcha_text.append(c) return captcha_text'''生成字符对应的验证码'''def gen_captcha_text_and_iamge(): image = ImageCaptcha() #获得随机生成的验证码 captcha_text = random_captcha_text() #把验证码列表转为字符串 captcha_text = "".join(captcha_text) #生成验证码 captcha = image.generate(captcha_text) image.write(captcha_text, CAPTCHA_SAVE_DIR + captcha_text + ".jpg") #写到文件 #循环生成10000次，但是重复的会被覆盖，所以&lt;10000num = 10000if __name__ == "__main__": for i in range(num): gen_captcha_text_and_iamge() sys.stdout.write("\r&gt;&gt; Creating image %d/%d" % (i+1, num)) sys.stdout.flush() sys.stdout.write("\n") sys.stdout.flush() print("Generate finished.") Creating image 10000/10000Generate finished. 2 将这些图片转为tfrecord文件我们生成tfrecord文件用于验证码识别程序的训练和测试，生成好后会产生2个.tfrecord文件 生成tfrecord代码如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124-# image_to_tfrecord_by_filename.py——把验证码转换成tfrecord文件#tfrecord文件，底层就是protobuf格式import tensorflow as tfimport numpy as npimport osimport randomimport mathimport sysfrom PIL import Image#验证集数量tf.app.flags.DEFINE_integer("num_validation", 500, "the divisiory number of validation data")#随机种子tf.app.flags.DEFINE_integer("random_seed", 7, "random seed")#图片目录tf.app.flags.DEFINE_string("dataset_dir", "D:/Tensorflow/captcha/images/", "dir of images and save position") #保存tfrecord目录tf.app.flags.DEFINE_string("tfrecord_dir", "D:/Tensorflow/captcha/", "dir of tfrecord")FLAGS = tf.app.flags.FLAGS#判断tfrecord文件是否存在def _dataset_exists(dataset_dir): for split_name in ["train", "validation"]: output_filename = os.path.join(dataset_dir, split_name + ".tfrecord") if not tf.gfile.Exists(output_filename): return False return True #获取总图片文件夹下的 所有图片文件名以及分类（子文件夹名）def _get_filenames_and_classes(dataset_dir): photo_filenames = [] for filename in os.listdir(dataset_dir): #合并文件路径 path = os.path.join(dataset_dir, filename) photo_filenames.append(path) return photo_filenamesdef int64_feature(values): if not isinstance(values, (tuple, list)): values = [values] return tf.train.Feature(int64_list=tf.train.Int64List(value=values))def bytes_feature(values): return tf.train.Feature(bytes_list=tf.train.BytesList(value=[values])) def image_to_tfexample(image_data, label0, label1, label2, label3): #Abstract base class for protocol message return tf.train.Example(features=tf.train.Features(feature=&#123; "image": bytes_feature(image_data), "label0": int64_feature(label0), "label1": int64_feature(label1), "label2": int64_feature(label2), "label3": int64_feature(label3), &#125;))#把数据转为tfrecord格式def _convert_dataset(split_name, filenames, dataset_dir): assert split_name in ["train", "validation"] with tf.Session() as sess: #定义tfrecord文件路径 output_filename = os.path.join(FLAGS.tfrecord_dir, split_name + ".tfrecord") with tf.python_io.TFRecordWriter(output_filename) as tfrecord_writer: for i, filename in enumerate(filenames): try: sys.stdout.write("\r&gt;&gt; Converting image(%s) %d/%d" % (split_name, i+1, len(filenames))) sys.stdout.flush() #读取图片 image_data = Image.open(filename) #根据模型的结构resize image_data = image_data.resize((224, 224)) #灰度化 image_data = np.array(image_data.convert("L")) #将图片转化为bytes image_data = image_data.tobytes() #获取label labels = filename.split("/")[-1][0:4] num_labels = [] for j in range(4): num_labels.append(int(labels[j])) #生成tfrecord文件 example = image_to_tfexample(image_data, num_labels[0], num_labels[1], num_labels[2], num_labels[3]) tfrecord_writer.write(example.SerializeToString()) except IOError as e: print("Could not read:", filenames[i]) print("Error:",e) print("Skip the pic.\n") sys.stdout.write("\n") sys.stdout.flush()def main(_): #判断tfrecord文件是否存在 if _dataset_exists(FLAGS.tfrecord_dir): print("tfrecord文件已存在") else: #获得所有图片以及分类 photo_filenames = _get_filenames_and_classes(FLAGS.dataset_dir) #数据切分为训练集和测试集 random.seed(FLAGS.random_seed) random.shuffle(photo_filenames) training_filenames = photo_filenames[FLAGS.num_validation:] #500之后的图片作为训练 validation_filenames = photo_filenames[:FLAGS.num_validation] #0-500的图片作为训练 #数据转换 _convert_dataset("train", training_filenames, FLAGS.dataset_dir) _convert_dataset("validation", validation_filenames, FLAGS.dataset_dir) print("finished.") if __name__ == "__main__": tf.app.run() Converting image(train) 5858/5858Converting image(validation) 500/500finished.]]></content>
      <categories>
        <category>tensorflow</category>
      </categories>
      <tags>
        <tag>alexnet</tag>
        <tag>slim</tag>
        <tag>tfrecord</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[anaconda安装命令整理]]></title>
    <url>%2F2018%2F04%2F25%2Finstall%20and%20config%2Fanaconda%E5%AE%89%E8%A3%85%E5%91%BD%E4%BB%A4%E6%95%B4%E7%90%86%2F</url>
    <content type="text"><![CDATA[查看库版本anaconda 查看版本号，以tensorflow为例 1conda list tensorflow 安装命令设置用清华镜像安装（如果需要）：1conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/ 1 Tensorflow 安装（以windows版本为例）S1. 查找所有Tensorflow版本：1anaconda search -t conda tensorflow 找到windows版本S2. 显示该版本的安装命令：1anaconda show dhirschfeld/tensorflow S3. 使用所提示的安装命令：1conda install --channel https://conda.anaconda.org/dhirschfeld tensorflow 2 tflearn 安装（以windows版本为例）用pip install tflearn命令安装tflearn后，运行下面代码，如果出现警告12from __future__ import division, print_function, absolute_import import tflearn “curses is not supported on this machine (please install/reinstall curses for an optimal experience” 使用命令：1pip search curses 再执行下面命令以安装windows版本的curses1pip install windows-curses 卸载命令1pip uninstall xxx or1conda uninstall xxx]]></content>
      <categories>
        <category>install and config</category>
      </categories>
      <tags>
        <tag>anaconda</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jupyter设置]]></title>
    <url>%2F2018%2F04%2F25%2Finstall%20and%20config%2Fjupyter%E8%AE%BE%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[更改默认启动目录方法一：右键jupyter notebook快捷方式属性，把“目标”属性最后的变量改为自己的路径，如下：1&quot;C:\\Users\\lenovo\\Desktop\\Python WORK SPACE\\&quot; 注意前面一个空格要保留 方法二：]]></content>
      <categories>
        <category>install and config</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Tensorflow学习9-5：谷歌inception-v3模型之fine-tune slim alexnet]]></title>
    <url>%2F2018%2F04%2F24%2Ftensorflow%2FTensorflow%E5%AD%A6%E4%B9%A09-5%EF%BC%9A%E8%B0%B7%E6%AD%8Cinception-v3%E6%A8%A1%E5%9E%8B%E4%B9%8Bfine-tune%20slim%20alexnet%2F</url>
    <content type="text"><![CDATA[进行 fine-tune 操作需要微调训练所有层，所以迭代训练次数比较多。 1 数据准备准备好tfrecord格式的图片数据文件，和labels.txt。可以参考上篇。 2 定义新的dataset文件首先，在dataset/目录下新建一个文件夹 satellite.py，并将flowers.py文件夹中的内容复制到 satellite.py 中，接下来需要修改以下几处内容。 第一处修改， 第二处修改修改为image/format部分1'image/format': tf.FixedLenFeature((), tf.string, default_value='jpg'), 修改完 satellite.py后，还需要在同目录的dataset_factory.py文件夹中注册satellite数据库。红色框内为新增加的satellite数据 3 下载训练好的inception-v3模型在http://download.tensorflow.org/models/inception_v3_2016_08_28.tar.gz 下载并解压后，会得到一个inception_v3.ckpt 文件 4 开始训练在slim文件夹下运行下面脚本开始训练：（★注意：如果softmax报错，修改文件D:\Anaconda3\Lib\site-packages\tensorflow\python\framework\ops.py第3385行，在函数 create_op() 内新增一行with tf.device(&#39;/cpu:0&#39;): ）1234567891011121314151617181920python train_image_classifier.py ^--train_dir=model2 :训练好的模型存放目录 ^--dataset_name=satellite :用于读取tfrecord数据集的python文件 ^--dataset_split_name=train :这里使用切分的训练集 ^--dataset_dir=images2 :tfrecord文件目录 ^--batch_size=5 :GPU内存小的建议不要改大，否则报错 ^--max_number_of_steps=1000 :训练次数 ^--model_name=inception_v3 :训练模型 ^--checkpoint_path=D:/Tensorflow/models/inception2016/inception_v3.ckpt :fine-tune专用^--checkpoint_exclude_scopes=InceptionV3/Logits,InceptionV3/AuxLogits :fine-tune专用^--trainable_scopes=InceptionV3/Logits,InceptionV3/AuxLogits :fine-tune专用^--learning_rate=0.001 ^--learning_rate_decay_type=fixed ^--save_interval_secs=300 ^--save_summaries_secs=2 ^--log_every_n_steps=10 ^--optimizer=rmsprop ^--weight_decay=0.00004 ^--clone_on_cpu=false :可以设置为true指定CPU执行pause 如图，一般训练10000步左右可以达到准确率要求，ctrl+c 停止脚本程序 5 验证模型准确率可以用eval_image_classifier.py程序进行验证，在slim文件夹下运行以下脚本 12345678python test_image_classifier.py ^--checkpoint_path=model2 ^--eval_dir=validation_result ^--dataset_name=satellite ^--dataset_split_name=validation ^--dataset_dir=images2 ^--model_name=inception_v3pause 得到训练后模型的accuracy结果： 6 导出模型，并对单张图片进行识别STEP 1:导出网络结构在slim文件夹下运行以下脚本123456python export_inference_graph.py ^--alsologtostderr ^--model_name=inception_v3 ^--output_file=model2/inception_v3_inf_graph.pb ^--dataset_name=satellitepause 这个命令会在 model2 文件夹下生成一个inception_v3_inf_graph.pb文件。（注：inception_v3_inf_graph.pb文件夹只保存了inception_v3的网络结构并不包含训练得到的模型。 STEP 2:生成完整的 .pd 模型文件运行下面脚本，将checkpoint中的模型参数保存进来，转换成完整的模型文件。（需将8100改成model文件夹中保存的实际的模型训练步数）1234567python freeze_graph.py ^--input_graph=model2/inception_v3_inf_graph.pb ^--input_checkpoint=model2/model.ckpt-8100 ^--input_binary=true ^--output_node_names=InceptionV3/Predictions/Reshape_1 ^--output_graph=model2/frozen_graph.pbpause STEP 3:运行导出模型分类单张图片运行下面脚本12345python test_image_classifier.py ^--model_path model2/frozen_graph.pb ^--label_path images2/labels.txt ^--image_file test_images/water.jpgpause 分类结果如下： 总结：脚本的运行顺序如图编号所示]]></content>
      <categories>
        <category>tensorflow</category>
      </categories>
      <tags>
        <tag>fine-tune</tag>
        <tag>slim</tag>
        <tag>inception-v3</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[markdown语法]]></title>
    <url>%2F2018%2F04%2F24%2Fhexo%2Fmarkdown%E8%AF%AD%E6%B3%95%2F</url>
    <content type="text"><![CDATA[markdown语法空格：输入法全角状态下space]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>markdown</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tensorflow学习9-4：谷歌inception-v3模型 之 生成tfrecord文件]]></title>
    <url>%2F2018%2F04%2F23%2Ftensorflow%2FTensorflow%E5%AD%A6%E4%B9%A09-4%EF%BC%9A%E8%B0%B7%E6%AD%8Cinception-v3%E6%A8%A1%E5%9E%8B%E4%B9%8B%E7%94%9F%E6%88%90tfrecord%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[生成tfrecord文件，用于从零训练自己的模型或者fine-tune微调训练。 1234567891011121314151617#tfrecord文件，底层就是protobuf格式import tensorflow as tfimport osimport randomimport mathimport sys#验证集数量_NUM_VALID = 1000#随机种子_RANDOM_SEED = 7#数据块_NUM_SHARDS = 2#DATASET_DIR = "D:/Tensorflow/slim/images2/"#标签文件名LABELS_FILENAME = "D:/Tensorflow/slim/images2/labels.txt" 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126#定义tfrecord文件的路径+名字def _get_dataset_filename(dataset_dir, split_name, shard_id): output_filename = "image_%s_%05d-of-%05d.tfrecord" % (split_name, shard_id, _NUM_SHARDS) return os.path.join(dataset_dir, output_filename)#判断tfrecord文件是否存在def _dataset_exists(dataset_dir): for split_name in ["train", "validation"]: for shard_id in range(_NUM_SHARDS): output_filename = _get_dataset_filename(dataset_dir, split_name, shard_id) if not tf.gfile.Exists(output_filename): return False return True #获取总图片文件夹下的 所有图片文件名以及分类（子文件夹名）def _get_filenames_and_classes(dataset_dir): #数据目录 directories = [] #分类名称 class_names = [] for filename in os.listdir(dataset_dir): #合并文件路径 path = os.path.join(dataset_dir, filename) #判断该路径是否为目录 if os.path.isdir(path): #加入数据目录 directories.append(path) #加入类别名称 class_names.append(filename) photo_filenames = [] #循环每个分类的文件夹 for directory in directories: for filename in os.listdir(directory): path = os.path.join(directory, filename) #把图片加入图片列表 photo_filenames.append(path) return photo_filenames, class_namesdef int64_feature(values): if not isinstance(values, (tuple, list)): values = [values] return tf.train.Feature(int64_list=tf.train.Int64List(value=values))def bytes_feature(values): return tf.train.Feature(bytes_list=tf.train.BytesList(value=[values])) def image_to_tfexample(image_data, image_format, class_id): #Abstract base class for protocol message return tf.train.Example(features=tf.train.Features(feature=&#123; "image/encoded": bytes_feature(image_data), "image/format": bytes_feature(image_format), "image/class/label": int64_feature(class_id), &#125;))def write_label_file(labels_to_class_names, dataset_dir, filename=LABELS_FILENAME): labels_filename = os.path.join(dataset_dir, filename) with tf.gfile.Open(labels_filename, "w") as f: for label in labels_to_class_names: class_name = labels_to_class_names[label] f.write("%d:%s\n" % (label, class_name))#把数据转为tfrecord格式def _convert_dataset(split_name, filenames, class_names_to_ids, dataset_dir): assert split_name in ["train", "validation"] #切分数据块维多个tfrecord文件，计算每个数据块有多少 num_per_shard = int(len(filenames) / _NUM_SHARDS) with tf.Graph().as_default(): with tf.Session() as sess: for shard_id in range(_NUM_SHARDS): #定义tfrecord文件路径 output_filename = _get_dataset_filename(dataset_dir, split_name, shard_id) with tf.python_io.TFRecordWriter(output_filename) as tfrecord_writer: #每一个数据块开始的位置 start_ndx = shard_id * num_per_shard #每一个数据块最后的位置 end_ndx = min((shard_id+1) * num_per_shard, len(filenames)) for i in range(start_ndx, end_ndx): try: sys.stdout.write("\r&gt;&gt; Converting image(%s) %d/%d shard %d" % (split_name, i+1, len(filenames), shard_id)) sys.stdout.flush() #读取图片 image_data = tf.gfile.FastGFile(filenames[i], "rb").read() #获得图片的类别名称 class_name = os.path.basename(os.path.dirname(filenames[i])) #找到类别名称对应的ID class_id = class_names_to_ids[class_name] #生成tfrecord文件 example = image_to_tfexample(image_data, b"jpg", class_id) tfrecord_writer.write(example.SerializeToString()) except IOError as e: print("Could not read:", filenames[i]) print("Error:",e) print("Skip the pic.\n") sys.stdout.write("\n") sys.stdout.flush() if __name__ == "__main__": #判断tfrecord文件是否存在 if _dataset_exists(DATASET_DIR): print("tfrecord文件已存在") else: #获得所有图片以及分类 photo_filenames, class_names = _get_filenames_and_classes(DATASET_DIR) #吧分类转为字典格式，类似于&#123;"house": 3, "flower": 1&#125; class_names_to_ids = dict(zip(class_names, range(len(class_names)))) #数据切分为训练集和测试集 random.seed(_RANDOM_SEED) random.shuffle(photo_filenames) training_filenames = photo_filenames[_NUM_VALID:] #500之后的图片作为训练 validation_filenames = photo_filenames[:_NUM_VALID] #0-500的图片作为训练# for var in training_filenames:# print("training_filenames: ", os.path.basename(var))# for var in validation_filenames:# print("validation_filenames: ", os.path.basename(var)) #数据转换 _convert_dataset("train", training_filenames, class_names_to_ids, DATASET_DIR) _convert_dataset("validation", validation_filenames, class_names_to_ids, DATASET_DIR) #输出labels文件 labels_to_class_names = dict(zip(range(len(class_names)), class_names)) write_label_file(labels_to_class_names, DATASET_DIR) Converting image(train) 3800/3800 shard 1Converting image(validation) 1000/1000 shard 1]]></content>
      <categories>
        <category>tensorflow</category>
      </categories>
      <tags>
        <tag>slim</tag>
        <tag>tfrecord</tag>
        <tag>inception-v3</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2模型评估与选择]]></title>
    <url>%2F2018%2F04%2F21%2Fmachine_learning_theory%2F2%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E4%B8%8E%E9%80%89%E6%8B%A9%2F</url>
    <content type="text"><![CDATA[性能度量二分类任务的 混淆矩阵 和 其衍生的度量指标 True Positive （真正, TP） 被模型预测为正样本，是真的判断正确。所以就是正样本，也称作正的数。 True Negative（真负 , TN）被模型判断为负样本，是真的判断正确。所以就是负样本，也称作负的数。 False Positive （假正, FP）被模型判断为正样本，是假的判断错误。所以应该是负样本，也称作误报数。 False Negative（假负 , FN）被模型判断为负样本，是假的判断错误。所以应该是正样本，也称作漏报数。 1）常用的3个指标（多用于交叉验证） accuracy（准确率）——检验模型预测的正确率A=\frac {TP+TN}{ALL} 预测正确个数/全部样本数 precision（精确率）——检验模型预测正例的正确率P=\frac {TP}{TP+FP} 预测正确的正样本数 / 预测为的正样本数 recall/TPR（召回率/真正率）——检验模型正例预测的全面性R\ /\ TPR=\frac {TP}{TP+FN} 预测正确的正样本数 / 真实的正样本数 2）不常用的3个指标（多用于绘图） specificity（特异性/真负率）——检验模型负例预测的正确率S=\frac {TN}{TN+FP} 预测正确的负样本数 / 真实的负样本数 FPR（假正率）——用于和TPR一起绘制ROC曲线FPR=\frac {FP}{TN+FP} 预测错误的正样本数 / 真实的负样本数 FNR（假负率）——用的少 FNR=\frac {FN}{TP+FN} 预测错误的负样本数 / 真实的正样本数]]></content>
      <categories>
        <category>machine_learning_theory</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Tensorflow学习9-3：谷歌inception-v3模型之transfer learning retrain]]></title>
    <url>%2F2018%2F04%2F21%2Ftensorflow%2FTensorflow%E5%AD%A6%E4%B9%A09-3%EF%BC%9A%E8%B0%B7%E6%AD%8Cinception-v3%E6%A8%A1%E5%9E%8B%E4%B9%8Btransfer%20learning%20retrain%2F</url>
    <content type="text"><![CDATA[因为是 transfer learning 操作，所以直到网络的bottleneck部分之前都不需要改变参数和训练。只需要传入图片数据到网络中计算得到结果。再拿到这个结果到后面的全连接层进行训练。 所以训练的内容不多，迭代200次左右就可以达到要求。 1 准备模型文件.pb模型文件下载地址：http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz 解压后得到如下内容： 2 准备retrain训练工程目录1、在D:/Tensorflow目录下新建retrain文件夹。在里面新建以下文件夹：bottleneck：存放瓶颈部分输出的数据，用于全连接层的训练data └ train：存放用于训练的用文件夹分类好的图片images：存放用于测试的单个图片 2、把image_retraining中的retrain.py文件拖过来（注意：这个文件不能用最新版的） 3、新建retrain.bat文件，内容如下12345678python retrain.py ^--bottleneck_dir bottleneck ^--how_many_training_steps 100 ^--model_dir D:/Tensorflow/models/inception/ ^--output_graph output_graph.pb ^--output_labels output_labels.txt ^--image_dir data/train/ pause 文件架构如下图： 3 执行retrain.bat脚本，进行transfer learning 跌倒200次训练完成后，会在当前目录生成output_graph.pb和output_labels.txt两个文件，至此训练完成。可以用这个pb模型文件来测试分类了。 4 测试分类效果自己写一个测试代码，用自己的图片测试下分类效果： 123456789import tensorflow as tfimport osimport numpy as npimport refrom PIL import Imageimport matplotlib.pyplot as pltTEST_IMG_DIR = "D:/Tensorflow/Test Images/"RETRAIN_DIR = "D:/Tensorflow/retrain/" #模型存放目录 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950lines = tf.gfile.GFile(RETRAIN_DIR + "output_labels.txt").readlines()uid_to_human = &#123;&#125;for uid,line in enumerate(lines): line=line.strip("\n") uid_to_human[uid] = line#print(uid_to_human)def id_to_string(node_id): if node_id not in uid_to_human: print("node_id not in uid_to_human") return "" return uid_to_human[node_id]#创建一个图来存放google训练好的模型with tf.gfile.FastGFile(RETRAIN_DIR + "output_graph.pb", "rb") as f: graph_def = tf.GraphDef() graph_def.ParseFromString(f.read()) tf.import_graph_def(graph_def, name="") with tf.Session() as sess: softmax_tensor = sess.graph.get_tensor_by_name("final_result:0") #遍历用于测试的图片目录 for root,dirs,files in os.walk(RETRAIN_DIR + "images/"): for file in files: #载入图片 image_data = tf.gfile.FastGFile(os.path.join(root,file), "rb").read() predictions = sess.run(softmax_tensor, &#123;"DecodeJpeg/contents:0" : image_data&#125;) #jpg格式图片 #predictions = sess.run(softmax_tensor, &#123;"DecodeJPGInput:0" : image_data&#125;) #jpg格式图片 predictions = np.squeeze(predictions) #吧结果转为1维数据 #打印图片路径及名称 image_path = os.path.join(root,file) print(image_path) #显示图片 img = Image.open(image_path) plt.imshow(img) plt.axis("off") plt.show() #排序 top_k = predictions.argsort()[::-1] for node_id in top_k: #获取分类名称 human_string = id_to_string(node_id) #获取该分类的概率 score = predictions[node_id] print("%s (score = %.5f)" % (human_string, score)) print() D:/Tensorflow/retrain/images/111.jpg pet (score = 0.80940)flower (score = 0.19060) D:/Tensorflow/retrain/images/222.jpg flower (score = 0.99097)pet (score = 0.00903)]]></content>
      <categories>
        <category>tensorflow</category>
      </categories>
      <tags>
        <tag>transfer learning</tag>
        <tag>inception-v3</tag>
        <tag>retrain</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[直线和平面方程]]></title>
    <url>%2F2018%2F04%2F20%2Fmath%2Fcalculus%2F%E7%9B%B4%E7%BA%BF%E5%92%8C%E5%B9%B3%E9%9D%A2%E6%96%B9%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[一、直线直线方程 方程名称 形式 说明 一般式 ax+by+c=0 优点：可以表示平面上的任意一条直线缺点：要确定的常数较多 斜截式 y=kx+b 优点：只需要斜率和截距缺点：不能表示垂直x轴的直线x=a 点斜式 y-y0=k(x-x0) 优点：只需要一个点和斜率缺点：不能表示垂直x轴的直线x=a 两点式 (y-y1)/(y2-y1)=(x-x1)/(x2-x1) 优点：只需要2个点缺点：不能表示两点x1=x2或y1=y2时的直线（即垂直或水平直线） 截距式 x/a+y/b=1 优点：只需要x轴截距a和y轴截距b缺点：不能表示截距为0时的直线,比如正比例直线 二、平面平面方程常用4种 方程名称 形式 说明 一般式 Ax+By+Cz+D=0 截距式 x/a+y/b+z/c=1 点法式 A(x-x0)+B(y-y0)+C(z-z0)=0 向量(A,B,C)为平面的法向量 法线式 xcosα+ycosβ+zcosγ=p 其中cosα、cosβ、cosγ是平面法矢量的方向余弦，p为原点到平面的距离。 ​ 平面方程全部7种 三、超平面二维空间的超平面是一条直线，三维空间的超平面是一个平面，而N维空间的超平面则是N-1维的仿射空间。]]></content>
      <categories>
        <category>math</category>
        <category>calculus</category>
      </categories>
      <tags>
        <tag>平面方程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2矩阵的运算、行列式]]></title>
    <url>%2F2018%2F04%2F20%2Fmath%2Flinear_algebra%2F2%E7%9F%A9%E9%98%B5%E7%9A%84%E8%BF%90%E7%AE%97%E3%80%81%E8%A1%8C%E5%88%97%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[矩阵的运算1 矩阵乘法的具体应用总结：A中每个元素和B中每个元素相乘是有意义的；B矩阵和最终C矩阵指标数相等，相当于对应指标类元素的求和 几种特殊的矩阵1.对角矩阵2.数量矩阵★3.单矩阵4.三角矩阵5.对称矩阵 分块矩阵和其运算1 简介：2 分块矩阵相加和相乘A+B 和 AB相加：要求每个子块矩阵有相同的行数和列数相乘：要求A的列 = B的行 行列式1 矩阵的行列式和他的转置的行列式相等]]></content>
      <categories>
        <category>math</category>
        <category>linear_algebra</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Tensorflow学习9-2：谷歌inception-v3模型之图像分类测试]]></title>
    <url>%2F2018%2F04%2F20%2Ftensorflow%2FTensorflow%E5%AD%A6%E4%B9%A09-2%EF%BC%9A%E8%B0%B7%E6%AD%8Cinception-v3%E6%A8%A1%E5%9E%8B%E4%B9%8B%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E6%B5%8B%E8%AF%95%2F</url>
    <content type="text"><![CDATA[123456789import tensorflow as tfimport osimport numpy as npimport refrom PIL import Imageimport matplotlib.pyplot as plt#模型存放目录MOD_DIR = "D:/Tensorflow/models/inception/"TEST_IMG_DIR = "D:/Tensorflow/Test Images/" 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889#节点映射解析类【目标获得1-1000分类数字 =》分类名称 的映射字典】class NodeLookup(object): def __init__(self): label_lookup_path = MOD_DIR + "imagenet_2012_challenge_label_map_proto.pbtxt" uid_lookup_path = MOD_DIR + "imagenet_synset_to_human_label_map.txt" self.node_lookup = self.load(label_lookup_path, uid_lookup_path) def load(self, label_lookup_path, uid_lookup_path): # 加载分类字符串n*******对应分类名称的文件 proto_as_ascii_lines = tf.gfile.GFile(uid_lookup_path).readlines() uid_to_human = &#123;&#125; #一行一行读取数据 for line in proto_as_ascii_lines: #去掉换行符 \n line=line.strip("\n") #按照 \t 分割 parsed_items = line.split("\t") #获取分类编号 uid = parsed_items[0] #获取分类名称 human_string = parsed_items[1] #保存分类编号n*******和分类名称的映射关系 uid_to_human[uid] = human_string # 加载分类字符串n*******对应分类编号1-1000的文件 proto_as_ascii = tf.gfile.GFile(label_lookup_path).readlines() node_id_to_uid = &#123;&#125; for line in proto_as_ascii: if line.startswith(" target_class:"): #获取分类编号1-1000 target_class = int(line.split(": ")[1]) if line.startswith(" target_class_string:"): #获取编号字符串n******* target_class_string = line.split(": ")[1] #保存分类编号1-1000和编号字符串n*******的映射关系 node_id_to_uid[target_class] = target_class_string[1:-2] #现在联立2个映射，合成新的 分类编号1-1000到分类名称的映射 node_id_to_name = &#123;&#125; for key, val in node_id_to_uid.items(): #获得分类名称 name = uid_to_human[val] #建立映射 node_id_to_name[key] = name return node_id_to_name #查询函数【传入分类编号1-1000返回分类名称】 def id_to_string(self, node_id): if node_id not in self.node_lookup: print("node_id not in self.node_lookup") return "" return self.node_lookup[node_id] #创建一个图来存放google训练好的模型with tf.gfile.FastGFile(MOD_DIR + "classify_image_graph_def.pb", "rb") as f: graph_def = tf.GraphDef() graph_def.ParseFromString(f.read()) tf.import_graph_def(graph_def, name="") with tf.Session() as sess: softmax_tensor = sess.graph.get_tensor_by_name("softmax:0") #遍历用于测试的图片目录 for root,dirs,files in os.walk(TEST_IMG_DIR): for file in files: #载入图片 image_data = tf.gfile.FastGFile(os.path.join(root,file), "rb").read() predictions = sess.run(softmax_tensor, &#123;"DecodeJpeg/contents:0" : image_data&#125;) #jpg格式图片 predictions = np.squeeze(predictions) #吧结果转为1维数据 #打印图片路径及名称 image_path = os.path.join(root,file) print(image_path) #显示图片 img = Image.open(image_path) plt.imshow(img) plt.axis("off") plt.show() #排序 top_k = predictions.argsort()[-5:][::-1] #取得从大到小的5个值 node_lookup = NodeLookup() for node_id in top_k: #获取分类名称 human_string = node_lookup.id_to_string(node_id) #获取该分类的概率 score = predictions[node_id] print("%s (score = %.5f)" % (human_string, score)) print() D:/Tensorflow/Test Images/555.jpg gown (score = 0.27292)hoopskirt, crinoline (score = 0.14043)maillot (score = 0.10369)brassiere, bra, bandeau (score = 0.06863)bikini, two-piece (score = 0.05091) D:/Tensorflow/Test Images/cl.jpg torch (score = 0.40945)volleyball (score = 0.11208)racket, racquet (score = 0.09447)tennis ball (score = 0.06729)soccer ball (score = 0.04869) D:/Tensorflow/Test Images/nissan jk.jpg sports car, sport car (score = 0.49891)beach wagon, station wagon, wagon, estate car, beach waggon, station waggon, waggon (score = 0.12817)car wheel (score = 0.07217)grille, radiator grille (score = 0.03533)cab, hack, taxi, taxicab (score = 0.01612)]]></content>
      <categories>
        <category>tensorflow</category>
      </categories>
      <tags>
        <tag>inception-v3</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[1机器学习流程]]></title>
    <url>%2F2018%2F04%2F19%2Fmachine_learning_theory%2F1%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%B5%81%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[一、数据收集并给定标签数据预处理1）归一化2）数据清洗先直观上 去掉不需要的列特征。 包括：1.预测后才出现的特征（比如：实际发放的贷款）、2.高度相关的特征（比如：123和ABC）、3.关系不大的特征（比如：ID等）4.列属性只有一个值的 需要先排除列中 nan 值，再用 unique() 判定 3）缺失值处理先查找列缺失值多的，去掉这些列，只剩下缺失值少的列。之后直接去掉那些样本行即可 4）字符串值处理把 object 类型转为 int 和 float 类型。包括：1.one-hot 编码2.列属性数字替换 5）数据样本均衡分析样本分布不均衡：指的是 label 不同的样本数量差距很大。如果数据样本不均衡，容易导致分类器效果很差。为了解决，有以下2种方案： P1：oversample 或 undersampleP2：用模型参数 调节分类惩罚权重比参数： class_weight，适用于所有分类算法 特征提取数据集划分二、训练一个分类器选择模型训练模型 让学习率随迭代次数收敛 三、模型测试、评估选择模型评估方法1.交叉验证 + 指标（比如：精度、TPR）2.均方误差3.交叉熵 模型评估结果分析如果分类效果不佳，可采取以下措施： 调节分类惩罚系数比（可在模型参数中调节） 调节模型其他参数 考虑 过拟合 可能，去掉一些列 集成多个模型 尝试其他模型]]></content>
      <categories>
        <category>machine_learning_theory</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于NLP的股价预测]]></title>
    <url>%2F2018%2F04%2F19%2Fmachine_learning_in_action%2F%E5%9F%BA%E4%BA%8ENLP%E7%9A%84%E8%82%A1%E4%BB%B7%E9%A2%84%E6%B5%8B%2F</url>
    <content type="text"><![CDATA[基于NLP的股价预测123import pandas as pdfrom sklearn.feature_extraction.text import CountVectorizerfrom sklearn.linear_model import LogisticRegression 1data = pd.read_csv('Combined_News_DJIA.csv') 每行是某公司 这一天股市数据；label表示当天涨/跌，Top表示依重要程度排列的当天新闻事件 通过NLP处理可以把这些字符串转换为 机器认识的语言 1data.head() .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } Date Label Top1 Top2 Top3 Top4 Top5 Top6 Top7 Top8 ... Top16 Top17 Top18 Top19 Top20 Top21 Top22 Top23 Top24 Top25 0 2008-08-08 0 b"Georgia 'downs two Russian warplanes' as cou... b'BREAKING: Musharraf to be impeached.' b'Russia Today: Columns of troops roll into So... b'Russian tanks are moving towards the capital... b"Afghan children raped with 'impunity,' U.N. ... b'150 Russian tanks have entered South Ossetia... b"Breaking: Georgia invades South Ossetia, Rus... b"The 'enemy combatent' trials are nothing but... ... b'Georgia Invades South Ossetia - if Russia ge... b'Al-Qaeda Faces Islamist Backlash' b'Condoleezza Rice: "The US would not act to p... b'This is a busy day: The European Union has ... b"Georgia will withdraw 1,000 soldiers from Ir... b'Why the Pentagon Thinks Attacking Iran is a ... b'Caucasus in crisis: Georgia invades South Os... b'Indian shoe manufactory - And again in a se... b'Visitors Suffering from Mental Illnesses Ban... b"No Help for Mexico's Kidnapping Surge" 1 2008-08-11 1 b'Why wont America and Nato help us? If they w... b'Bush puts foot down on Georgian conflict' b"Jewish Georgian minister: Thanks to Israeli ... b'Georgian army flees in disarray as Russians ... b"Olympic opening ceremony fireworks 'faked'" b'What were the Mossad with fraudulent New Zea... b'Russia angered by Israeli military sale to G... b'An American citizen living in S.Ossetia blam... ... b'Israel and the US behind the Georgian aggres... b'"Do not believe TV, neither Russian nor Geor... b'Riots are still going on in Montreal (Canada... b'China to overtake US as largest manufacturer' b'War in South Ossetia [PICS]' b'Israeli Physicians Group Condemns State Tort... b' Russia has just beaten the United States ov... b'Perhaps *the* question about the Georgia - R... b'Russia is so much better at war' b"So this is what it's come to: trading sex fo... 2 2008-08-12 0 b'Remember that adorable 9-year-old who sang a... b"Russia 'ends Georgia operation'" b'"If we had no sexual harassment we would hav... b"Al-Qa'eda is losing support in Iraq because ... b'Ceasefire in Georgia: Putin Outmaneuvers the... b'Why Microsoft and Intel tried to kill the XO... b'Stratfor: The Russo-Georgian War and the Bal... b"I'm Trying to Get a Sense of This Whole Geor... ... b'U.S. troops still in Georgia (did you know t... b'Why Russias response to Georgia was right' b'Gorbachev accuses U.S. of making a "serious ... b'Russia, Georgia, and NATO: Cold War Two' b'Remember that adorable 62-year-old who led y... b'War in Georgia: The Israeli connection' b'All signs point to the US encouraging Georgi... b'Christopher King argues that the US and NATO... b'America: The New Mexico?' b"BBC NEWS | Asia-Pacific | Extinction 'by man... 3 2008-08-13 0 b' U.S. refuses Israel weapons to attack Iran:... b"When the president ordered to attack Tskhinv... b' Israel clears troops who killed Reuters cam... b'Britain\'s policy of being tough on drugs is... b'Body of 14 year old found in trunk; Latest (... b'China has moved 10 *million* quake survivors... b"Bush announces Operation Get All Up In Russi... b'Russian forces sink Georgian ships ' ... b'Elephants extinct by 2020?' b'US humanitarian missions soon in Georgia - i... b"Georgia's DDOS came from US sources" b'Russian convoy heads into Georgia, violating... b'Israeli defence minister: US against strike ... b'Gorbachev: We Had No Choice' b'Witness: Russian forces head towards Tbilisi... b' Quarter of Russians blame U.S. for conflict... b'Georgian president says US military will ta... b'2006: Nobel laureate Aleksander Solzhenitsyn... 4 2008-08-14 1 b'All the experts admit that we should legalis... b'War in South Osetia - 89 pictures made by a ... b'Swedish wrestler Ara Abrahamian throws away ... b'Russia exaggerated the death toll in South O... b'Missile That Killed 9 Inside Pakistan May Ha... b"Rushdie Condemns Random House's Refusal to P... b'Poland and US agree to missle defense deal. ... b'Will the Russians conquer Tblisi? Bet on it,... ... b'Bank analyst forecast Georgian crisis 2 days... b"Georgia confict could set back Russia's US r... b'War in the Caucasus is as much the product o... b'"Non-media" photos of South Ossetia/Georgia ... b'Georgian TV reporter shot by Russian sniper ... b'Saudi Arabia: Mother moves to block child ma... b'Taliban wages war on humanitarian aid workers' b'Russia: World "can forget about" Georgia\'s... b'Darfur rebels accuse Sudan of mounting major... b'Philippines : Peace Advocate say Muslims nee... 5 rows × 27 columns 1 数据简单预处理、划分123# 根据日期划分 训练集 测试集train = data[data['Date'] &lt; '2015-01-01']test = data[data['Date'] &gt; '2014-12-31'] 12example = train.iloc[3,10]print(example) b&quot;The commander of a Navy air reconnaissance squadron that provides the President and the defense secretary the airborne ability to command the nation&#39;s nuclear weapons has been relieved of duty&quot; 12example2 = example.lower()print(example2) b&quot;the commander of a navy air reconnaissance squadron that provides the president and the defense secretary the airborne ability to command the nation&#39;s nuclear weapons has been relieved of duty&quot; 12example3 = CountVectorizer().build_tokenizer()(example2)print(example3) [&#39;the&#39;, &#39;commander&#39;, &#39;of&#39;, &#39;navy&#39;, &#39;air&#39;, &#39;reconnaissance&#39;, &#39;squadron&#39;, &#39;that&#39;, &#39;provides&#39;, &#39;the&#39;, &#39;president&#39;, &#39;and&#39;, &#39;the&#39;, &#39;defense&#39;, &#39;secretary&#39;, &#39;the&#39;, &#39;airborne&#39;, &#39;ability&#39;, &#39;to&#39;, &#39;command&#39;, &#39;the&#39;, &#39;nation&#39;, &#39;nuclear&#39;, &#39;weapons&#39;, &#39;has&#39;, &#39;been&#39;, &#39;relieved&#39;, &#39;of&#39;, &#39;duty&#39;] 1pd.DataFrame([[x,example3.count(x)] for x in set(example3)], columns = ['Word', 'Count']) .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } Word Count 0 the 5 1 command 1 2 secretary 1 3 weapons 1 4 has 1 5 defense 1 6 commander 1 7 squadron 1 8 relieved 1 9 navy 1 10 of 2 11 air 1 12 reconnaissance 1 13 provides 1 14 president 1 15 been 1 16 to 1 17 and 1 18 ability 1 19 nation 1 20 that 1 21 duty 1 22 nuclear 1 23 airborne 1 2 基于词频的特征提取——构造词频矩阵1）构造一个字符串数组StringList，每个元素是对应行所有top特征字符串拼成的长字符串 1234trainheadlines = [] for row in range(0,len(train.index)): trainheadlines.append(' '.join(str(x) for x in train.iloc[row,2:27]))print(trainheadlines[0:1]) [&#39;b&quot;Georgia \&#39;downs two Russian warplanes\&#39; as countries move to brink of war&quot; b\&#39;BREAKING: Musharraf to be impeached.\&#39; b\&#39;Russia Today: Columns of troops roll into South Ossetia; footage from fighting (YouTube)\&#39; b\&#39;Russian tanks are moving towards the capital of South Ossetia, which has reportedly been completely destroyed by Georgian artillery fire\&#39; b&quot;Afghan children raped with \&#39;impunity,\&#39; U.N. official says - this is sick, a three year old was raped and they do nothing&quot; b\&#39;150 Russian tanks have entered South Ossetia whilst Georgia shoots down two Russian jets.\&#39; b&quot;Breaking: Georgia invades South Ossetia, Russia warned it would intervene on SO\&#39;s side&quot; b&quot;The \&#39;enemy combatent\&#39; trials are nothing but a sham: Salim Haman has been sentenced to 5 1/2 years, but will be kept longer anyway just because they feel like it.&quot; b\&#39;Georgian troops retreat from S. Osettain capital, presumably leaving several hundred people killed. [VIDEO]\&#39; b\&#39;Did the U.S. Prep Georgia for War with Russia?\&#39; b\&#39;Rice Gives Green Light for Israel to Attack Iran: Says U.S. has no veto over Israeli military ops\&#39; b\&#39;Announcing:Class Action Lawsuit on Behalf of American Public Against the FBI\&#39; b&quot;So---Russia and Georgia are at war and the NYT\&#39;s top story is opening ceremonies of the Olympics? What a fucking disgrace and yet further proof of the decline of journalism.&quot; b&quot;China tells Bush to stay out of other countries\&#39; affairs&quot; b\&#39;Did World War III start today?\&#39; b\&#39;Georgia Invades South Ossetia - if Russia gets involved, will NATO absorb Georgia and unleash a full scale war?\&#39; b\&#39;Al-Qaeda Faces Islamist Backlash\&#39; b\&#39;Condoleezza Rice: &quot;The US would not act to prevent an Israeli strike on Iran.&quot; Israeli Defense Minister Ehud Barak: &quot;Israel is prepared for uncompromising victory in the case of military hostilities.&quot;\&#39; b\&#39;This is a busy day: The European Union has approved new sanctions against Iran in protest at its nuclear programme.\&#39; b&quot;Georgia will withdraw 1,000 soldiers from Iraq to help fight off Russian forces in Georgia\&#39;s breakaway region of South Ossetia&quot; b\&#39;Why the Pentagon Thinks Attacking Iran is a Bad Idea - US News &amp;amp; World Report\&#39; b\&#39;Caucasus in crisis: Georgia invades South Ossetia\&#39; b\&#39;Indian shoe manufactory - And again in a series of &quot;you do not like your work?&quot;\&#39; b\&#39;Visitors Suffering from Mental Illnesses Banned from Olympics\&#39; b&quot;No Help for Mexico\&#39;s Kidnapping Surge&quot;&#39;] 2）将这个字符串数组 转换成 词频矩阵，以便可以作为训练集 123basicvectorizer = CountVectorizer()basictrain = basicvectorizer.fit_transform(trainheadlines)print(basictrain.shape) # 生成了一个词频矩阵，总共1611个样本，31675个不重复的单词 (1611, 31675) 3 用逻辑回归 进行训练，查看训练结果精度 和 每个单词的权重参数ceof_123# 逻辑回归 fit 训练集词频矩阵basicmodel = LogisticRegression()basicmodel = basicmodel.fit(basictrain, train["Label"]) 123456testheadlines = []for row in range(0,len(test.index)): testheadlines.append(' '.join(str(x) for x in test.iloc[row,2:27]))basictest = basicvectorizer.transform(testheadlines)# 逻辑回归 predict 测试集词频矩阵predictions = basicmodel.predict(basictest) 123# 构造简易 混淆矩阵pd.crosstab(test["Label"], predictions, rownames=["Actual"], colnames=["Predicted"])#0.42 .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } Predicted 0 1 Actual 0 61 125 1 92 100 1观察：用精度做的混淆矩阵，精度只有42% 不理想 123456basicwords = basicvectorizer.get_feature_names() # 得到分词模型中所有单词（特征）basiccoeffs = basicmodel.coef_.tolist()[0] # 得到logistic模型中所有单词对应的 权重参数coeffdf = pd.DataFrame(&#123;'Word' : basicwords, 'Coefficient' : basiccoeffs&#125;)coeffdf = coeffdf.sort_values(['Coefficient', 'Word'], ascending=[0, 1]) # 从大到小排序coeffdf.head(10) # 前面的正相关 Coefficient Word 19419 0.497924 nigeria 25261 0.452526 self 29286 0.428011 tv 15998 0.425863 korea 20135 0.425716 olympics 15843 0.411636 kills 26323 0.411267 so 29256 0.394855 turn 10874 0.388555 fears 28274 0.384031 territory 1coeffdf.tail(10) # 前面的负相关 Coefficient Word 27299 -0.424441 students 8478 -0.427079 did 6683 -0.431925 congo 12818 -0.444069 hacking 7139 -0.448570 country 16949 -0.463116 low 3651 -0.470454 begin 25433 -0.494555 sex 24754 -0.549725 sanctions 24542 -0.587794 run 4 改进特征选择方法。用2个单词的词组 进行分词提取特征，构造新的频率矩阵12advancedvectorizer = CountVectorizer(ngram_range=(2,2))advancedtrain = advancedvectorizer.fit_transform(trainheadlines) 1print(advancedtrain.shape) (1611, 366721) 12advancedmodel = LogisticRegression()advancedmodel = advancedmodel.fit(advancedtrain, train["Label"]) 12345testheadlines = []for row in range(0,len(test.index)): testheadlines.append(' '.join(str(x) for x in test.iloc[row,2:27]))advancedtest = advancedvectorizer.transform(testheadlines)advpredictions = advancedmodel.predict(advancedtest) 12pd.crosstab(test["Label"], advpredictions, rownames=["Actual"], colnames=["Predicted"])#.57 Predicted 0 1 Actual 0 66 120 1 45 147 123456advwords = advancedvectorizer.get_feature_names()advcoeffs = advancedmodel.coef_.tolist()[0]advcoeffdf = pd.DataFrame(&#123;'Words' : advwords, 'Coefficient' : advcoeffs&#125;)advcoeffdf = advcoeffdf.sort_values(['Coefficient', 'Words'], ascending=[0, 1])advcoeffdf.head(10) Coefficient Words 272047 0.286533 right to 24710 0.275274 and other 285392 0.274698 set to 316194 0.262873 the first 157511 0.227943 in china 159522 0.224184 in south 125870 0.219130 found in 124411 0.216726 forced to 173246 0.211137 it has 322590 0.209239 this is 1advcoeffdf.tail(10) Coefficient Words 326846 -0.198495 to help 118707 -0.201654 fire on 155038 -0.209702 if he 242528 -0.211303 people are 31669 -0.213362 around the 321333 -0.215699 there is 327113 -0.221812 to kill 340714 -0.226289 up in 358917 -0.227516 with iran 315485 -0.331153 the country 12]]></content>
      <categories>
        <category>machine_learning_in_action</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>CountVectorizer</tag>
        <tag>LogisticRegression</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PCA 手写主成分分析]]></title>
    <url>%2F2018%2F04%2F19%2Fmachine_learning_in_action%2FPCA%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[PCA 手写主成分分析1234import numpy as npimport pandas as pddf = pd.read_csv('iris.data')df.head() .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } 5.1 3.5 1.4 0.2 Iris-setosa 0 4.9 3.0 1.4 0.2 Iris-setosa 1 4.7 3.2 1.3 0.2 Iris-setosa 2 4.6 3.1 1.5 0.2 Iris-setosa 3 5.0 3.6 1.4 0.2 Iris-setosa 4 5.4 3.9 1.7 0.4 Iris-setosa 1 数据预处理123#加上列名df.columns=['sepal_len', 'sepal_wid', 'petal_len', 'petal_wid', 'class']df.head() .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } sepal_len sepal_wid petal_len petal_wid class 0 4.9 3.0 1.4 0.2 Iris-setosa 1 4.7 3.2 1.3 0.2 Iris-setosa 2 4.6 3.1 1.5 0.2 Iris-setosa 3 5.0 3.6 1.4 0.2 Iris-setosa 4 5.4 3.9 1.7 0.4 Iris-setosa 2 画图，进行降维特征分析1234# split data table into data X and class labels yX = df.iloc[:,0:4].valuesy = df.iloc[:,4].values 123456789101112131415161718192021222324252627# 把每个特征用于分类的结果，都画成条形图，观察哪个特征更容易划分种类from matplotlib import pyplot as pltimport mathlabel_dict = &#123;1: 'Iris-Setosa', 2: 'Iris-Versicolor', 3: 'Iris-Virgnica'&#125;feature_dict = &#123;0: 'sepal length [cm]', 1: 'sepal width [cm]', 2: 'petal length [cm]', 3: 'petal width [cm]'&#125;plt.figure(figsize=(8, 6))for cnt in range(4): plt.subplot(2, 2, cnt+1) for lab in ('Iris-setosa', 'Iris-versicolor', 'Iris-virginica'): plt.hist(X[y==lab, cnt], label=lab, bins=10, alpha=0.3,) plt.xlabel(feature_dict[cnt]) plt.legend(loc='upper right', fancybox=True, fontsize=8)plt.tight_layout()plt.show() 123# 特征 归一化from sklearn.preprocessing import StandardScalerX_std = StandardScaler().fit_transform(X) 3 协方差分析（发现有2个有用特征，决定从4维降到2维）1 计算样本X的 协方差矩阵（有4个特征，所以是4x4）1234# 自己算 协方差矩阵mean_vec = np.mean(X_std, axis=0)cov_mat = (X_std - mean_vec).T.dot((X_std - mean_vec)) / (X_std.shape[0]-1)print('Covariance matrix \n%s' %cov_mat) Covariance matrix [[ 1.00675676 -0.10448539 0.87716999 0.82249094] [-0.10448539 1.00675676 -0.41802325 -0.35310295] [ 0.87716999 -0.41802325 1.00675676 0.96881642] [ 0.82249094 -0.35310295 0.96881642 1.00675676]] 12# numpy算 协方差矩阵print('NumPy covariance matrix: \n%s' %np.cov(X_std.T)) NumPy covariance matrix: [[ 1.00675676 -0.10448539 0.87716999 0.82249094] [-0.10448539 1.00675676 -0.41802325 -0.35310295] [ 0.87716999 -0.41802325 1.00675676 0.96881642] [ 0.82249094 -0.35310295 0.96881642 1.00675676]] 2 对协方差矩阵进行 特征值分解123456cov_mat = np.cov(X_std.T)eig_vals, eig_vecs = np.linalg.eig(cov_mat)print('Eigenvectors \n%s' %eig_vecs)print('\nEigenvalues \n%s' %eig_vals) Eigenvectors [[ 0.52308496 -0.36956962 -0.72154279 0.26301409] [-0.25956935 -0.92681168 0.2411952 -0.12437342] [ 0.58184289 -0.01912775 0.13962963 -0.80099722] [ 0.56609604 -0.06381646 0.63380158 0.52321917]] Eigenvalues [ 2.92442837 0.93215233 0.14946373 0.02098259] 3 把特征值从大到小排列，并配对特征向量1234567891011# Make a list of (eigenvalue, eigenvector) tupleseig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:,i]) for i in range(len(eig_vals))]print (eig_pairs)print ('----------')# Sort the (eigenvalue, eigenvector) tuples from high to loweig_pairs.sort(key=lambda x: x[0], reverse=True)# Visually confirm that the list is correctly sorted by decreasing eigenvaluesprint('Eigenvalues in descending order:')for i in eig_pairs: print(i[0],"对应",i[1]) [(2.9244283691111144, array([ 0.52308496, -0.25956935, 0.58184289, 0.56609604])), (0.93215233025350641, array([-0.36956962, -0.92681168, -0.01912775, -0.06381646])), (0.14946373489813314, array([-0.72154279, 0.2411952 , 0.13962963, 0.63380158])), (0.020982592764270606, array([ 0.26301409, -0.12437342, -0.80099722, 0.52321917]))] ---------- Eigenvalues in descending order: 2.92442836911 对应 [ 0.52308496 -0.25956935 0.58184289 0.56609604] 0.932152330254 对应 [-0.36956962 -0.92681168 -0.01912775 -0.06381646] 0.149463734898 对应 [-0.72154279 0.2411952 0.13962963 0.63380158] 0.0209825927643 对应 [ 0.26301409 -0.12437342 -0.80099722 0.52321917] 4 通过前面特征值累加所占比重 的图像，判断取前多少特征值合适，组成投影矩阵W12345tot = sum(eig_vals)var_exp = [(i / tot)*100 for i in sorted(eig_vals, reverse=True)]print (var_exp)cum_var_exp = np.cumsum(var_exp)cum_var_exp [72.620033326920336, 23.147406858644135, 3.7115155645845164, 0.52104424985101538] array([ 72.62003333, 95.76744019, 99.47895575, 100. ]) 1234a = np.array([1,2,3,4])print (a)print ('-----------')print (np.cumsum(a)) [1 2 3 4] ----------- [ 1 3 6 10] 123456789101112plt.figure(figsize=(6, 4))plt.bar(range(4), var_exp, alpha=0.5, align='center', label='individual explained variance')plt.step(range(4), cum_var_exp, where='mid', label='cumulative explained variance')plt.ylabel('Explained variance ratio')plt.xlabel('Principal components')plt.legend(loc='best')plt.tight_layout()plt.show() 1234matrix_w = np.hstack((eig_pairs[0][1].reshape(4,1), eig_pairs[1][1].reshape(4,1)))print('Matrix W:\n', matrix_w) Matrix W: [[ 0.52308496 -0.36956962] [-0.25956935 -0.92681168] [ 0.58184289 -0.01912775] [ 0.56609604 -0.06381646]] 4 开始降维——用投影矩阵降维样本矩阵X1Y = X_std.dot(matrix_w) 5 画图观察 降维前 和 降维后的样本分布123456789101112plt.figure(figsize=(6, 4))for lab, col in zip(('Iris-setosa', 'Iris-versicolor', 'Iris-virginica'), ('blue', 'red', 'green')): plt.scatter(X[y==lab, 0], X[y==lab, 1], label=lab, c=col)plt.xlabel('sepal_len')plt.ylabel('sepal_wid')plt.legend(loc='best')plt.tight_layout()plt.show() 123456789101112plt.figure(figsize=(6, 4))for lab, col in zip(('Iris-setosa', 'Iris-versicolor', 'Iris-virginica'), ('blue', 'red', 'green')): plt.scatter(Y[y==lab, 0], Y[y==lab, 1], label=lab, c=col)plt.xlabel('Principal Component 1')plt.ylabel('Principal Component 2')plt.legend(loc='lower center')plt.tight_layout()plt.show() 12 12 12 12]]></content>
      <categories>
        <category>machine_learning_in_action</category>
      </categories>
      <tags>
        <tag>PCA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pandas常用功能、函数]]></title>
    <url>%2F2018%2F04%2F19%2Fpandas%2FPandas%E5%B8%B8%E7%94%A8%E5%8A%9F%E8%83%BD%E3%80%81%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"></content>
      <categories>
        <category>pandas</category>
      </categories>
      <tags>
        <tag>pandas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[贷款分析]]></title>
    <url>%2F2018%2F04%2F19%2Fmachine_learning_in_action%2F%E8%B4%B7%E6%AC%BE%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[123456import pandas as pdloans_2007 = pd.read_csv('LoanStats3a.csv', skiprows=1)half_count = len(loans_2007) / 2loans_2007 = loans_2007.dropna(thresh=half_count, axis=1)loans_2007 = loans_2007.drop(['desc', 'url'],axis=1)loans_2007.to_csv('loans_2007.csv', index=False) D:\Anaconda3\lib\site-packages\IPython\core\interactiveshell.py:2717: DtypeWarning: Columns (0,47) have mixed types. Specify dtype option on import or set low_memory=False. interactivity=interactivity, compiler=compiler, result=result) 12345import pandas as pdloans_2007 = pd.read_csv("loans_2007.csv")#loans_2007.drop_duplicates()print(loans_2007.iloc[0].head(15))print(loans_2007.shape[1]) #共52个特征 id 1077501 member_id 1.2966e+06 loan_amnt 5000 funded_amnt 5000 funded_amnt_inv 4975 term 36 months int_rate 10.65% installment 162.87 grade B sub_grade B2 emp_title NaN emp_length 10+ years home_ownership RENT annual_inc 24000 verification_status Verified Name: 0, dtype: object 52 D:\Anaconda3\lib\site-packages\IPython\core\interactiveshell.py:2717: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False. interactivity=interactivity, compiler=compiler, result=result) 预处理——数据清洗先直观上 去掉不需要的特征。包括： 1.预测后才出现的特征（比如：实际发放的贷款）、2.高度相关的特征（比如：123和ABC）、3。关系不大的特征（比如：ID等） 12loans_2007 = loans_2007.drop(["id", "member_id", "funded_amnt", "funded_amnt_inv", "grade", "sub_grade", "emp_title", "issue_d"], axis=1)loans_2007 = loans_2007.drop(["zip_code", "out_prncp", "out_prncp_inv", "total_pymnt", "total_pymnt_inv", "total_rec_prncp"], axis=1) 123loans_2007 = loans_2007.drop(["total_rec_int", "total_rec_late_fee", "recoveries", "collection_recovery_fee", "last_pymnt_d", "last_pymnt_amnt"], axis=1)print(loans_2007.iloc[0])print(loans_2007.shape[1]) loan_amnt 5000 term 36 months int_rate 10.65% installment 162.87 emp_length 10+ years home_ownership RENT annual_inc 24000 verification_status Verified loan_status Fully Paid pymnt_plan n purpose credit_card title Computer addr_state AZ dti 27.65 delinq_2yrs 0 earliest_cr_line Jan-1985 inq_last_6mths 1 open_acc 3 pub_rec 0 revol_bal 13648 revol_util 83.7% total_acc 9 initial_list_status f last_credit_pull_d Nov-2016 collections_12_mths_ex_med 0 policy_code 1 application_type INDIVIDUAL acc_now_delinq 0 chargeoff_within_12_mths 0 delinq_amnt 0 pub_rec_bankruptcies 0 tax_liens 0 Name: 0, dtype: object 32 预处理——label列属性数字替换对label属性进行统计，选择适合用于分类的属性 1print(loans_2007['loan_status'].value_counts()) Fully Paid 33902 Charged Off 5658 Does not meet the credit policy. Status:Fully Paid 1988 Does not meet the credit policy. Status:Charged Off 761 Current 201 Late (31-120 days) 10 In Grace Period 9 Late (16-30 days) 5 Default 1 Name: loan_status, dtype: int64 12345678910111213# 属性为Fully Paid 和 Charged off 的替换为 1 和 0# 只取这部分数据loans_2007 = loans_2007[(loans_2007['loan_status'] == "Fully Paid") | (loans_2007['loan_status'] == "Charged Off")]# 替换指定列的 指定属性为 指定的值★status_replace = &#123; "loan_status" : &#123; "Fully Paid": 1, "Charged Off": 0, &#125;&#125;loans_2007 = loans_2007.replace(status_replace) 预处理——最后再去掉列属性只有一个的列123456789101112#let's look for any columns that contain only one unique value and remove themorig_columns = loans_2007.columnsdrop_columns = []for col in orig_columns: col_series = loans_2007[col].dropna().unique() #这里要去掉空值nan以后 再判定列的属性是否只有一个 if len(col_series) == 1: drop_columns.append(col)loans_2007 = loans_2007.drop(drop_columns, axis=1)print(drop_columns)print(loans_2007.shape)loans_2007.to_csv('filtered_loans_2007.csv', index=False) [&#39;initial_list_status&#39;, &#39;collections_12_mths_ex_med&#39;, &#39;policy_code&#39;, &#39;application_type&#39;, &#39;acc_now_delinq&#39;, &#39;chargeoff_within_12_mths&#39;, &#39;delinq_amnt&#39;, &#39;tax_liens&#39;] (39560, 24) 处理完毕，最终得到24列数据 预处理——缺失值先查找列缺失值多的，去掉这些列，只剩下缺失值少的列。之后直接去掉那些样本行即可 1234import pandas as pdloans = pd.read_csv('filtered_loans_2007.csv')null_counts = loans.isnull().sum()print(null_counts) loan_amnt 0 term 0 int_rate 0 installment 0 emp_length 0 home_ownership 0 annual_inc 0 verification_status 0 loan_status 0 pymnt_plan 0 purpose 0 title 10 addr_state 0 dti 0 delinq_2yrs 0 earliest_cr_line 0 inq_last_6mths 0 open_acc 0 pub_rec 0 revol_bal 0 revol_util 50 total_acc 0 last_credit_pull_d 2 pub_rec_bankruptcies 697 dtype: int64 12loans = loans.drop("pub_rec_bankruptcies", axis=1)loans = loans.dropna(axis=0) object 12 float64 10 int64 1 dtype: int64 12# 处理完缺失值，统计每种数据类型的列 有几个print(loans.dtypes.value_counts()) 预处理——字符串值转换12object_columns_df = loans.select_dtypes(include=["object"])print(object_columns_df.iloc[0]) term 36 months int_rate 10.65% emp_length 10+ years home_ownership RENT verification_status Verified pymnt_plan n purpose credit_card title Computer addr_state AZ earliest_cr_line Jan-1985 revol_util 83.7% last_credit_pull_d Nov-2016 Name: 0, dtype: object 123cols = ['home_ownership', 'verification_status', 'emp_length', 'term', 'addr_state']for c in cols: print(loans[c].value_counts()) RENT 18780 MORTGAGE 17574 OWN 3045 OTHER 96 NONE 3 Name: home_ownership, dtype: int64 Not Verified 16856 Verified 12705 Source Verified 9937 Name: verification_status, dtype: int64 10+ years 8821 &lt; 1 year 4563 2 years 4371 3 years 4074 4 years 3409 5 years 3270 1 year 3227 6 years 2212 7 years 1756 8 years 1472 9 years 1254 n/a 1069 Name: emp_length, dtype: int64 36 months 29041 60 months 10457 Name: term, dtype: int64 CA 7070 NY 3788 FL 2856 TX 2714 NJ 1838 IL 1517 PA 1504 VA 1400 GA 1393 MA 1336 OH 1208 MD 1049 AZ 874 WA 834 CO 786 NC 780 CT 747 MI 722 MO 682 MN 611 NV 492 SC 470 WI 453 AL 446 OR 445 LA 435 KY 325 OK 298 KS 269 UT 256 AR 243 DC 211 RI 198 NM 188 WV 176 HI 172 NH 172 DE 113 MT 84 WY 83 AK 79 SD 63 VT 54 MS 19 TN 17 IN 9 ID 6 IA 5 NE 5 ME 3 Name: addr_state, dtype: int64 123# 这2个特征内容差不多，选择去掉title列print(loans["purpose"].value_counts())print(loans["title"].value_counts()) debt_consolidation 18533 credit_card 5099 other 3963 home_improvement 2965 major_purchase 2181 small_business 1815 car 1544 wedding 945 medical 692 moving 581 vacation 379 house 378 educational 320 renewable_energy 103 Name: purpose, dtype: int64 Debt Consolidation 2168 Debt Consolidation Loan 1706 Personal Loan 658 Consolidation 509 debt consolidation 502 Credit Card Consolidation 356 Home Improvement 354 Debt consolidation 333 Small Business Loan 322 Credit Card Loan 313 Personal 308 Consolidation Loan 255 Home Improvement Loan 246 personal loan 234 personal 220 Loan 212 Wedding Loan 209 consolidation 200 Car Loan 200 Other Loan 190 Credit Card Payoff 155 Wedding 152 Major Purchase Loan 144 Credit Card Refinance 143 Consolidate 127 Medical 122 Credit Card 117 home improvement 111 My Loan 94 Credit Cards 93 ... DebtConsolidationn 1 Freedom 1 Credit Card Consolidation Loan - SEG 1 SOLAR PV 1 Pay on Credit card 1 To pay off balloon payments due 1 Paying off the debt 1 Payoff ING PLOC 1 Josh CC Loan 1 House payoff 1 Taking care of Business 1 Gluten Free Bakery in ideal town for it 1 Startup Money for Small Business 1 FundToFinanceCar 1 getting ready for Baby 1 Dougs Wedding Loan 1 d rock 1 LC Loan 2 1 swimming pool repair 1 engagement 1 Cut the credit cards Loan 1 vinman 1 working hard to get out of debt 1 consolidate the rest of my debt 1 Medical/Vacation 1 2BDebtFree 1 Paying Off High Interest Credit Cards! 1 Baby on the way! 1 cart loan 1 Consolidaton 1 Name: title, dtype: int64 1234567891011121314151617181920mapping_dict = &#123; "emp_length": &#123; "10+ years": 10, "9 years": 9, "8 years": 8, "7 years": 7, "6 years": 6, "5 years": 5, "4 years": 4, "3 years": 3, "2 years": 2, "1 year": 1, "&lt; 1 year": 0, "n/a": 0 &#125;&#125;loans = loans.drop(["last_credit_pull_d", "earliest_cr_line", "addr_state", "title"], axis=1)loans["int_rate"] = loans["int_rate"].str.rstrip("%").astype("float")loans["revol_util"] = loans["revol_util"].str.rstrip("%").astype("float")loans = loans.replace(mapping_dict) 123456cat_columns = ["home_ownership", "verification_status", "emp_length", "purpose", "term"]dummy_df = pd.get_dummies(loans[cat_columns])loans = pd.concat([loans, dummy_df], axis=1)loans = loans.drop(cat_columns, axis=1)loans = loans.drop("pymnt_plan", axis=1) 1loans.to_csv('cleaned_loans2007.csv', index=False) 模型训练、评估、调参——训练目标是盈利最大化模型训练目标：是确保TP的数量至少是FP的10倍。然而这里不适合使用精度，故我们选择 TPR 和 FPR ，要使 TPR尽可能大， FPR 尽可能小 123import pandas as pdloans = pd.read_csv("cleaned_loans2007.csv")print(loans.info()) &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 39498 entries, 0 to 39497 Data columns (total 37 columns): loan_amnt 39498 non-null float64 int_rate 39498 non-null float64 installment 39498 non-null float64 annual_inc 39498 non-null float64 loan_status 39498 non-null int64 dti 39498 non-null float64 delinq_2yrs 39498 non-null float64 inq_last_6mths 39498 non-null float64 open_acc 39498 non-null float64 pub_rec 39498 non-null float64 revol_bal 39498 non-null float64 revol_util 39498 non-null float64 total_acc 39498 non-null float64 home_ownership_MORTGAGE 39498 non-null int64 home_ownership_NONE 39498 non-null int64 home_ownership_OTHER 39498 non-null int64 home_ownership_OWN 39498 non-null int64 home_ownership_RENT 39498 non-null int64 verification_status_Not Verified 39498 non-null int64 verification_status_Source Verified 39498 non-null int64 verification_status_Verified 39498 non-null int64 purpose_car 39498 non-null int64 purpose_credit_card 39498 non-null int64 purpose_debt_consolidation 39498 non-null int64 purpose_educational 39498 non-null int64 purpose_home_improvement 39498 non-null int64 purpose_house 39498 non-null int64 purpose_major_purchase 39498 non-null int64 purpose_medical 39498 non-null int64 purpose_moving 39498 non-null int64 purpose_other 39498 non-null int64 purpose_renewable_energy 39498 non-null int64 purpose_small_business 39498 non-null int64 purpose_vacation 39498 non-null int64 purpose_wedding 39498 non-null int64 term_ 36 months 39498 non-null int64 term_ 60 months 39498 non-null int64 dtypes: float64(12), int64(25) memory usage: 11.1 MB None 12345678910111213141516import pandas as pd# False positives.fp_filter = (predictions == 1) &amp; (loans["loan_status"] == 0)fp = len(predictions[fp_filter])# True positives.tp_filter = (predictions == 1) &amp; (loans["loan_status"] == 1)tp = len(predictions[tp_filter])# False negatives.fn_filter = (predictions == 0) &amp; (loans["loan_status"] == 1)fn = len(predictions[fn_filter])# True negativestn_filter = (predictions == 0) &amp; (loans["loan_status"] == 0)tn = len(predictions[tn_filter]) 12345678from sklearn.linear_model import LogisticRegressionlr = LogisticRegression()cols = loans.columnstrain_cols = cols.drop("loan_status")features = loans[train_cols]target = loans["loan_status"]lr.fit(features, target)predictions = lr.predict(features) 第一次，逻辑回归123456789101112131415161718192021222324252627282930from sklearn.linear_model import LogisticRegressionfrom sklearn.cross_validation import cross_val_predict, KFoldlr = LogisticRegression()kf = KFold(features.shape[0], random_state=1)predictions = cross_val_predict(lr, features, target, cv=kf)predictions = pd.Series(predictions)# False positives.fp_filter = (predictions == 1) &amp; (loans["loan_status"] == 0)fp = len(predictions[fp_filter])# True positives.tp_filter = (predictions == 1) &amp; (loans["loan_status"] == 1)tp = len(predictions[tp_filter])# False negatives.fn_filter = (predictions == 0) &amp; (loans["loan_status"] == 1)fn = len(predictions[fn_filter])# True negativestn_filter = (predictions == 0) &amp; (loans["loan_status"] == 0)tn = len(predictions[tn_filter])# Ratestpr = tp / float((tp + fn))fpr = fp / float((fp + tn))print(tpr)print(fpr)print predictions[:20] 0.999084438406 0.998049299521 0 1 1 1 2 1 3 1 4 1 5 1 6 1 7 1 8 1 9 1 10 1 11 1 12 1 13 1 14 1 15 1 16 1 17 1 18 1 19 1 dtype: int64 因为样本不均衡导致效果不好。 第二次，逻辑回归，设置模型参数 class_weight=”balanced” ，让模型自动添加惩罚权重123456789101112131415161718192021222324252627282930from sklearn.linear_model import LogisticRegressionfrom sklearn.cross_validation import cross_val_predictlr = LogisticRegression(class_weight="balanced")kf = KFold(features.shape[0], random_state=1)predictions = cross_val_predict(lr, features, target, cv=kf)predictions = pd.Series(predictions)# False positives.fp_filter = (predictions == 1) &amp; (loans["loan_status"] == 0)fp = len(predictions[fp_filter])# True positives.tp_filter = (predictions == 1) &amp; (loans["loan_status"] == 1)tp = len(predictions[tp_filter])# False negatives.fn_filter = (predictions == 0) &amp; (loans["loan_status"] == 1)fn = len(predictions[fn_filter])# True negativestn_filter = (predictions == 0) &amp; (loans["loan_status"] == 0)tn = len(predictions[tn_filter])# Ratestpr = tp / float((tp + fn))fpr = fp / float((fp + tn))print(tpr)print(fpr)print predictions[:20] 0.670781771464 0.400780280192 0 1 1 0 2 0 3 1 4 1 5 0 6 0 7 0 8 0 9 0 10 1 11 0 12 1 13 1 14 0 15 0 16 1 17 1 18 1 19 0 dtype: int64 模型终于起了效果，但还是不理想 第三次，逻辑回归，手动调节 cclass_weight 参数设置惩罚系数12345678910111213141516171819202122232425262728293031323334from sklearn.linear_model import LogisticRegressionfrom sklearn.cross_validation import cross_val_predictpenalty = &#123; 0: 5, 1: 1&#125;lr = LogisticRegression(class_weight=penalty)kf = KFold(features.shape[0], random_state=1)predictions = cross_val_predict(lr, features, target, cv=kf)predictions = pd.Series(predictions)# False positives.fp_filter = (predictions == 1) &amp; (loans["loan_status"] == 0)fp = len(predictions[fp_filter])# True positives.tp_filter = (predictions == 1) &amp; (loans["loan_status"] == 1)tp = len(predictions[tp_filter])# False negatives.fn_filter = (predictions == 0) &amp; (loans["loan_status"] == 1)fn = len(predictions[fn_filter])# True negativestn_filter = (predictions == 0) &amp; (loans["loan_status"] == 0)tn = len(predictions[tn_filter])# Ratestpr = tp / float((tp + fn))fpr = fp / float((fp + tn))print(tpr)print(fpr) 0.731799521545 0.478985635751 效果又好了一些 第四次，随机森林123456789101112131415161718192021222324252627from sklearn.ensemble import RandomForestClassifierfrom sklearn.cross_validation import cross_val_predictrf = RandomForestClassifier(n_estimators=10,class_weight="balanced", random_state=1)#print help(RandomForestClassifier)kf = KFold(features.shape[0], random_state=1)predictions = cross_val_predict(rf, features, target, cv=kf)predictions = pd.Series(predictions)# False positives.fp_filter = (predictions == 1) &amp; (loans["loan_status"] == 0)fp = len(predictions[fp_filter])# True positives.tp_filter = (predictions == 1) &amp; (loans["loan_status"] == 1)tp = len(predictions[tp_filter])# False negatives.fn_filter = (predictions == 0) &amp; (loans["loan_status"] == 1)fn = len(predictions[fn_filter])# True negativestn_filter = (predictions == 0) &amp; (loans["loan_status"] == 0)tn = len(predictions[tn_filter])# Ratestpr = tp / float((tp + fn))fpr = fp / float((fp + tn))]]></content>
      <categories>
        <category>machine_learning_in_action</category>
      </categories>
      <tags>
        <tag>pandas列属性数字替换</tag>
        <tag>pandas缺失值处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python基本函数]]></title>
    <url>%2F2018%2F04%2F19%2Fpython%2FPython%E5%9F%BA%E6%9C%AC%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[open()1简单的文件读写 123456789f = open("123.txt", "r");content = f.read();print(content);f.close();f = open("123.txt", "a+");f.write("\n");f.write("222222222222");f.close(); 2读取数据到矩阵 123456789101112#读取txt二维数据到矩阵def loadDataSet(fileName): dataMat = np.mat([0,0]) f = open(fileName) for line in f.readlines(): curLine = line.strip().split('\t') if len(curLine)==1 : continue curLineMat = np.mat(curLine) dataMat = np.vstack((dataMat, curLineMat)) #拼接矩阵 dataMat = dataMat[1:,:].astype(float) #不要第一行；转为纯数字 return dataMat os.getcwd()获得当前工作目录12import osprint os.getcwd() set()将一个字符串拆成 单个字符 组成的字符集，可进行关系测试，删除重复数据，还可以计算交集、差集、并集等。 1234567&gt;&gt;&gt; x = set('runoob') &gt;&gt;&gt; y = set('google') &gt;&gt;&gt; x, y (set(['b', 'r', 'u', 'o', 'n']), set(['e', 'o', 'g', 'l'])) # 重复的被删除 &gt;&gt;&gt; x &amp; y # 交集 set(['o']) &gt;&gt;&gt; x | y # 并集 set(['b', 'e', 'g', 'l', 'o', 'n', 'r', 'u']) &gt;&gt;&gt; x - y # 差集 set(['r', 'b', 'u', 'n']) &gt;&gt;&gt; copy() = 赋值 传引用 =》内存不独立 =》 同步跟随变化 copy 浅拷贝 只拷贝父对象 =》父对象内存独立 =》只有子对象跟随变化 deepcopy 深拷贝 拷贝对象及其子对象 =》全部内存独立 =》 不跟随变化（深拷贝 和 浅拷贝——只对数组结构有用，int之类的没用）123456789101112131415161718a = [1, 2, 3, 4, ['a', 'b']] #原始对象 b = a #赋值，传对象的引用c = copy.copy(a) #对象拷贝，浅拷贝d = copy.deepcopy(a) #对象拷贝，深拷贝 a.append(5) #修改对象aa[4].append('c') #修改对象a中的['a', 'b']数组对象 print 'a = ', aprint 'b = ', bprint 'c = ', cprint 'd = ', d输出结果：a = [1, 2, 3, 4, ['a', 'b', 'c'], 5]b = [1, 2, 3, 4, ['a', 'b', 'c'], 5]c = [1, 2, 3, 4, ['a', 'b', 'c']]d = [1, 2, 3, 4, ['a', 'b']] zip()zip函数接受任意多个序列作为参数，返回一个tuple列表123456789101112print(zip(range(3),range(5)))[(0, 0), (1, 1), (2, 2)]for i,j in zip(range(3),range(5)): print(i) print(j)001122 运算符and和or 注意：and or 是python特有的短路运算符 表达式从左至右运算，若 or 的左侧逻辑值为 True ，则短路 or 后所有的表达式（不管是 and 还是 or），直接输出 or 左侧表达式 。 表达式从左至右运算，若 and 的左侧逻辑值为 False ，则短路其后所有 and 表达式，直到有 or 出现，输出 and 左侧表达式到 or 的左侧，参与接下来的逻辑运算。 若 or 的左侧为 False ，或者 and 的左侧为 True 则不能使用短路逻辑。]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python面向对象和类]]></title>
    <url>%2F2018%2F04%2F19%2Fpython%2FPython%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%E5%92%8C%E7%B1%BB%2F</url>
    <content type="text"><![CDATA[Python 面向对象Python从设计之初就已经是一门面向对象的语言，正因为如此，在Python中创建一个类和对象是很容易的。本章节我们将详细介绍Python的面向对象编程。 如果你以前没有接触过面向对象的编程语言，那你可能需要先了解一些面向对象语言的一些基本特征，在头脑里头形成一个基本的面向对象的概念，这样有助于你更容易的学习Python的面向对象编程。 接下来我们先来简单的了解下面向对象的一些基本特征。 面向对象技术简介 类(Class): 用来描述具有相同的属性和方法的对象的集合。它定义了该集合中每个对象所共有的属性和方法。对象是类的实例。 类变量：类变量在整个实例化的对象中是公用的。类变量定义在类中且在函数体之外。类变量通常不作为实例变量使用。 数据成员：类变量或者实例变量, 用于处理类及其实例对象的相关的数据。 方法重写：如果从父类继承的方法不能满足子类的需求，可以对其进行改写，这个过程叫方法的覆盖（override），也称为方法的重写。 实例变量：定义在方法中的变量，只作用于当前实例的类。 继承：即一个派生类（derived class）继承基类（base class）的字段和方法。继承也允许把一个派生类的对象作为一个基类对象对待。例如，有这样一个设计：一个Dog类型的对象派生自Animal类，这是模拟”是一个（is-a）”关系（例图，Dog是一个Animal）。 实例化：创建一个类的实例，类的具体对象。 方法：类中定义的函数。 对象：通过类定义的数据结构实例。对象包括两个数据成员（类变量和实例变量）和方法。 创建类使用 class 语句来创建一个新类，class 之后为类的名称并以冒号结尾: 123class ClassName: '类的帮助信息' #类文档字符串 class_suite #类体 类的帮助信息可以通过ClassName.doc查看。 class_suite 由类成员，方法，数据属性组成。 实例以下是一个简单的 Python 类的例子: 实例1234567891011121314151617#!/usr/bin/python# -*- coding: UTF-8 -*- class Employee: '所有员工的基类' empCount = 0 def __init__(self, name, salary): self.name = name self.salary = salary Employee.empCount += 1 def displayCount(self): print "Total Employee %d" % Employee.empCount def displayEmployee(self): print "Name : ", self.name, ", Salary: ", self.salary empCount 变量是一个类变量，它的值将在这个类的所有实例之间共享。你可以在内部类或外部类使用 Employee.empCount 访问。 第一种方法init()方法是一种特殊的方法，被称为类的构造函数或初始化方法，当创建了这个类的实例时就会调用该方法 self 代表类的实例，self 在定义类的方法时是必须有的，虽然在调用时不必传入相应的参数。 self代表类的实例，而非类类的方法与普通的函数只有一个特别的区别——它们必须有一个额外的第一个参数名称, 按照惯例它的名称是 self。 1234567class Test: def prt(self): print(self) print(self.__class__) t = Test()t.prt() 以上实例执行结果为： 12&lt;__main__.Test instance at 0x10d066878&gt;__main__.Test 从执行结果可以很明显的看出，self 代表的是类的实例，代表当前对象的地址，而 self.class 则指向类。 self 不是 python 关键字，我们把他换成 runoob 也是可以正常执行的: 实例1234567class Test: def prt(runoob): print(runoob) print(runoob.__class__) t = Test()t.prt() 以上实例执行结果为： 12&lt;__main__.Test instance at 0x10d066878&gt;__main__.Test 创建实例对象实例化类其他编程语言中一般用关键字 new，但是在 Python 中并没有这个关键字，类的实例化类似函数调用方式。 以下使用类的名称 Employee 来实例化，并通过 init 方法接收参数。 1234"创建 Employee 类的第一个对象"emp1 = Employee("Zara", 2000)"创建 Employee 类的第二个对象"emp2 = Employee("Manni", 5000) 访问属性您可以使用点号 . 来访问对象的属性。使用如下类的名称访问类变量: 123emp1.displayEmployee()emp2.displayEmployee()print "Total Employee %d" % Employee.empCount 完整实例： 实例12345678910111213141516171819202122class Employee: '所有员工的基类' empCount = 0 def __init__(self, name, salary): self.name = name self.salary = salary Employee.empCount += 1 def displayCount(self): print "Total Employee %d" % Employee.empCount def displayEmployee(self): print "Name : ", self.name, ", Salary: ", self.salary "创建 Employee 类的第一个对象"emp1 = Employee("Zara", 2000)"创建 Employee 类的第二个对象"emp2 = Employee("Manni", 5000)emp1.displayEmployee()emp2.displayEmployee()print "Total Employee %d" % Employee.empCount 执行以上代码输出结果如下： 123Name : Zara ,Salary: 2000Name : Manni ,Salary: 5000Total Employee 2 你可以添加，删除，修改类的属性，如下所示： 123emp1.age = 7 # 添加一个 'age' 属性emp1.age = 8 # 修改 'age' 属性del emp1.age # 删除 'age' 属性 你也可以使用以下函数的方式来访问属性： getattr(obj, name[, default]) : 访问对象的属性。 hasattr(obj,name) : 检查是否存在一个属性。 setattr(obj,name,value) : 设置一个属性。如果属性不存在，会创建一个新属性。 delattr(obj, name) : 删除属性。 12345hasattr(emp1, 'age') # 如果存在 'age' 属性返回 True。getattr(emp1, 'age') # 返回 'age' 属性的值setattr(emp1, 'age', 8) # 添加属性 'age' 值为 8delattr(emp1, 'age') # 删除属性 'age' Python内置类属性 dict : 类的属性（包含一个字典，由类的数据属性组成） doc :类的文档字符串 name: 类名 module: 类定义所在的模块（类的全名是’main.className’，如果类位于一个导入模块mymod中，那么className.module 等于 mymod） bases : 类的所有父类构成元素（包含了一个由所有父类组成的元组） Python内置类属性调用实例如下： 实例1234567891011121314151617181920class Employee: '所有员工的基类' empCount = 0 def __init__(self, name, salary): self.name = name self.salary = salary Employee.empCount += 1 def displayCount(self): print "Total Employee %d" % Employee.empCount def displayEmployee(self): print "Name : ", self.name, ", Salary: ", self.salary print "Employee.__doc__:", Employee.__doc__print "Employee.__name__:", Employee.__name__print "Employee.__module__:", Employee.__module__print "Employee.__bases__:", Employee.__bases__print "Employee.__dict__:", Employee.__dict__ 执行以上代码输出结果如下： 12345Employee.__doc__: 所有员工的基类Employee.__name__: EmployeeEmployee.__module__: __main__Employee.__bases__: ()Employee.__dict__: &#123;'__module__': '__main__', 'displayCount': &lt;function displayCount at 0x10a939c80&gt;, 'empCount': 0, 'displayEmployee': &lt;function displayEmployee at 0x10a93caa0&gt;, '__doc__': '\xe6\x89\x80\xe6\x9c\x89\xe5\x91\x98\xe5\xb7\xa5\xe7\x9a\x84\xe5\x9f\xba\xe7\xb1\xbb', '__init__': &lt;function __init__ at 0x10a939578&gt;&#125; python对象销毁(垃圾回收)Python 使用了引用计数这一简单技术来跟踪和回收垃圾。 在 Python 内部记录着所有使用中的对象各有多少引用。 一个内部跟踪变量，称为一个引用计数器。 当对象被创建时， 就创建了一个引用计数， 当这个对象不再需要时， 也就是说， 这个对象的引用计数变为0 时， 它被垃圾回收。但是回收不是”立即”的， 由解释器在适当的时机，将垃圾对象占用的内存空间回收。 1234567a = 40 # 创建对象 &lt;40&gt;b = a # 增加引用， &lt;40&gt; 的计数c = [b] # 增加引用. &lt;40&gt; 的计数del a # 减少引用 &lt;40&gt; 的计数b = 100 # 减少引用 &lt;40&gt; 的计数c[0] = -1 # 减少引用 &lt;40&gt; 的计数 垃圾回收机制不仅针对引用计数为0的对象，同样也可以处理循环引用的情况。循环引用指的是，两个对象相互引用，但是没有其他变量引用他们。这种情况下，仅使用引用计数是不够的。Python 的垃圾收集器实际上是一个引用计数器和一个循环垃圾收集器。作为引用计数的补充， 垃圾收集器也会留心被分配的总量很大（及未通过引用计数销毁的那些）的对象。 在这种情况下， 解释器会暂停下来， 试图清理所有未引用的循环。 实例析构函数 del ，del在对象销毁的时候被调用，当对象不再被使用时，del方法运行： 实例123456789101112131415class Point: def __init__( self, x=0, y=0): self.x = x self.y = y def __del__(self): class_name = self.__class__.__name__ print class_name, "销毁" pt1 = Point()pt2 = pt1pt3 = pt1print id(pt1), id(pt2), id(pt3) # 打印对象的iddel pt1del pt2del pt3 以上实例运行结果如下： 123083401324 3083401324 3083401324Point 销毁 注意：通常你需要在单独的文件中定义一个类， 类的继承面向对象的编程带来的主要好处之一是代码的重用，实现这种重用的方法之一是通过继承机制。继承完全可以理解成类之间的类型和子类型关系。 需要注意的地方：继承语法 class 派生类名（基类名）：//… 基类名写在括号里，基本类是在类定义的时候，在元组之中指明的。 在python中继承中的一些特点： 1：在继承中基类的构造（init()方法）不会被自动调用，它需要在其派生类的构造中亲自专门调用。 2：在调用基类的方法时，需要加上基类的类名前缀，且需要带上self参数变量。区别在于类中调用普通函数时并不需要带上self参数 3：Python总是首先查找对应类型的方法，如果它不能在派生类中找到对应的方法，它才开始到基类中逐个查找。（先在本类中查找调用的方法，找不到才去基类中找）。 如果在继承元组中列了一个以上的类，那么它就被称作”多重继承” 。 语法： 派生类的声明，与他们的父类类似，继承的基类列表跟在类名之后，如下所示： 123class SubClassName (ParentClass1[, ParentClass2, ...]): 'Optional class documentation string' class_suite 实例1234567891011121314151617181920212223242526class Parent: # 定义父类 parentAttr = 100 def __init__(self): print "调用父类构造函数" def parentMethod(self): print '调用父类方法' def setAttr(self, attr): Parent.parentAttr = attr def getAttr(self): print "父类属性 :", Parent.parentAttr class Child(Parent): # 定义子类 def __init__(self): print "调用子类构造方法" def childMethod(self): print '调用子类方法' c = Child() # 实例化子类c.childMethod() # 调用子类的方法c.parentMethod() # 调用父类方法c.setAttr(200) # 再次调用父类的方法 - 设置属性值c.getAttr() # 再次调用父类的方法 - 获取属性值 以上代码执行结果如下： 1234调用子类构造方法调用子类方法调用父类方法父类属性 : 200 你可以继承多个类 12345678class A: # 定义类 A.....class B: # 定义类 B.....class C(A, B): # 继承类 A 和 B..... 你可以使用issubclass()或者isinstance()方法来检测。 issubclass() - 布尔函数判断一个类是另一个类的子类或者子孙类，语法：issubclass(sub,sup) isinstance(obj, Class) 布尔函数如果obj是Class类的实例对象或者是一个Class子类的实例对象则返回true。 方法重写如果你的父类方法的功能不能满足你的需求，你可以在子类重写你父类的方法： 实例： 实例12345678910class Parent: # 定义父类 def myMethod(self): print '调用父类方法' class Child(Parent): # 定义子类 def myMethod(self): print '调用子类方法' c = Child() # 子类实例c.myMethod() # 子类调用重写方法 执行以上代码输出结果如下： 1调用子类方法 基础重载方法下表列出了一些通用的功能，你可以在自己的类重写： 序号 方法, 描述 &amp; 简单的调用 1 init ( self [,args…] )构造函数简单的调用方法: obj = className(args) 2 del( self )析构方法, 删除一个对象简单的调用方法 : del obj 3 repr( self )转化为供解释器读取的形式简单的调用方法 : repr(obj) 4 str( self )用于将值转化为适于人阅读的形式简单的调用方法 : str(obj) 5 cmp ( self, x )对象比较简单的调用方法 : cmp(obj, x) 运算符重载Python同样支持运算符重载，实例如下： 实例1234567891011121314class Vector: def __init__(self, a, b): self.a = a self.b = b def __str__(self): return 'Vector (%d, %d)' % (self.a, self.b) def __add__(self,other): return Vector(self.a + other.a, self.b + other.b) v1 = Vector(2,10)v2 = Vector(5,-2)print v1 + v2 以上代码执行结果如下所示: 1Vector(7,8) 类属性与方法类的私有属性__private_attrs：两个下划线开头，声明该属性为私有，不能在类的外部被使用或直接访问。在类内部的方法中使用时 self.__private_attrs。 类的方法在类的内部，使用 def 关键字可以为类定义一个方法，与一般函数定义不同，类方法必须包含参数 self,且为第一个参数 类的私有方法__private_method：两个下划线开头，声明该方法为私有方法，不能在类地外部调用。在类的内部调用 self.__private_methods 实例1234567891011121314class JustCounter: __secretCount = 0 # 私有变量 publicCount = 0 # 公开变量 def count(self): self.__secretCount += 1 self.publicCount += 1 print self.__secretCount counter = JustCounter()counter.count()counter.count()print counter.publicCountprint counter.__secretCount # 报错，实例不能访问私有变量 Python 通过改变名称来包含类名: 1234567122Traceback (most recent call last): File "test.py", line 17, in &lt;module&gt; print counter.__secretCount # 报错，实例不能访问私有变量AttributeError: JustCounter instance has no attribute '__secretCount' Python不允许实例化的类访问私有数据，但你可以使用 object._className__attrName 访问属性，将如下代码替换以上代码的最后一行代码： 12.........................print counter._JustCounter__secretCount 执行以上代码，执行结果如下： 12341222 单下划线、双下划线、头尾双下划线说明： foo: 定义的是特殊方法，一般是系统定义名字 ，类似 init() 之类的。 _foo: 以单下划线开头的表示的是 protected 类型的变量，即保护类型只能允许其本身与子类进行访问，不能用于 from module import * __foo: 双下划线的表示的是私有类型(private)的变量, 只能是允许这个类本身进行访问了。]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据挖掘——信用卡欺诈检测]]></title>
    <url>%2F2018%2F04%2F19%2Fmachine_learning_in_action%2FCreditCard%2F</url>
    <content type="text"><![CDATA[案例：用 逻辑回归 预测 信用卡欺诈12345import pandas as pdimport matplotlib.pyplot as pltimport numpy as np%matplotlib inline 12data = pd.read_csv("creditcard.csv")data.head() .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } Time V1 V2 V3 V4 V5 V6 V7 V8 V9 ... V21 V22 V23 V24 V25 V26 V27 V28 Amount Class 0 0.0 -1.359807 -0.072781 2.536347 1.378155 -0.338321 0.462388 0.239599 0.098698 0.363787 ... -0.018307 0.277838 -0.110474 0.066928 0.128539 -0.189115 0.133558 -0.021053 149.62 0 1 0.0 1.191857 0.266151 0.166480 0.448154 0.060018 -0.082361 -0.078803 0.085102 -0.255425 ... -0.225775 -0.638672 0.101288 -0.339846 0.167170 0.125895 -0.008983 0.014724 2.69 0 2 1.0 -1.358354 -1.340163 1.773209 0.379780 -0.503198 1.800499 0.791461 0.247676 -1.514654 ... 0.247998 0.771679 0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752 378.66 0 3 1.0 -0.966272 -0.185226 1.792993 -0.863291 -0.010309 1.247203 0.237609 0.377436 -1.387024 ... -0.108300 0.005274 -0.190321 -1.175575 0.647376 -0.221929 0.062723 0.061458 123.50 0 4 2.0 -1.158233 0.877737 1.548718 0.403034 -0.407193 0.095921 0.592941 -0.270533 0.817739 ... -0.009431 0.798278 -0.137458 0.141267 -0.206010 0.502292 0.219422 0.215153 69.99 0 5 rows × 31 columns 1数据预处理——归一化、去掉不用的列123456789count_classes = pd.value_counts(data['Class'], sort = True).sort_index()count_classes.plot(kind = 'bar')plt.title("Fraud class histogram")plt.xlabel("Class")plt.ylabel("Frequency")#发现样本分布十分不均衡#策略：统一不同类别样本总数 #1）oversample——过采样，把少的增多 #2) undersample——欠采样，把多的减少 &lt;matplotlib.text.Text at 0x5d32f27ef0&gt; 123456#用sklearn 函数来进行归一化(自带合并到原dataframe功能)from sklearn.preprocessing import StandardScaler data['normAmount'] = StandardScaler().fit_transform(data['Amount'].values.reshape(-1, 1))data = data.drop(['Time','Amount'],axis=1)data.head() .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } V1 V2 V3 V4 V5 V6 V7 V8 V9 V10 ... V21 V22 V23 V24 V25 V26 V27 V28 Class normAmount 0 -1.359807 -0.072781 2.536347 1.378155 -0.338321 0.462388 0.239599 0.098698 0.363787 0.090794 ... -0.018307 0.277838 -0.110474 0.066928 0.128539 -0.189115 0.133558 -0.021053 0 0.244964 1 1.191857 0.266151 0.166480 0.448154 0.060018 -0.082361 -0.078803 0.085102 -0.255425 -0.166974 ... -0.225775 -0.638672 0.101288 -0.339846 0.167170 0.125895 -0.008983 0.014724 0 -0.342475 2 -1.358354 -1.340163 1.773209 0.379780 -0.503198 1.800499 0.791461 0.247676 -1.514654 0.207643 ... 0.247998 0.771679 0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752 0 1.160686 3 -0.966272 -0.185226 1.792993 -0.863291 -0.010309 1.247203 0.237609 0.377436 -1.387024 -0.054952 ... -0.108300 0.005274 -0.190321 -1.175575 0.647376 -0.221929 0.062723 0.061458 0 0.140534 4 -1.158233 0.877737 1.548718 0.403034 -0.407193 0.095921 0.592941 -0.270533 0.817739 0.753074 ... -0.009431 0.798278 -0.137458 0.141267 -0.206010 0.502292 0.219422 0.215153 0 -0.073403 5 rows × 30 columns 1数据预处理——解决样本分布不均衡问题之undersample1234567891011121314151617181920212223242526272829303132# 方式一：采用“undersample”构建模型#把数据集切分为 样本 和 标记 存变量X = data.loc[:, data.columns != 'Class']y = data.loc[:, data.columns == 'Class']#计算欺诈样本总数number_records_fraud = len(data[data.Class == 1]) #取得欺诈行为的样本indexfraud_indices = np.array(data[data.Class == 1].index) #取得正常的样本indexnormal_indices = data[data.Class == 0].index # 随机选出 和 欺诈类数量相同的 正常Indexrandom_normal_indices = np.random.choice(normal_indices, number_records_fraud, replace = False)random_normal_indices = np.array(random_normal_indices)# 合并取得的两组index，作为欠采样indexunder_sample_indices = np.concatenate([fraud_indices,random_normal_indices])# 取得欠采样datasetunder_sample_data = data.iloc[under_sample_indices,:]#把undersample数据集切分为 样本 和 标记 存变量X_undersample = under_sample_data.loc[:, under_sample_data.columns != 'Class']y_undersample = under_sample_data.loc[:, under_sample_data.columns == 'Class']# 显示处理结果print("Percentage of normal transactions: ", len(under_sample_data[under_sample_data.Class == 0])/len(under_sample_data))print("Percentage of fraud transactions: ", len(under_sample_data[under_sample_data.Class == 1])/len(under_sample_data))print("Total number of transactions in resampled data: ", len(under_sample_data)) Percentage of normal transactions: 0.5 Percentage of fraud transactions: 0.5 Total number of transactions in resampled data: 984 1数据预处理——划分训练和测试集1234567891011121314151617181920#引入数据集切分函数from sklearn.cross_validation import train_test_split# Whole dataset 划分全部数据X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.3, random_state = 0)print("Number transactions train dataset: ", len(X_train))print("Number transactions test dataset: ", len(X_test))print("Total number of transactions: ", len(X_train)+len(X_test))# Undersampled dataset 划分欠采样数据X_train_undersample, X_test_undersample, y_train_undersample, y_test_undersample = train_test_split(X_undersample ,y_undersample ,test_size = 0.3 ,random_state = 0)print("")print("Number transactions train dataset: ", len(X_train_undersample))print("Number transactions test dataset: ", len(X_test_undersample))print("Total number of transactions: ", len(X_train_undersample)+len(X_test_undersample)) Number transactions train dataset: 199364 Number transactions test dataset: 85443 Total number of transactions: 284807 Number transactions train dataset: 688 Number transactions test dataset: 296 Total number of transactions: 984 2 交叉验证——在训练集上做，找最好的逻辑回归正则惩罚系数C1234#Recall = TP/(TP+FN) 这里适用召回率来检测from sklearn.linear_model import LogisticRegressionfrom sklearn.cross_validation import KFold, cross_val_scorefrom sklearn.metrics import confusion_matrix,recall_score,classification_report 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657# 自己实现召回率的 K=5的交叉验证函数（注意：此处是在训练集上的交叉验证）def printing_Kfold_scores(x_train_data,y_train_data): fold = KFold(len(y_train_data),5,shuffle=False) # Different C parameters #在sklearn里面，惩罚系数是倒数，比如100其实是0.01 #每个都试一遍，看哪个模型最好 c_param_range = [0.01,0.1,1,10,100] results_table = pd.DataFrame(index = range(len(c_param_range),2), columns = ['C_parameter','Mean recall score']) results_table['C_parameter'] = c_param_range # the k-fold will give 2 lists: train_indices = indices[0], test_indices = indices[1] j = 0 for c_param in c_param_range: print('-------------------------------------------') print('C parameter: ', c_param) print('-------------------------------------------') print('') recall_accs = [] # iteration：迭代轮数1-5 # indices：[0]表示训练集索引集合，[1]表示测试集索引集合 for iteration, indices in enumerate(fold,start=1): # Call the logistic regression model with a certain C parameter # C：指定惩罚项的参数 # penalty：指定惩罚项的算法 lr = LogisticRegression(C = c_param, penalty = 'l1') # Use the training data to fit the model. In this case, we use the portion of the fold to train the model # with indices[0]. We then predict on the portion assigned as the 'test cross validation' with indices[1] lr.fit(x_train_data.iloc[indices[0],:],y_train_data.iloc[indices[0],:].values.ravel()) # Predict values using the test indices in the training data y_pred_undersample = lr.predict(x_train_data.iloc[indices[1],:].values) # Calculate the recall score and append it to a list for recall scores representing the current c_parameter recall_acc = recall_score(y_train_data.iloc[indices[1],:].values,y_pred_undersample) recall_accs.append(recall_acc) print('Iteration ', iteration,': recall score = ', recall_acc) # The mean value of those recall scores is the metric we want to save and get hold of. results_table.loc[j,'Mean recall score'] = np.mean(recall_accs) j += 1 print('') print('Mean recall score ', np.mean(recall_accs)) print('') best_c = results_table.loc[results_table['Mean recall score'].idxmax()]['C_parameter'] # Finally, we can check which C parameter is the best amongst the chosen. print('*********************************************************************************') print('Best model to choose from cross validation is with C parameter = ', best_c) print('*********************************************************************************') return best_c 1best_c = printing_Kfold_scores(X_train_undersample,y_train_undersample) ------------------------------------------- C parameter: 0.01 ------------------------------------------- Iteration 1 : recall score = 0.931506849315 Iteration 2 : recall score = 0.931506849315 Iteration 3 : recall score = 1.0 Iteration 4 : recall score = 0.972972972973 Iteration 5 : recall score = 0.969696969697 Mean recall score 0.96113672826 ------------------------------------------- C parameter: 0.1 ------------------------------------------- Iteration 1 : recall score = 0.849315068493 Iteration 2 : recall score = 0.86301369863 Iteration 3 : recall score = 0.932203389831 Iteration 4 : recall score = 0.945945945946 Iteration 5 : recall score = 0.893939393939 Mean recall score 0.896883499368 ------------------------------------------- C parameter: 1 ------------------------------------------- Iteration 1 : recall score = 0.86301369863 Iteration 2 : recall score = 0.890410958904 Iteration 3 : recall score = 0.983050847458 Iteration 4 : recall score = 0.945945945946 Iteration 5 : recall score = 0.909090909091 Mean recall score 0.918302472006 ------------------------------------------- C parameter: 10 ------------------------------------------- Iteration 1 : recall score = 0.86301369863 Iteration 2 : recall score = 0.904109589041 Iteration 3 : recall score = 0.983050847458 Iteration 4 : recall score = 0.945945945946 Iteration 5 : recall score = 0.909090909091 Mean recall score 0.921042198033 ------------------------------------------- C parameter: 100 ------------------------------------------- Iteration 1 : recall score = 0.876712328767 Iteration 2 : recall score = 0.890410958904 Iteration 3 : recall score = 0.983050847458 Iteration 4 : recall score = 0.945945945946 Iteration 5 : recall score = 0.909090909091 Mean recall score 0.921042198033 ********************************************************************************* Best model to choose from cross validation is with C parameter = 0.01 ********************************************************************************* 3训练 + 测试——用best_C在训练集上重新训练一遍，再在undersample测试集上预测用 混淆矩阵 计算recall值1234567891011121314151617181920212223import itertoolsdef plot_confusion_matrix(cm, classes, title='Confusion matrix', cmap=plt.cm.Blues): """ This function prints and plots the confusion matrix. """ plt.imshow(cm, interpolation='nearest', cmap=cmap) plt.title(title) plt.colorbar() tick_marks = np.arange(len(classes)) plt.xticks(tick_marks, classes, rotation=0) plt.yticks(tick_marks, classes) thresh = cm.max() / 2. for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])): plt.text(j, i, cm[i, j], horizontalalignment="center", color="white" if cm[i, j] &gt; thresh else "black") plt.tight_layout() plt.ylabel('True label') plt.xlabel('Predicted label') 123456789101112131415161718lr = LogisticRegression(C = best_c, penalty = 'l1')lr.fit(X_train_undersample,y_train_undersample.values.ravel())y_pred_undersample = lr.predict(X_test_undersample.values)# Compute confusion matrixcnf_matrix = confusion_matrix(y_test_undersample,y_pred_undersample)np.set_printoptions(precision=2)print("Recall metric in the testing dataset: ", cnf_matrix[1,1]/(cnf_matrix[1,0]+cnf_matrix[1,1]))# Plot non-normalized confusion matrixclass_names = [0,1]plt.figure()plot_confusion_matrix(cnf_matrix , classes=class_names , title='Confusion matrix')plt.show()#混淆矩阵 显示模型分类效果 Recall metric in the testing dataset: 0.931972789116 3训练 + 测试——用best_C在训练集上重新训练一遍，再在 完整 测试集上预测用 混淆矩阵 计算recall值1234567891011121314151617lr = LogisticRegression(C = best_c, penalty = 'l1')lr.fit(X_train_undersample,y_train_undersample.values.ravel())y_pred = lr.predict(X_test.values)# Compute confusion matrixcnf_matrix = confusion_matrix(y_test,y_pred)np.set_printoptions(precision=2)print("Recall metric in the testing dataset: ", cnf_matrix[1,1]/(cnf_matrix[1,0]+cnf_matrix[1,1]))# Plot non-normalized confusion matrixclass_names = [0,1]plt.figure()plot_confusion_matrix(cnf_matrix , classes=class_names , title='Confusion matrix')plt.show() Recall metric in the testing dataset: 0.918367346939 12这里发现虽然recall值还可以，但是误伤了8581个（被检测成欺诈了），也就是精度accuracy有点低。故要权衡两者，都要较高才行 这里展示的是：不做样本平衡处理，直接把所有样本做交叉验证，发现效果很差1best_c = printing_Kfold_scores(X_train,y_train) ------------------------------------------- C parameter: 0.01 ------------------------------------------- Iteration 1 : recall score = 0.492537313433 Iteration 2 : recall score = 0.602739726027 Iteration 3 : recall score = 0.683333333333 Iteration 4 : recall score = 0.569230769231 Iteration 5 : recall score = 0.45 Mean recall score 0.559568228405 ------------------------------------------- C parameter: 0.1 ------------------------------------------- Iteration 1 : recall score = 0.567164179104 Iteration 2 : recall score = 0.616438356164 Iteration 3 : recall score = 0.683333333333 Iteration 4 : recall score = 0.584615384615 Iteration 5 : recall score = 0.525 Mean recall score 0.595310250644 ------------------------------------------- C parameter: 1 ------------------------------------------- Iteration 1 : recall score = 0.55223880597 Iteration 2 : recall score = 0.616438356164 Iteration 3 : recall score = 0.716666666667 Iteration 4 : recall score = 0.615384615385 Iteration 5 : recall score = 0.5625 Mean recall score 0.612645688837 ------------------------------------------- C parameter: 10 ------------------------------------------- Iteration 1 : recall score = 0.55223880597 Iteration 2 : recall score = 0.616438356164 Iteration 3 : recall score = 0.733333333333 Iteration 4 : recall score = 0.615384615385 Iteration 5 : recall score = 0.575 Mean recall score 0.61847902217 ------------------------------------------- C parameter: 100 ------------------------------------------- Iteration 1 : recall score = 0.55223880597 Iteration 2 : recall score = 0.616438356164 Iteration 3 : recall score = 0.733333333333 Iteration 4 : recall score = 0.615384615385 Iteration 5 : recall score = 0.575 Mean recall score 0.61847902217 ********************************************************************************* Best model to choose from cross validation is with C parameter = 10.0 ********************************************************************************* 1234567891011121314151617lr = LogisticRegression(C = best_c, penalty = 'l1')lr.fit(X_train,y_train.values.ravel())y_pred_undersample = lr.predict(X_test.values)# Compute confusion matrixcnf_matrix = confusion_matrix(y_test,y_pred_undersample)np.set_printoptions(precision=2)print("Recall metric in the testing dataset: ", cnf_matrix[1,1]/(cnf_matrix[1,0]+cnf_matrix[1,1]))# Plot non-normalized confusion matrixclass_names = [0,1]plt.figure()plot_confusion_matrix(cnf_matrix , classes=class_names , title='Confusion matrix')plt.show() Recall metric in the testing dataset: 0.619047619048 4 用predict_proba来测试 最好的逻辑回归 阈值1234567891011121314151617181920212223242526lr = LogisticRegression(C = 0.01, penalty = 'l1')lr.fit(X_train_undersample,y_train_undersample.values.ravel())y_pred_undersample_proba = lr.predict_proba(X_test_undersample.values)thresholds = [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]plt.figure(figsize=(10,10))j = 1for threshold in thresholds: y_test_predictions_high_recall = y_pred_undersample_proba[:,1] &gt; threshold plt.subplot(3,3,j) j += 1 # Compute confusion matrix cnf_matrix = confusion_matrix(y_test_undersample,y_test_predictions_high_recall) np.set_printoptions(precision=2) print("Recall metric in the testing dataset: ", cnf_matrix[1,1]/(cnf_matrix[1,0]+cnf_matrix[1,1])) # Plot non-normalized confusion matrix class_names = [0,1] plot_confusion_matrix(cnf_matrix , classes=class_names , title='Threshold &gt;= %s'%threshold) Recall metric in the testing dataset: 1.0 Recall metric in the testing dataset: 1.0 Recall metric in the testing dataset: 1.0 Recall metric in the testing dataset: 0.993197278912 Recall metric in the testing dataset: 0.931972789116 Recall metric in the testing dataset: 0.884353741497 Recall metric in the testing dataset: 0.843537414966 Recall metric in the testing dataset: 0.748299319728 Recall metric in the testing dataset: 0.578231292517 1数据预处理——解决样本分布不均衡问题之oversample123456import pandas as pd# 安装命令：pip install imblearnfrom imblearn.over_sampling import SMOTEfrom sklearn.ensemble import RandomForestClassifierfrom sklearn.metrics import confusion_matrixfrom sklearn.model_selection import train_test_split 12345678credit_cards=pd.read_csv('creditcard.csv')columns=credit_cards.columns# The labels are in the last column ('Class'). Simply remove it to obtain features columnsfeatures_columns=columns.delete(len(columns)-1)features=credit_cards[features_columns]labels=credit_cards['Class'] 12345#划分数据集features_train, features_test, labels_train, labels_test = train_test_split(features, labels, test_size=0.2, random_state=0) 123#★SMOTE算法通过给定的训练集，生成新的随机扩充训练集oversampler=SMOTE(random_state=0)os_features,os_labels=oversampler.fit_sample(features_train,labels_train) 12345#生成前，label=0print(len(labels_train[labels_train == 0]))#生成以后，label=1的变成和=0的一样多print(len(os_labels[os_labels == 1])) 227454 227454 123os_features = pd.DataFrame(os_features)os_labels = pd.DataFrame(os_labels)best_c = printing_Kfold_scores(os_features,os_labels) ------------------------------------------- C parameter: 0.01 ------------------------------------------- Iteration 1 : recall score = 0.890322580645 Iteration 2 : recall score = 0.894736842105 Iteration 3 : recall score = 0.968617904172 Iteration 4 : recall score = 0.944471922709 Iteration 5 : recall score = 0.958397907255 Mean recall score 0.931309431377 ------------------------------------------- C parameter: 0.1 ------------------------------------------- Iteration 1 : recall score = 0.890322580645 Iteration 2 : recall score = 0.894736842105 Iteration 3 : recall score = 0.970255615802 Iteration 4 : recall score = 0.959991646608 Iteration 5 : recall score = 0.96051922929 Mean recall score 0.93516518289 ------------------------------------------- C parameter: 1 ------------------------------------------- Iteration 1 : recall score = 0.890322580645 Iteration 2 : recall score = 0.894736842105 Iteration 3 : recall score = 0.970211353325 Iteration 4 : recall score = 0.960134533584 Iteration 5 : recall score = 0.960442290148 Mean recall score 0.935169519962 ------------------------------------------- C parameter: 10 ------------------------------------------- Iteration 1 : recall score = 0.890322580645 Iteration 2 : recall score = 0.894736842105 Iteration 3 : recall score = 0.970322009516 Iteration 4 : recall score = 0.95977182049 Iteration 5 : recall score = 0.960783020631 Mean recall score 0.935187254678 ------------------------------------------- C parameter: 100 ------------------------------------------- Iteration 1 : recall score = 0.890322580645 Iteration 2 : recall score = 0.894736842105 Iteration 3 : recall score = 0.969635941131 Iteration 4 : recall score = 0.960255437949 Iteration 5 : recall score = 0.960398324925 Mean recall score 0.935069825351 ********************************************************************************* Best model to choose from cross validation is with C parameter = 10.0 ********************************************************************************* 1234567891011121314151617lr = LogisticRegression(C = best_c, penalty = 'l1')lr.fit(os_features,os_labels.values.ravel())y_pred = lr.predict(features_test.values)# Compute confusion matrixcnf_matrix = confusion_matrix(labels_test,y_pred)np.set_printoptions(precision=2)print("Recall metric in the testing dataset: ", cnf_matrix[1,1]/(cnf_matrix[1,0]+cnf_matrix[1,1]))# Plot non-normalized confusion matrixclass_names = [0,1]plt.figure()plot_confusion_matrix(cnf_matrix , classes=class_names , title='Confusion matrix')plt.show() Recall metric in the testing dataset: 0.910891089109 12 12 12]]></content>
      <categories>
        <category>machine_learning_in_action</category>
      </categories>
      <tags>
        <tag>usersample</tag>
        <tag>oversample</tag>
        <tag>K折交叉验证</tag>
        <tag>混淆矩阵</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[1向量、矩阵、向量范数、矩阵范数]]></title>
    <url>%2F2018%2F04%2F19%2Fmath%2Flinear_algebra%2F1%E5%90%91%E9%87%8F%E3%80%81%E7%9F%A9%E9%98%B5%E3%80%81%E5%90%91%E9%87%8F%E8%8C%83%E6%95%B0%E3%80%81%E7%9F%A9%E9%98%B5%E8%8C%83%E6%95%B0%2F</url>
    <content type="text"><![CDATA[向量向量内积 和 投影内积：1用点乘：a•b2用转置乘：a^T b3向量的模是范数的一种4 wT w《=》向量自己做内积 = 自身长度（模）²，因为投影结果还是w向量本身 投影：1 b在a上的投影 = |b|cosθ2 内积 = 向量1在向量2上的投影 * 他的长度（模，绝对值符号）；3 由2又有：b在a上的投影 = $\frac {a^T b} {||a||}$ ★ 向量外积向量范数 向量范数的定义和性质： 齐次性：数乘以后会放大相应的倍数 1-范数、2-范数、无穷范数： 稀疏性 和 0-范数： 稀疏性用到的范数：特殊的0-范数，他不满足齐次性；所以需要1-范数来辅助解决； 范数的几何意义： 向量组 初始单位向两组 向量组等价 线性相关 和 线性无关 施密特正交化由施密特正交化生成的 正交向量组 和 之前的线性无关向量组 可以互相线性表示 矩阵 O矩阵所有元素都为0的矩阵 一阶矩阵(a)等同于数a 矩阵的内积内积 A·B &lt;=&gt; ${A^T}B$矩阵范数矩阵的范数比向量的范数多一条相容性]]></content>
      <categories>
        <category>math</category>
        <category>linear_algebra</category>
      </categories>
      <tags>
        <tag>范数</tag>
        <tag>外积</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[备忘记录]]></title>
    <url>%2F2018%2F04%2F19%2FOTHERS%2F%E5%A4%87%E5%BF%98%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[变量命名标准整型：语义名数组：语义名+S 或 语义名+Arr一般：语义名+数据结构名+其他特征 比如：classArrTry 标题title设置标准为了好看，把“库/框架名”放前面，把“算法名”放后面，如：Tensorflow - LinearRegression 标签tags设置标准文章内所包含的 技术点名称]]></content>
      <categories>
        <category>其他</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Python功能函数]]></title>
    <url>%2F2018%2F04%2F19%2Fpython%2FPython%E5%8A%9F%E8%83%BD%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[一、图片处理1 更改图片尺寸123456789101112131415161718192021import osimport os.pathfrom PIL import Image'''filein: 输入图片fileout: 输出图片width: 输出图片宽度height:输出图片高度type:输出图片类型（png, gif, jpeg...）'''def ResizeImage(filein, fileout, width, height, type): img = Image.open(filein) out = img.resize((width, height),Image.ANTIALIAS) #resize image with high-quality out.save(fileout, type)if __name__ == "__main__": filein = r'image\test.png' fileout = r'image\testout.png' width = 60 height = 85 type = 'png' ResizeImage(filein, fileout, width, height, type) 2 把图片转为3维数组（两种方法）用np + Image12345678910import numpy as npfrom PIL import Imageimport matplotlib.pyplot as pltimage = Image.open(r'C:\Users\Administrator\Desktop\data\train\forest_001.jpg') #读取图片文件plt.imshow(image)plt.show() #将图片输出到屏幕image_arr = np.array(image) #将图片以数组的形式读入变量print (image_arr) 用Tensorflow1234567891011121314151617import tensorflow as tfimport numpy as npimport matplotlib.pyplot as pltimage_contents = tf.read_file('D:/Tensorflow/slim/test images/000.jpg') #读取文件image = tf.image.decode_jpeg(image_contents, channels=3) #解码jpegwith tf.Session() as sess: sess.run(tf.global_variables_initializer()) img=sess.run((image)) #img为三维数组 print (img.shape) #输出数组形状 print (img) #打印数组 plt.imshow(img) #显示数组 plt.show() 3 整合1，2实现把图片转为指定大小的三维数组12345678910111213import numpy as npfrom PIL import Imageimport matplotlib.pyplot as pltdef ImageToArray(imagePath, width, height): image = Image.open(imagePath, "r") #读取图片文件 plt.imshow(image) plt.show() #将图片输出到屏幕 out = image.resize((width, height),Image.ANTIALIAS) return np.array(out) #将图片以数组的形式读入变量image_arr = ImageToArray('D:/Tensorflow/slim/test images/000.jpg', 299, 299)print (image_arr.shape)]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tensorflow学习9-1：谷歌inception-v3模型之下载模型和查看结构]]></title>
    <url>%2F2018%2F04%2F19%2Ftensorflow%2FTensorflow%E5%AD%A6%E4%B9%A09-1%EF%BC%9A%E8%B0%B7%E6%AD%8Cinception-v3%E6%A8%A1%E5%9E%8B%E4%B9%8B%E4%B8%8B%E8%BD%BD%E6%A8%A1%E5%9E%8B%E5%92%8C%E6%9F%A5%E7%9C%8B%E7%BB%93%E6%9E%84%2F</url>
    <content type="text"><![CDATA[1234567891011import tensorflow as tfimport osimport tarfileimport requests#模型下载地址MOD_URL = "http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz"#模型存放目录MOD_DIR = "D:/Tensorflow/models/inception/"#模型结构存放目录LOG_DIR = "D:/Tensorflow/logs/inception/" 12345678910111213141516171819202122232425262728293031323334353637if not os.path.exists(MOD_DIR): os.makedirs(MOD_DIR) #取得文件名，以及完整路径file_name = MOD_URL.split("/")[-1]file_path = os.path.join(MOD_DIR, file_name)#file_path = MOD_DIR + file_name#下载模型if not os.path.exists(file_path): print("download: ", file_name) r = requests.get(MOD_URL, stream=True) with open(file_path, "wb") as f: for chunk in r.iter_content(chunk_size=1024): if(chunk): f.write(chunk)print("finish: ", file_name)#解压模型文件到指定目录tarfile.open(file_path, "r:gz").extractall(MOD_DIR)#存放模型结构（用tensorboard查看）if not os.path.exists(LOG_DIR): os.makedirs(LOG_DIR) # 解压后的xxx.pb是训练好的模型inception_model_path = os.path.join(MOD_DIR, "classify_image_graph_def.pb")with tf.Session() as sess: #创建一个图来存放google训练好的模型 with tf.gfile.FastGFile(inception_model_path, "rb") as f: graph_def = tf.GraphDef() graph_def.ParseFromString(f.read()) tf.import_graph_def(graph_def, name="") #保存图的结构 writer = tf.summary.FileWriter(LOG_DIR, sess.graph) writer.close() finish: inception-2015-12-05.tgz 结构如下图]]></content>
      <categories>
        <category>tensorflow</category>
      </categories>
      <tags>
        <tag>inception-v3</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Numpy常用功能、函数]]></title>
    <url>%2F2018%2F04%2F18%2Fnumpy%2FNumpy%E5%B8%B8%E7%94%A8%E5%8A%9F%E8%83%BD%E3%80%81%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[Numpynp.matrix()：矩阵标准写法1234567891011&gt;&gt;&gt; a = np.matrix('1 2 7; 3 4 8; 5 6 9') #单行写法&gt;&gt;&gt; a #矩阵的换行必须是用分号(;)隔开，内部数据必须为字符串形式(“ ”)，矩matrix([[1, 2, 7], #阵的元素之间必须以空格隔开。[3, 4, 8],[5, 6, 9]])&gt;&gt;&gt; b=np.array([[1,5],[3,2]])&gt;&gt;&gt; x=np.matrix(b) #矩阵中的data可以为数组对象。&gt;&gt;&gt; xmatrix([[1, 5],[3, 2]]) np.inf：无穷大np.set_printoptions(suppress=True)不用科学记数法输出 np.multiply()：矩阵对应元素相乘np.linalg：矩阵运算，中的常用函数diag 以一维数组的形式返回方阵的对角线元素dot 矩阵乘法det 计算矩阵行列式eig 计算方阵的特征值和特征向量inv 计算方阵的逆lstsq 计算Ax=b的最小二乘解norm 计算范数（ord=指定范数），默认为２范数pinv 计算矩阵的Moore-Penrose伪逆qr 计算qr分解svd 计算奇异值分解solve 解线性方程组Ax=b，其中A为一个方阵trace 计算对角线元素的和 np.newaxis： 新增纬度1import numpy as np In [30]: 123#np.newaxis多用于防止取出一行或列后数据降维a = np.arange(6).reshape(2,3);a Out[30]: 12array([[0, 1, 2], [3, 4, 5]]) In [31]: 1234# np.newaxis加在哪个位置，就能在shape里看到相应位置增加了一个纬度c = a[:, np.newaxis] #这里相当于a[:, np.newaxis, :]加在了行的后面列的前面print(c)print("c.shape",c.shape) 1234[[[0 1 2]] [[3 4 5]]]c.shape (2, 1, 3) In [27]: 123d = a[:,np.newaxis, 2] #这里是取2号列print(d)print("d.shape",d.shape) 123[[2] [5]]d.shape (2, 1) In [25]: 12345e = a[1, np.newaxis, :] #这里是取1号行print(e)print("e.shape",e.shape)[[3 4 5]]e.shape (1, 3)]]></content>
      <categories>
        <category>numpy</category>
      </categories>
      <tags>
        <tag>numpy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MathJax数学公式语法]]></title>
    <url>%2F2018%2F04%2F16%2Fhexo%2FMathJax%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F%E8%AF%AD%E6%B3%95%2F</url>
    <content type="text"><![CDATA[概述在Markdown中输入数学公式需要LaTeX语法的支持。 基本语法呈现位置 正文(inline)中的LaTeX公式用\$…$定义 语句为\sum_{i=0}^N\int_{a}^{b}g(t,i)\text{d}t 显示为 $\sum_{i=0}^N\int_{a}^{b}g(t,i)\text{d}t$ 单独显示(display)的LaTeX公式用\$\$…$$定义，此时公式居中并放大显示 语句为\sum_{i=0}^N\int_{a}^{b}g(t,i)\text{d}t 显示为 ​ \sum_{i=0}^N\int_{a}^{b}g(t,i)\text{d}t 下列描述语句中若非特别指出均省略\$…$ 希腊字母 显示 命令 显示 命令 α \alpha β \beta γ \gamma δ \delta ε \epsilon ζ \zeta η \eta θ \theta ι \iota κ \kappa λ \lambda μ \mu ν \nu ξ \xi π \pi ρ \rho σ \sigma τ \tau υ \upsilon φ \phi χ \chi ψ \psi ω \omega - 若需要大写希腊字母，将命令首字母大写即可。 \Gamma呈现为 $\Gamma$- 若需要斜体希腊字母，将命令前加上var前缀即可。 \varGamma呈现为 $\varGamma$ 字母修饰上下标和hat 上标：^ 下标：_ hat：$\hat b$ 举例：C_n^2\hat b呈现为 $C_n^2\hat b$ 矢量 \vec a呈现为 $\vec a$ \overrightarrow{xy}呈现为 $\overrightarrow{xy}$ 字体 Typewriter：\mathtt{A}呈现为 $\mathtt{A}$ $\mathtt{ABCDEFGHIJKLMNOPQRSTUVWXYZABCDEFGHIJKLMNOPQRSTUVWXYZ}$ Blackboard Bold：\mathbb{A}呈现为 $\mathbb{A}$ $\mathbb{ABCDEFGHIJKLMNOPQRSTUVWXYZABCDEFGHIJKLMNOPQRSTUVWXYZ}$ Sans Serif：\mathsf{A}呈现为 $\mathsf{A}$ $\mathsf{ABCDEFGHIJKLMNOPQRSTUVWXYZABCDEFGHIJKLMNOPQRSTUVWXYZ}$ 分组 使用{}将具有相同等级的内容扩入其中，成组处理 举例：10^{10}呈现为 $10^{10}$，而10^10呈现为 $10^10$ 括号 小括号：()呈现为() 中括号：[]呈现为[] 尖括号：\langle,\rangle呈现为⟨⟩ 此处为与分组符号{}相区别，使用转义字符\ 使用\left(或\right)使符号大小与邻近的公式相适应；该语句适用于所有括号类型 (\frac{x}{y})呈现为$(\frac{x}{y})$ 而\left(\frac{x}{y}\right)呈现为$\left(\frac{x}{y}\right)$ 求和、极限与积分 求和：\sum 举例：`\sum_{i=1}^n{a_i}`呈现为$\sum_{i=1}^n{a_i}$ 极限：\lim_{x\to 0}呈现为 $\lim_{x\to 0}$ 积分：\int 举例：`\int_0^\infty{fxdx}`呈现为 $\int_0^\infty{fxdx}$ 分式与根式 分式(fractions)：\frac{公式1}{公式2}呈现为 $\frac{a+b}{a-b}$ 根式：\sqrt[x]{y}呈现为$\sqrt[x]{y}$ 特殊函数 \函数名 举例：\sin x，\ln x，\max(A,B,C)呈现为 $\sin x ,\ln x, \max(A,B,C)$ 特殊符号 显示 命令 ∞ \infty ∪ \cup ∩ \cap ⊂ \subset ⊆ \subseteq ⊃ \supset ∈ \in ∉ \notin ∅ \varnothing ∀ \forall ∃ \exists ¬ \lnot ∇ \nabla ∂ \partial 空格 LaTeX语法本身会忽略空格的存在 小空格：a\ b呈现为 $a\ b$ 4格空格：a\quad b呈现为 $a\quad b$ 矩阵基本语法 起始标记\begin{matrix}``，结束标记``\end{matrix} 每一行末尾标记\\，行间元素之间以&amp;分隔 举例 12345$$\begin&#123;matrix&#125;1&amp;0&amp;0\\0&amp;1&amp;0\\0&amp;0&amp;1\\\end&#123;matrix&#125;$$ 呈现为 ​ \begin{matrix}1&0&0\\0&1&0\\0&0&1\\\end{matrix} 矩阵边框 在起始、结束标记处用下列词替换matrix pmatrix：小括号边框 bmatrix：中括号边框 Bmatrix：大括号边框 vmatrix：单竖线边框 Vmatrix：双竖线边框 省略元素 横省略号：\cdots 竖省略号：\vdots 斜省略号：\ddots 举例 123456$$\begin&#123;bmatrix&#125;&#123;a_&#123;11&#125;&#125;&amp;&#123;a_&#123;12&#125;&#125;&amp;&#123;\cdots&#125;&amp;&#123;a_&#123;1n&#125;&#125;\\&#123;a_&#123;21&#125;&#125;&amp;&#123;a_&#123;22&#125;&#125;&amp;&#123;\cdots&#125;&amp;&#123;a_&#123;2n&#125;&#125;\\&#123;\vdots&#125;&amp;&#123;\vdots&#125;&amp;&#123;\ddots&#125;&amp;&#123;\vdots&#125;\\&#123;a_&#123;m1&#125;&#125;&amp;&#123;a_&#123;m2&#125;&#125;&amp;&#123;\cdots&#125;&amp;&#123;a_&#123;mn&#125;&#125;\\\end&#123;bmatrix&#125;$$ 呈现为 \begin{bmatrix} {a_{11}}&{a_{12}}&{\cdots}&{a_{1n}}\\ {a_{21}}&{a_{22}}&{\cdots}&{a_{2n}}\\ {\vdots}&{\vdots}&{\ddots}&{\vdots}\\ {a_{m1}}&{a_{m2}}&{\cdots}&{a_{mn}}\\ \end{bmatrix}阵列 需要array环境：起始、结束处以{array}声明 对齐方式：在{array}后以{}逐行统一声明 左对齐：l；居中：c；右对齐：r 竖直线：在声明对齐方式时，插入|建立竖直线 插入水平线：\hline 举例 12345$$\begin&#123;array&#125;&#123;c|lll&#125;&#123;↓&#125;&amp;&#123;a&#125;&amp;&#123;b&#125;&amp;&#123;c&#125;\\&#123;R_1&#125;&amp;&#123;c&#125;&amp;&#123;b&#125;&amp;&#123;a&#125;\\&#123;R_2&#125;&amp;&#123;b&#125;&amp;&#123;c&#125;&amp;&#123;c&#125;\\\end&#123;array&#125;$$ 呈现为 \begin{array}{c|lll} {↓}&{a}&{b}&{c}\\ {R_1}&{c}&{b}&{a}\\ {R_2}&{b}&{c}&{c}\\ \end{array}方程组 需要cases环境：起始、结束处以{cases}声明 举例 123456$$\begin&#123;cases&#125;a_1x+b_1y+c_1z=d_1\\a_2x+b_2y+c_2z=d_2\\a_3x+b_3y+c_3z=d_3\\\end&#123;cases&#125;$$ 呈现为 \begin{cases} a_1x+b_1y+c_1z=d_1\\ a_2x+b_2y+c_2z=d_2\\ a_3x+b_3y+c_3z=d_3\\ \end{cases}]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>MathJax</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo Next优化&踩坑]]></title>
    <url>%2F2018%2F04%2F16%2Fhexo%2FHexo%20Next%E4%BC%98%E5%8C%96%26%E8%B8%A9%E5%9D%91%2F</url>
    <content type="text"><![CDATA[一、界面 篇1 添加动态背景修改配置文件在主题配置文件中找到canvas_nest: false，把它改为canvas_nest: true 修改_layout.swig打开 next/layout/_layout.swig在 &lt; /body&gt;之前添加代码 1234&#123;% if theme.canvas_nest %&#125;&lt;script type=&quot;text/javascript&quot;color=&quot;0,0,0&quot; opacity=&apos;0.5&apos; zIndex=&quot;-2&quot; count=&quot;50&quot; src=&quot;//cdn.bootcss.com/canvas-nest.js/1.0.0/canvas-nest.min.js&quot;&gt;&lt;/script&gt;&#123;% endif %&#125; 配置项说明 color ：线条颜色, 默认: &#39;0,0,0&#39;；三个数字分别为(R,G,B) opacity: 线条透明度（0~1）, 默认: 0.5 count: 线条的总数量, 默认: 150 zIndex: 背景的z-index属性，css属性用于控制所在层的位置, 默认: -1 2 直接展开文章全部目录搜索打开这个文件：sidebar-toc.styl 把下面的内容注释掉： 1234567//取消逐渐展开，改为直接展开所有TOC//.post-toc .nav .nav-child &#123; display: none; &#125;.post-toc .nav .active &gt; .nav-child &#123; display: block; &#125;.post-toc .nav .active-current &gt; .nav-child &#123; display: block; &amp; &gt; .nav-item &#123; display: block; &#125;&#125; 3 添加文章结束标记在 next\layout_macro\post.swig 中wechat-subscriber.swig 上面加入如下代码： 1234&lt;!-- 添加文章结束标记 --&gt;&#123;% if not is_index %&#125; &lt;div style=&quot;text-align:center;color: #000;font-size:14px;&quot;&gt;----------------- The End -----------------&lt;/div&gt;&#123;% endif %&#125; 4 实现主页文章预览效果进入hexo博客项目的themes/next目录用文本编辑器打开_config.yml文件搜索”auto_excerpt”,找到如下部分：12345# Automatically Excerpt. Not recommand.# Please use &lt;!-- more --&gt; in the post to control excerpt accurately.auto_excerpt: enable: true length: 150 把enable值设置为true，就可以控制文章在主页的显示了 5 添加MathJax数学公式支持在主题中开启mathjax开关如何使用了主题了，别忘了在主题（Theme）中开启mathjax开关，下面以next主题为例，介绍下如何打开mathjax开关。 进入到主题目录，找到_config.yml配置问题，把mathjax默认的false修改为true，具体如下： 1234# MathJax Supportmathjax: enable: true per_page: true 别着急，这样还不够，还需要在文章的Front-matter里打开mathjax开关，如下： 123456---title: index.htmldate: 2016-12-28 21:01:30tags:mathjax: true-- 更换Hexo的markdown渲染引擎hexo-renderer-kramed引擎是在默认的渲染引擎hexo-renderer-marked的基础上修改了一些bug，两者比较接近，也比较轻量级。 12npm uninstall hexo-renderer-marked --savenpm install hexo-renderer-kramed --save 执行上面的命令即可，先卸载原来的渲染引擎，再安装新的。 然后，跟换引擎后行间公式可以正确渲染了，但是这样还没有完全解决问题，行内公式的渲染还是有问题，因为hexo-renderer-kramed引擎也有语义冲突的问题。接下来到博客根目录下，找到node_modules\kramed\lib\rules\inline.js，把第11行的escape变量的值做相应的修改： 12// escape: /^\\([\\`*&#123;&#125;\[\]()#$+\-.!_&gt;])/, escape: /^\\([`*\[\]()#+\-.!_&gt;])/ 同时把第20行的em变量也要做相应的修改。 12// em: /^\b_((?:__|[\s\S])+?)_\b|^\*((?:\*\*|[\s\S])+?)\*(?!\*)/, em: /^\*((?:\*\*|[\s\S])+?)\*(?!\*)/ 重新启动hexo（先clean再generate）,问题完美解决。 6 调整页面CSS布局为了加宽文章页面显示，在下面两个文件中添加自定义代码在 themes\next\source\css_custom\custom.styl 中：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657// Custom styles.//边框效果/*// 最上面.content-wrap &#123; padding: 0 40px 40px 40px;&#125;.posts-expand &#123; padding-top: 0;&#125;// 文章.post &#123; margin-top: 60px; margin-bottom: 60px; padding: 25px; -webkit-box-shadow: 1px 1px 1px 1px rgba(202, 203, 203, .5); -moz-box-shadow: 1px 1px 1px 1px rgba(202, 203, 204, .5); &#125;// 右上.sidebar-position-right .header-inner &#123; -webkit-box-shadow: 1px 1px 1px 1px rgba(202, 203, 203, .5); -moz-box-shadow: 1px 1px 1px 1px rgba(202, 203, 204, .5);&#125;// 右下.sidebar .sidebar-inner &#123; -webkit-box-shadow: 1px 1px 1px 1px rgba(202, 203, 203, .5); -moz-box-shadow: 1px 1px 1px 1px rgba(202, 203, 204, .5);&#125;// 右上.sidebar-position-right .header-inner &#123; -webkit-box-shadow: 0 1px 0 0 #262a30; -moz-box-shadow: 0 1px 0 0 #262a30;&#125;*/// 最下面.sidebar-position-right .footer-inner &#123; padding-left: 40px; padding-right: 280px;&#125;// 首页文章添加分割线.posts-expand .post-eof &#123; display: block; margin: 80px auto 60px; width: 61.8%; height: 1px; background: #bbb; text-align: center;&#125;.sidebar-inner &#123; padding: 20px 0 0 0;&#125;.music163 &#123; margin: 20px 0 0 0;&#125; 在 D:\wxy555123.github.io\themes\next\source\css_variables\custom.styl 中：1234567891011121314151617181920// base.styl Layout sizes// --------------------------------------------------//$main-desktop = 960px $main-desktop = 1230px //new 主宽度，也调大防止sidebar遮挡$main-desktop-large = 1200px//$content-desktop = 700px$content-desktop = 990px //new 文章宽度调大$content-desktop-large = 900px$content-desktop-padding = 40px$content-tablet-padding = 10px$content-mobile-padding = 8px$sidebar-desktop = 240px$footer-height = 50px$gap-between-main-and-footer = 100px 7 添加 Gitment 评论系统简介本文介绍hexo next主题(5.1.2)集成giment评论系统的过程。所谓gitment就是把评论放到github的issues系统里，评论支持md，比较适合程序员. 一.注册OAuth Application点击https://github.com/settings/applications/new注册，注意Authorization callback URL填自己的网站urlhttp://yangq.me/.记下Client ID和Client Secret. 二.修改themes/next/_config.yml在其中添加: 123456789# Gitment# Introduction: https://imsun.net/posts/gitment-introduction/gitment: enable: true githubID: yourid repo: yourrepo ClientID: yourid ClientSecret: yoursecret lazy: true123456789 注意:格式要正确，该空格的一定要空格。所有的yourXXX都换成自己的. 三.修改gitment.swig在主题下layout/_third-party/comments/目录下中修改文件gitment.swig使得能够正确初始化： 修改红框标记的id字段，用日期时间戳代替，使得id不会超过50个字符 8 博文压缩博文压缩用于加快网站访问速度 step1:在根目录新建 gulpfile.js 文件 step2:方法一，使用下面代码和命令：123456789101112131415161718192021222324252627282930313233var gulp = require('gulp');var minifycss = require('gulp-minify-css');var uglify = require('gulp-uglify');var htmlmin = require('gulp-htmlmin');var htmlclean = require('gulp-htmlclean');// 压缩 public 目录 cssgulp.task('minify-css', function() &#123; return gulp.src('./public/**/*.css') .pipe(minifycss()) .pipe(gulp.dest('./public'));&#125;);// 压缩 public 目录 htmlgulp.task('minify-html', function() &#123; return gulp.src('./public/**/*.html') .pipe(htmlclean()) .pipe(htmlmin(&#123; removeComments: true, minifyJS: true, minifyCSS: true, minifyURLs: true, &#125;)) .pipe(gulp.dest('./public'))&#125;);// 压缩 public/js 目录 jsgulp.task('minify-js', function() &#123; return gulp.src('./public/**/*.js') .pipe(uglify()) .pipe(gulp.dest('./public'));&#125;);// 执行 gulp 命令时执行的任务gulp.task('default', [ 'minify-html','minify-css','minify-js']); 命令为 hexo g&amp;&amp;gulp方法二，使用下面代码和命令：1234567891011121314151617181920212223242526272829303132333435363738var gulp = require('gulp'), uglify = require('gulp-uglify'), rename = require('gulp-rename'), cssmin = require('gulp-minify-css'), imagemin = require('gulp-imagemin');//JS压缩gulp.task('uglify', function() &#123; return gulp.src('././public/js/*.js') .pipe(uglify()) .pipe(gulp.dest('././public/js/'));&#125;);//public-fancybox-js压缩gulp.task('fancybox:js', function() &#123; return gulp.src('././public/fancybox/jquery.fancybox.js') .pipe(uglify()) .pipe(gulp.dest('././public/fancybox/'));&#125;);//public-fancybox-css压缩gulp.task('fancybox:css', function() &#123; return gulp.src('././public/fancybox/jquery.fancybox.css') .pipe(cssmin()) .pipe(gulp.dest('././public/fancybox/'));&#125;);//CSS压缩gulp.task('cssmin', function() &#123; return gulp.src('././public/css/style.css') .pipe(cssmin()) .pipe(gulp.dest('././public/css/'));&#125;);//图片压缩gulp.task('images', function() &#123; gulp.src('././public/img/*.*') .pipe(imagemin(&#123; progressive: false &#125;)) .pipe(gulp.dest('././public/img/'));&#125;);gulp.task('build', ['uglify', 'cssmin', 'images', 'fancybox:js', 'fancybox:css']); 命令为 hexo g&amp;&amp;gulp build 二、操作 篇1 Hexo命令 安装主题：用git clone到themes文件夹中 生成静态文件：hexo g 启动本地服务器：hexo s 发布到远程网站：hexo d （hexo d -g 生成的后自动发布） ​ 创建文章：hexo new “标题” （默认就在“post”目录里） 创建草稿：hexo new draft “标题” 把草稿转到“post”目录：hexo publish “标题” ​ （注：中间的命令可以用哦个首字母简写） 2 Git命令1）更新上传文件clone后，在clone的文件夹下123git add .git commit -m &quot;message&quot;git push 2）清空你的仓库项目中所有文件进入到仓库文件夹下123git rm -rf *git commit -m &apos;clean all file&apos;git push 3 修改Hexo生成文件模版可在根目录 scaffolds 文件夹下修改3类文章模版 4 添加创建文件后，用vscode自动打开脚本在根目录下新建 scripts 文件夹，里面新建 js 文件 名字随意，代码如下： 12345var exec = require("child_process").exec;hexo.on("new", function(data) &#123; exec("Code.exe " + [data.path]);&#125;); 以后每次执行 hexo n 新建文件后都会自动运行 vscode 打开编辑 5修改node.js默认启动路径右键node.js快捷方式，把“起始位置”属性修改为以下内容：1D:\wxy555123.github.io\]]></content>
      <categories>
        <category>hexo</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[数据的标准化（归一化）]]></title>
    <url>%2F2018%2F04%2F13%2Fmachine_learning_theory%2F%E6%95%B0%E6%8D%AE%E7%9A%84%E6%A0%87%E5%87%86%E5%8C%96%EF%BC%88%E5%BD%92%E4%B8%80%E5%8C%96%EF%BC%89%2F</url>
    <content type="text"><![CDATA[几种归一化方式数据的标准化（normalization）是将数据按比例缩放，使之落入一个小的特定区间。在某些比较和评价的指标处理中经常会用到，去除数据的单位限制，将其转化为无量纲的纯数值，便于不同单位或量级的指标能够进行比较和加权。 其中最典型的就是数据的归一化处理，即将数据统一映射到[0,1]区间上，常见的数据归一化的方法有： 1）min-max标准化(Min-max normalization) 也叫离差标准化，是对原始数据的线性变换，使结果落到[0,1]区间，转换函数如下： 其中max为样本数据的最大值，min为样本数据的最小值。这种方法有一个缺陷就是当有新数据加入时，可能导致max和min的变化，需要重新定义。 2）log函数转换 通过以10为底的log函数转换的方法同样可以实现归一下，具体方法如下： 看了下网上很多介绍都是x*=log10(x)，其实是有问题的，这个结果并非一定落到[0,1]区间上，应该还要除以log10(max)，max为样本数据最大值，并且所有的数据都要大于等于1。 3）atan函数转换 用反正切函数也可以实现数据的归一化： 使用这个方法需要注意的是如果想映射的区间为[0,1]，则数据都应该大于等于0，小于0的数据将被映射到[-1,0]区间上。 而并非所有数据标准化的结果都映射到[0,1]区间上，其中最常见的标准化方法就是Z标准化，也是SPSS中最为常用的标准化方法： 4）z-score 标准化(zero-mean normalization) 也叫标准差标准化，经过处理的数据符合标准正态分布，即均值为0，标准差为1，其转化函数为： 其中μ为所有样本数据的均值，σ为所有样本数据的标准差。]]></content>
      <categories>
        <category>machine_learning_theory</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Tensorflow学习7：长短期记忆网络LSTM]]></title>
    <url>%2F2018%2F04%2F09%2Ftensorflow%2FTensorflow%E5%AD%A6%E4%B9%A07%EF%BC%9A%E9%95%BF%E7%9F%AD%E6%9C%9F%E8%AE%B0%E5%BF%86%E7%BD%91%E7%BB%9CLSTM%2F</url>
    <content type="text"><![CDATA[用LSTM网络进行手写数字识别 1234import tensorflow as tffrom tensorflow.examples.tutorials.mnist import input_dataPWS_DIR = "C:/Users/lenovo/Desktop/Python WORK SPACE/"LOG_DIR = "D:/Tensorflow/logs/" 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253mnist = input_data.read_data_sets(PWS_DIR + "DATASET/MNIST_data", one_hot=True)n_inputs = 28 #图片一行一行输入，一行28个数据max_time = 28 #一共28行（时间序列长度）lstm_size = 100 #隐层block数n_class = 10batch_size = 50n_batch = mnist.train.num_examples // batch_sizex = tf.placeholder(tf.float32, [None, 784])y = tf.placeholder(tf.float32, [None, 10])weights = tf.Variable(tf.truncated_normal([lstm_size, n_class], stddev=0.1))biases = tf.Variable(tf.constant(0.1, shape=[n_class]))def LSTM(X, weights, biases): #inputs = [batch_size, max_time, n_inputs] inputs = tf.reshape(X, [-1, max_time, n_inputs]) #定义LSTM基本CELL lstm_cell = tf.contrib.rnn.BasicLSTMCell(lstm_size)# final_state[state, batch_size, cell.state_size] (state_size即lstm_size)# final_state[state=0]是cell state# final_state[state=1]是hidden state （最终输出）# outputs: The RNN output `Tensor`. （单个序列输出）# If time_major == False (default), this will be a `Tensor` shaped:# `[batch_size, max_time, cell.output_size]`.# If time_major == True, this will be a `Tensor` shaped:# `[max_time, batch_size, cell.output_size]`. outputs, final_state = tf.nn.dynamic_rnn(lstm_cell, inputs, dtype=tf.float32) results = tf.nn.softmax(tf.matmul(final_state[1], weights) + biases) return results#计算LSTM返回结果predict = LSTM(x, weights, biases)#使用交叉熵代价函数loss_cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=predict))#使用AdamOptimizer进行优化train_step = tf.train.AdamOptimizer(1e-4).minimize(loss_cross_entropy)#结果放在一个BOOL型列表中predict_bool = tf.equal(tf.argmax(y, 1), tf.argmax(predict, 1))#求准确率accuracy = tf.reduce_mean(tf.cast(predict_bool, tf.float32))with tf.Session() as sess: sess.run(tf.global_variables_initializer()) for epoch in range(6): for batch in range(n_batch): batch_xs, batch_ys = mnist.train.next_batch(batch_size) sess.run(train_step, feed_dict=&#123;x:batch_xs, y:batch_ys&#125;) acc = sess.run(accuracy, feed_dict=&#123;x:mnist.test.images, y:mnist.test.labels&#125;) print("Iter" + str(epoch) + ", Testing Accuracy= " + str(acc)) Extracting C:/Users/lenovo/Desktop/Python WORK SPACE/DATASET/MNIST_data\train-images-idx3-ubyte.gzExtracting C:/Users/lenovo/Desktop/Python WORK SPACE/DATASET/MNIST_data\train-labels-idx1-ubyte.gzExtracting C:/Users/lenovo/Desktop/Python WORK SPACE/DATASET/MNIST_data\t10k-images-idx3-ubyte.gzExtracting C:/Users/lenovo/Desktop/Python WORK SPACE/DATASET/MNIST_data\t10k-labels-idx1-ubyte.gzIter0, Testing Accuracy= 0.7837Iter1, Testing Accuracy= 0.8641Iter2, Testing Accuracy= 0.8951Iter3, Testing Accuracy= 0.9087Iter4, Testing Accuracy= 0.9263Iter5, Testing Accuracy= 0.9325]]></content>
      <categories>
        <category>tensorflow</category>
      </categories>
      <tags>
        <tag>LSTM</tag>
        <tag>RNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tensorflow学习6：卷积神经网络CNN]]></title>
    <url>%2F2018%2F04%2F08%2Ftensorflow%2FTensorflow%E5%AD%A6%E4%B9%A06%EF%BC%9A%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CCNN%2F</url>
    <content type="text"><![CDATA[用卷积神经网络进行手写数字识别 1234import tensorflow as tffrom tensorflow.examples.tutorials.mnist import input_dataPWS_DIR = "C:/Users/lenovo/Desktop/Python WORK SPACE/"LOG_DIR = "D:/Tensorflow/logs/" 1 普通代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697mnist = input_data.read_data_sets(PWS_DIR + "DATASET/MNIST_data", one_hot=True)batch_size = 100n_batch = mnist.train.num_examples // batch_size# 初始化权值def weight_variable(shape): initial = tf.truncated_normal(shape, stddev=0.1) return tf.Variable(initial)#初始化偏置def bias_variable(shape): initial = tf.constant(0.1, shape=shape) return tf.Variable(initial)#卷积层def conv2d(x, W): # x input tensor of shape [batch, in_height, in_width, in_channels] # W filter / kernel tensor of shape [filter_height, filter_weight, in_channels, out_channels] # "strides[0] = strides[3] = 1", strides[1]代表x方向的步长，strides[2]代表y方向的步长 # padding: A "string" from: "SAME", "VALID" ，由于SAME补0，所以卷积后不改变尺寸大小 return tf.nn.conv2d(x, W, strides=[1,1,1,1], padding="SAME")#池化层def max_pool_2x2(x): # ksize [1,x,y,1] 窗口大小，同上0，3位要设置为1 return tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding="SAME")#定义2个placeholderx = tf.placeholder(tf.float32, [None, 784]) # 28*28y = tf.placeholder(tf.float32, [None, 10])#改变x的shape为4维数据 [batch, in_height, in_width, in_channels]x_image = tf.reshape(x, [-1,28,28,1])#初始化第一个卷积层的权值和偏置W_conv1 = weight_variable([5,5,1,32]) #代表5*5的卷积核，1的输入深度，32的输出个数（即32个特征平面）b_conv1 = bias_variable([32]) # 一个卷积核对应一个偏置值，所以有32个#第一次卷积、池化操作h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)h_pool1 = max_pool_2x2(h_conv1)#初始化第二个卷积层的权值和偏置W_conv2 = weight_variable([5,5,32,64]) #代表5*5的卷积核，32的输入深度，64的输出个数（即64个特征平面）b_conv2 = bias_variable([64]) # 一个卷积核对应一个偏置值，所以有64个#第二次卷积、池化操作h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)h_pool2 = max_pool_2x2(h_conv2)'''28*28的图片，第一次卷积后为28*28，第一次池化后为14*14第二次卷积后为14*14，第二次池化后为7*7最后得到64张7*7的平面'''#把第二次池化的输出降为1维h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])#初始化第一个全连接层的权值和偏置W_fc1 = weight_variable([7*7*64, 1024]) #上一层有7*7*64个神经元，全连接层有1024个神经元b_fc1 = bias_variable([1024])#计算第一个全连接层h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)#定义比率keep_prob用来控制dropoutkeep_prob = tf.placeholder(tf.float32)h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)#初始化第二个全连接层的权值和偏置W_fc2 = weight_variable([1024, 10])b_fc2 = bias_variable([10])#计算第二个全连接层predict = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)#使用交叉熵代价函数loss_cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=predict))#使用AdamOptimizer进行优化train_step = tf.train.AdamOptimizer(1e-4).minimize(loss_cross_entropy)#结果放在一个BOOL型列表中predict_bool = tf.equal(tf.argmax(y, 1), tf.argmax(predict, 1))#求准确率accuracy = tf.reduce_mean(tf.cast(predict_bool, tf.float32))with tf.Session() as sess: sess.run(tf.global_variables_initializer()) for epoch in range(6): for batch in range(n_batch): batch_xs, batch_ys = mnist.train.next_batch(batch_size) sess.run(train_step, feed_dict=&#123;x:batch_xs, y:batch_ys, keep_prob:0.7&#125;) acc = sess.run(accuracy, feed_dict=&#123;x:mnist.test.images, y:mnist.test.labels, keep_prob:1.0&#125;) print("Iter" + str(epoch) + ", Testing Accuracy= " + str(acc)) Extracting C:/Users/lenovo/Desktop/Python WORK SPACE/DATASET/MNIST_data\train-images-idx3-ubyte.gzExtracting C:/Users/lenovo/Desktop/Python WORK SPACE/DATASET/MNIST_data\train-labels-idx1-ubyte.gzExtracting C:/Users/lenovo/Desktop/Python WORK SPACE/DATASET/MNIST_data\t10k-images-idx3-ubyte.gzExtracting C:/Users/lenovo/Desktop/Python WORK SPACE/DATASET/MNIST_data\t10k-labels-idx1-ubyte.gzIter0, Testing Accuracy= 0.8614Iter1, Testing Accuracy= 0.9578Iter2, Testing Accuracy= 0.9769Iter3, Testing Accuracy= 0.9819Iter4, Testing Accuracy= 0.9836Iter5, Testing Accuracy= 0.9857 2 结构化代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148mnist = input_data.read_data_sets(PWS_DIR + "DATASET/MNIST_data", one_hot=True)batch_size = 100n_batch = mnist.train.num_examples // batch_size# 设计统计函数，用于计算传入的张量的各种统计量def variable_summary(var): with tf.name_scope("summary"): mean = tf.reduce_mean(var) tf.summary.scalar("mean", mean) #平均值 with tf.name_scope("stddev"): stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean))) tf.summary.scalar("stddev", stddev) #标准差 tf.summary.scalar("max", tf.reduce_max(var)) #最大值 tf.summary.scalar("min", tf.reduce_min(var)) #最小值 tf.summary.histogram("histogram", var) #直方图 # 初始化权值def weight_variable(shape): initial = tf.truncated_normal(shape, stddev=0.1) return tf.Variable(initial)#初始化偏置def bias_variable(shape): initial = tf.constant(0.1, shape=shape) return tf.Variable(initial)#卷积层def conv2d(x, W): # x input tensor of shape [batch, in_height, in_width, in_channels] # W filter / kernel tensor of shape [filter_height, filter_weight, in_channels, out_channels] # "strides[0] = strides[3] = 1", strides[1]代表x方向的步长，strides[2]代表y方向的步长 # padding: A "string" from: "SAME", "VALID" ，由于SAME补0，所以卷积后不改变尺寸大小 return tf.nn.conv2d(x, W, strides=[1,1,1,1], padding="SAME")#池化层def max_pool_2x2(x): # ksize [1,x,y,1] 窗口大小，同上0，3位要设置为1 return tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding="SAME")with tf.name_scope("input"): #定义2个placeholder x = tf.placeholder(tf.float32, [None, 784]) # 28*28 y = tf.placeholder(tf.float32, [None, 10]) with tf.name_scope("x_image"): #改变x的shape为4维数据 [batch, in_height, in_width, in_channels] x_image = tf.reshape(x, [-1,28,28,1]) with tf.name_scope("Conv1"): #初始化第一个卷积层的权值和偏置 with tf.name_scope("W_conv1"): W_conv1 = weight_variable([5,5,1,32]) #代表5*5的卷积核，1的输入深度，32的输出个数（即32个特征平面） with tf.name_scope("b_conv1"): b_conv1 = bias_variable([32]) # 一个卷积核对应一个偏置值，所以有32个 #第一次卷积、池化操作 with tf.name_scope("h_conv1"): h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1) with tf.name_scope("h_pool1"): h_pool1 = max_pool_2x2(h_conv1) with tf.name_scope("Conv2"): #初始化第二个卷积层的权值和偏置 with tf.name_scope("W_conv2"): W_conv2 = weight_variable([5,5,32,64]) #代表5*5的卷积核，32的输入深度，64的输出个数（即64个特征平面） with tf.name_scope("b_conv2"): b_conv2 = bias_variable([64]) # 一个卷积核对应一个偏置值，所以有64个 #第二次卷积、池化操作 with tf.name_scope("h_conv2"): h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2) with tf.name_scope("h_pool2"): h_pool2 = max_pool_2x2(h_conv2)'''with tf.name_scope(""):28*28的图片，第一次卷积后为28*28，第一次池化后为14*14第二次卷积后为14*14，第二次池化后为7*7最后得到64张7*7的平面'''with tf.name_scope("fc1"): #把第二次池化的输出降为1维 with tf.name_scope("h_pool2_flat"): h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64]) #初始化第一个全连接层的权值和偏置 with tf.name_scope("W_fc1"): W_fc1 = weight_variable([7*7*64, 1024]) #上一层有7*7*64个神经元，全连接层有1024个神经元 with tf.name_scope("b_fc1"): b_fc1 = bias_variable([1024]) #计算第一个全连接层 with tf.name_scope("h_fc1_relu"): h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1) #定义比率keep_prob用来控制dropout with tf.name_scope("h_fc1_drop"): keep_prob = tf.placeholder(tf.float32) h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob) with tf.name_scope("fc2"): #初始化第二个全连接层的权值和偏置 with tf.name_scope("W_fc2"): W_fc2 = weight_variable([1024, 10]) with tf.name_scope("b_fc2"): b_fc2 = bias_variable([10]) #计算第二个全连接层 with tf.name_scope("h_fc2_softmax"): predict = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)with tf.name_scope("loss_cross_entropy"): #使用交叉熵代价函数 loss_cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=predict)) _loss = tf.summary.scalar("loss_cross_entropy", loss_cross_entropy) #使用AdamOptimizer进行优化with tf.name_scope("train"): train_step = tf.train.AdamOptimizer(1e-4).minimize(loss_cross_entropy) #求准确率with tf.name_scope("accuracy"): #结果放在一个BOOL型列表中 predict_bool = tf.equal(tf.argmax(y, 1), tf.argmax(predict, 1)) accuracy = tf.reduce_mean(tf.cast(predict_bool, tf.float32)) _accuracy = tf.summary.scalar("accuracy", accuracy)# merged = tf.summary.merge 这里训练看loss，测试看accuracy，不用合并with tf.Session() as sess: sess.run(tf.global_variables_initializer()) train_writer = tf.summary.FileWriter(LOG_DIR + "train", sess.graph) test_writer = tf.summary.FileWriter(LOG_DIR + "test", sess.graph) for epoch in range(6): for batch in range(n_batch): batch_xs, batch_ys = mnist.train.next_batch(batch_size) sess.run(train_step, feed_dict=&#123;x:batch_xs, y:batch_ys, keep_prob:0.7&#125;) train_summary = sess.run(summary_loss, feed_dict=&#123;x:batch_xs, y:batch_ys, keep_prob:1.0&#125;) train_writer.add_summary(train_summary, batch) acc = sess.run(accuracy, feed_dict=&#123;x:mnist.test.images, y:mnist.test.labels, keep_prob:1.0&#125;) test_summary = sess.run(summary_accuracy, feed_dict=&#123;x:mnist.test.images, y:mnist.test.labels, keep_prob:1.0&#125;) test_writer.add_summary(test_summary, epoch) print("Iter" + str(epoch) + ", Testing Accuracy= " + str(acc)) Extracting C:/Users/lenovo/Desktop/Python WORK SPACE/DATASET/MNIST_data\train-images-idx3-ubyte.gzExtracting C:/Users/lenovo/Desktop/Python WORK SPACE/DATASET/MNIST_data\train-labels-idx1-ubyte.gzExtracting C:/Users/lenovo/Desktop/Python WORK SPACE/DATASET/MNIST_data\t10k-images-idx3-ubyte.gzExtracting C:/Users/lenovo/Desktop/Python WORK SPACE/DATASET/MNIST_data\t10k-labels-idx1-ubyte.gzIter0, Testing Accuracy= 0.9401Iter1, Testing Accuracy= 0.974Iter2, Testing Accuracy= 0.9781Iter3, Testing Accuracy= 0.9823Iter4, Testing Accuracy= 0.9839Iter5, Testing Accuracy= 0.9853]]></content>
      <categories>
        <category>tensorflow</category>
      </categories>
      <tags>
        <tag>CNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tensorflow学习5-3：Tensorboard可视化]]></title>
    <url>%2F2018%2F04%2F07%2Ftensorflow%2FTensorflow%E5%AD%A6%E4%B9%A05-3%EF%BC%9ATensorboard%E5%8F%AF%E8%A7%86%E5%8C%96%2F</url>
    <content type="text"><![CDATA[对 5-2 进行修改，加入放映器（projector）实现分类过程中图片可视化 例子123import tensorflow as tffrom tensorflow.examples.tutorials.mnist import input_datafrom tensorflow.contrib.tensorboard.plugins import projector 1234567891011121314151617181920#载入数据集mnist = input_data.read_data_sets("MNIST_data", one_hot=True)# ---------------------------NEW ADD 1#运行次数max_steps = 31#图片数量image_num = 3000#文件路径DIR = "D:/Tensorflow/"#定义会话 sess = tf.Session()#载入图片（将n个行向量堆叠起来）embedding = tf.Variable(tf.stack(mnist.test.images[:image_num]), trainable=False, name = "embadding")# ------------------------------------#每个批次大小batch_size = 100 Extracting MNIST_data\train-images-idx3-ubyte.gzExtracting MNIST_data\train-labels-idx1-ubyte.gzExtracting MNIST_data\t10k-images-idx3-ubyte.gzExtracting MNIST_data\t10k-labels-idx1-ubyte.gz 1234567891011# STEP 1 设计统计函数，用于计算传入的张量的各种统计量def variable_summary(var): with tf.name_scope("summary"): mean = tf.reduce_mean(var) tf.summary.scalar("mean", mean) #平均值 with tf.name_scope("stddev"): stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean))) tf.summary.scalar("stddev", stddev) #标准差 tf.summary.scalar("max", tf.reduce_max(var)) #最大值 tf.summary.scalar("min", tf.reduce_min(var)) #最小值 tf.summary.histogram("histogram", var) #直方图 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108''' DEL 这次数据集很大，不计算批次数，而是自己指定 batch_num 即 max_steps#计算有多少批次 = 总数 // 批次batch_num = mnist.train.num_examples // batch_size'''with tf.name_scope("input"): input_x = tf.placeholder(tf.float32, [None, 784], name="input_x") input_y = tf.placeholder(tf.float32, [None, 10], name="input_y") # --------------------------------NEW ADD 2#显示图片with tf.name_scope("input_reshape"): image_shape_input = tf.reshape(input_x, [-1,28,28,1]) # -1表示未知 tf.summary.image("input", image_shape_input, 10) # 10张图片用来在 IMAGES 选项中预览# ------------------------------------#创建神经网络模型with tf.name_scope("layers"): W1 = tf.Variable(tf.truncated_normal([784,128], 0.,0.5), name="W1") variable_summary(W1) # STEP 2.1 b1 = tf.Variable(tf.zeros([128]) + 0.1, name="b1") variable_summary(b1) # STEP 2.1 L1 = tf.nn.relu(tf.matmul(input_x, W1) + b1, name="L1") W2 = tf.Variable(tf.truncated_normal([128,10], 0.,0.5), name="W2") b2 = tf.Variable(tf.zeros([10]) + 0.1, name="b2") L2 = tf.add(tf.matmul(L1, W2), b2, name="L2")with tf.name_scope("loss"): loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=L2, labels=input_y)) tf.summary.scalar("loss", loss) # STEP 2.2with tf.name_scope("train_and_optimizer"): train = tf.train.GradientDescentOptimizer(0.2).minimize(loss)#获取用于显示的精度——优化效果with tf.name_scope("accuracy"): correct_indices = tf.equal(tf.argmax(input_y, 1), tf.argmax(L2, 1)) accuracy = tf.reduce_mean(tf.cast(correct_indices, tf.float32)) tf.summary.scalar("accuracy", accuracy) # STEP 2.2 # -----------------------NEW ADD 3#产生metadata文件，写入测试集的10进制labelsif tf.gfile.Exists(DIR + "projector/projector/meadata.tsv"): tf.gfile.DeleteRecursively(DIR + "projector/projector/metadata.tsv")with open(DIR + "projector/projector/metadata.tsv", "w") as f: labels = sess.run(tf.argmax(mnist.test.labels[:], 1)) # 把one-hot转为10进制标签 for i in range(image_num): f.write(str(labels[i]) + "\n") projector_writer = tf.summary.FileWriter(DIR + "projector/projector/", sess.graph) saver = tf.train.Saver() #保存模型config = projector.ProjectorConfig()embed = config.embeddings.add()embed.tensor_name = embedding.nameembed.metadata_path = DIR + "projector/projector/metadata.tsv"embed.sprite.image_path = DIR + "projector/data/mnist_10k_sprite.png"embed.sprite.single_image_dim.extend([28,28])projector.visualize_embeddings(projector_writer, config)# ------------------------------------# STEP 3:合并所有summarymerged = tf.summary.merge_all()''' DEL init = tf.global_variables_initializer()with tf.Session() as sess: sess.run(init) # STEP 2 writer = tf.summary.FileWriter("D:/Tensorflow/logs", sess.graph) for epoch in range(5): for batch in range(batch_num): batch_xs, batch_ys = mnist.train.next_batch(batch_size) # STEP 4:每次训练，计算一次merge summary summary,_ = sess.run([merged, train], feed_dict=&#123;input_x:batch_xs, input_y:batch_ys&#125;) # STEP 5:选择多久更新写入一次summary writer.add_summary(summary, epoch) _accuracy = sess.run(accuracy, feed_dict=&#123;input_x:mnist.test.images, input_y:mnist.test.labels&#125;) print("epoch:"+ str(epoch) + ", accuracy:"+ str(_accuracy))'''# -----------------------NEW ADD 4：由于自己指定batch_num 数量很多，所以不需要来回迭代，修改如下sess.run(tf.global_variables_initializer())writer = tf.summary.FileWriter("D:/Tensorflow/logs", sess.graph)for i in range(max_steps): batch_xs, batch_ys = mnist.train.next_batch(batch_size) # 这里加入2个参数 run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE) run_metadata = tf.RunMetadata() summary,_ = sess.run([merged, train], feed_dict=&#123;input_x:batch_xs, input_y:batch_ys&#125;, options = run_options, run_metadata = run_metadata) projector_writer.add_run_metadata(run_metadata, "step%03d" % i) projector_writer.add_summary(summary, i) if i%3 == 0: _accuracy = sess.run(accuracy, feed_dict=&#123;input_x:mnist.test.images, input_y:mnist.test.labels&#125;) print("step:"+ str(i) + ", accuracy:"+ str(_accuracy))# 模型保存到如下路径saver.save(sess, DIR + "projector/projector/a_model.ckpt", global_step = max_steps)projector_writer.close()sess.close()# ----------------------------- step:0, accuracy:0.1275step:3, accuracy:0.3143step:6, accuracy:0.5248step:9, accuracy:0.5986step:12, accuracy:0.6266step:15, accuracy:0.6751step:18, accuracy:0.6905step:21, accuracy:0.7041step:24, accuracy:0.7387step:27, accuracy:0.747step:30, accuracy:0.7538 总结输入下面代码运行 tensorboard --logdir=D:/Tensorflow/projector/projector 可以在 Tensorboard 的 IMAGES 和 EMBEDDINGS 选项卡查看新效果 效果如图：]]></content>
      <categories>
        <category>tensorflow</category>
      </categories>
      <tags>
        <tag>Tensorboard</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tensorflow学习5-2：Tensorboard网络运行]]></title>
    <url>%2F2018%2F04%2F06%2Ftensorflow%2FTensorflow%E5%AD%A6%E4%B9%A05-2%EF%BC%9ATensorboard%E7%BD%91%E7%BB%9C%E8%BF%90%E8%A1%8C%2F</url>
    <content type="text"><![CDATA[用 Tensorboard 查看网络运行下的各种统计量变化 例子12import tensorflow as tffrom tensorflow.examples.tutorials.mnist import input_data 1234567891011# STEP 1: 设计统计函数，用于计算传入的张量的各种统计量def variable_summary(var): with tf.name_scope("summary"): mean = tf.reduce_mean(var) tf.summary.scalar("mean", mean) #平均值 with tf.name_scope("stddev"): stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean))) tf.summary.scalar("stddev", stddev) #标准差 tf.summary.scalar("max", tf.reduce_max(var)) #最大值 tf.summary.scalar("min", tf.reduce_min(var)) #最小值 tf.summary.histogram("histogram", var) #直方图 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152#载入数据集mnist = input_data.read_data_sets("MNIST_data", one_hot=True)#每个批次大小batch_size = 100#计算有多少批次batch_num = mnist.train.num_examples // batch_sizewith tf.name_scope("input"): input_x = tf.placeholder(tf.float32, [None, 784], name="input_x") input_y = tf.placeholder(tf.float32, [None, 10], name="input_y")#创建神经网络模型with tf.name_scope("layers"): W1 = tf.Variable(tf.truncated_normal([784,128], 0.,0.5), name="W1") variable_summary(W1) # STEP 2.1 b1 = tf.Variable(tf.zeros([128]) + 0.1, name="b1") variable_summary(b1) # STEP 2.1 L1 = tf.nn.relu(tf.matmul(input_x, W1) + b1, name="L1") W2 = tf.Variable(tf.truncated_normal([128,10], 0.,0.5), name="W2") b2 = tf.Variable(tf.zeros([10]) + 0.1, name="b2") L2 = tf.add(tf.matmul(L1, W2), b2, name="L2")with tf.name_scope("loss"): loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=L2, labels=input_y)) tf.summary.scalar("loss", loss) # STEP 2.2with tf.name_scope("train_and_optimizer"): train = tf.train.GradientDescentOptimizer(0.2).minimize(loss)#获取用于显示的精度——优化效果with tf.name_scope("accuracy"): correct_indices = tf.equal(tf.argmax(input_y, 1), tf.argmax(L2, 1)) accuracy = tf.reduce_mean(tf.cast(correct_indices, tf.float32)) tf.summary.scalar("accuracy", accuracy) # STEP 2.2 # STEP 3:合并所有summarymerged = tf.summary.merge_all()init = tf.global_variables_initializer()with tf.Session() as sess: sess.run(init) # STEP 4 writer = tf.summary.FileWriter("D:/Tensorflow/logs", sess.graph) for epoch in range(5): for batch in range(batch_num): batch_xs, batch_ys = mnist.train.next_batch(batch_size) # STEP 5:每次训练，计算一次merge summary summary,_ = sess.run([merged, train], feed_dict=&#123;input_x:batch_xs, input_y:batch_ys&#125;) # STEP 6:选择多久更新写入一次summary writer.add_summary(summary, epoch) _accuracy = sess.run(accuracy, feed_dict=&#123;input_x:mnist.test.images, input_y:mnist.test.labels&#125;) print("epoch:"+ str(epoch) + ", accuracy:"+ str(_accuracy)) Extracting MNIST_data\train-images-idx3-ubyte.gzExtracting MNIST_data\train-labels-idx1-ubyte.gzExtracting MNIST_data\t10k-images-idx3-ubyte.gzExtracting MNIST_data\t10k-labels-idx1-ubyte.gzepoch:0, accuracy:0.9063epoch:1, accuracy:0.9252epoch:2, accuracy:0.9332epoch:3, accuracy:0.9426epoch:4, accuracy:0.9456 总结Tensorboard显示指定张量的统计量方法步骤：STEP 1:设计自定义函数计算统计量STEP 2:在想指定的张量后插入函数2.1:对于权值、偏置等要查看各种统计量的，使用自定义函数 variable_summary2.2:对于只需要显示自身数值的张量，使用 tf.summary.scalarSTEP 3:在Session()前，合并所有summarySTEP 4:在Session()里，定义writer，确定日志目录STEP 5:每次训练后，计算一次mergedSTEP 6:选择多久更新写入一次summarySTEP 7:运行代码后启动tensorboard1tensorboard --logdir=D:/logs]]></content>
      <categories>
        <category>tensorflow</category>
      </categories>
      <tags>
        <tag>Tensorboard</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tensorflow学习5-1：Tensorboard网络结构]]></title>
    <url>%2F2018%2F04%2F05%2Ftensorflow%2FTensorflow%E5%AD%A6%E4%B9%A05-1%EF%BC%9ATensorboard%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%2F</url>
    <content type="text"><![CDATA[例子12import tensorflow as tffrom tensorflow.examples.tutorials.mnist import input_data 123456789101112131415161718192021222324252627282930313233343536373839404142#载入数据集mnist = input_data.read_data_sets("MNIST_data", one_hot=True)#每个批次大小batch_size = 100#计算有多少批次batch_num = mnist.train.num_examples // batch_size# STEP 1with tf.name_scope("input"): input_x = tf.placeholder(tf.float32, [None, 784], name="input_x") input_y = tf.placeholder(tf.float32, [None, 10], name="input_y")#创建神经网络模型with tf.name_scope("layers"): W1 = tf.Variable(tf.truncated_normal([784,128], 0.,0.5), name="W1") b1 = tf.Variable(tf.zeros([128]) + 0.1, name="b1") L1 = tf.nn.relu(tf.matmul(input_x, W1) + b1, name="L1") W2 = tf.Variable(tf.truncated_normal([128,10], 0.,0.5), name="W2") b2 = tf.Variable(tf.zeros([10]) + 0.1, name="b2") L2 = tf.add(tf.matmul(L1, W2), b2, name="L2")with tf.name_scope("loss"): loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=L2, labels=input_y))with tf.name_scope("train_and_optimizer"): train = tf.train.GradientDescentOptimizer(0.2).minimize(loss)#获取用于显示的精度——优化效果with tf.name_scope("accuracy"): correct_indices = tf.equal(tf.argmax(input_y, 1), tf.argmax(L2, 1)) accuracy = tf.reduce_mean(tf.cast(correct_indices, tf.float32))init = tf.global_variables_initializer()with tf.Session() as sess: sess.run(init) # STEP 2 writer = tf.summary.FileWriter("D:/Tensorboard/logs", sess.graph) for epoch in range(5): for batch in range(batch_num): batch_xs, batch_ys = mnist.train.next_batch(batch_size) sess.run(train, feed_dict=&#123;input_x:batch_xs, input_y:batch_ys&#125;) _accuracy = sess.run(accuracy, feed_dict=&#123;input_x:mnist.test.images, input_y:mnist.test.labels&#125;) print("epoch:"+ str(epoch) + ", accuracy:"+ str(_accuracy)) Extracting MNIST_data\train-images-idx3-ubyte.gzExtracting MNIST_data\train-labels-idx1-ubyte.gzExtracting MNIST_data\t10k-images-idx3-ubyte.gzExtracting MNIST_data\t10k-labels-idx1-ubyte.gzepoch:0, accuracy:0.909epoch:1, accuracy:0.9272epoch:2, accuracy:0.9358epoch:3, accuracy:0.942epoch:4, accuracy:0.9459 总结Tensorboard使用方法： STEP 1:在sess外部给tensor加上命名空间name_scope 和 name。name_scope对应矩形name对应矩形内部的tensor123with tf.name_scope(&quot;input&quot;): input_x = tf.placeholder(tf.float32, [None, 784], name=&quot;input_x&quot;) input_y = tf.placeholder(tf.float32, [None, 10], name=&quot;input_y&quot;) STEP 2:在sess内部加上打印日志1writer = tf.summary.FileWriter(&quot;logs&quot;, sess.graph) STEP 3:运行代码。如果之前运行过，可以restart kernel选择“Restart &amp; Run All” STEP 4:将tensorboard目录D:\Anaconda3\envs\tensorflow\Scripts添加的环境变量中，打开anaconda cmd，切换到log的相应盘符下，否则tensorboard无法显示。接下来输入命令1tensorboard --logdir=D:/Tensorflow/logs 复制生成的地址在浏览器打开]]></content>
      <categories>
        <category>tensorflow</category>
      </categories>
      <tags>
        <tag>Tensorboard</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tensorflow学习8：模型的保存和加载]]></title>
    <url>%2F2018%2F04%2F01%2Ftensorflow%2FTensorflow%E5%AD%A6%E4%B9%A08%EF%BC%9A%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BF%9D%E5%AD%98%E5%92%8C%E5%8A%A0%E8%BD%BD%2F</url>
    <content type="text"><![CDATA[123import tensorflow as tffrom tensorflow.examples.tutorials.mnist import input_dataMOD_DIR = "D:/Tensorflow/models/" 1# 模型的保存 12345678910111213141516171819202122232425262728293031323334353637mnist = input_data.read_data_sets("MNIST_data", one_hot=True)batch_size = 100batch_num = mnist.train.num_examples // batch_sizeinput_x = tf.placeholder(tf.float32, [None, 784])input_y = tf.placeholder(tf.float32, [None, 10])W1 = tf.Variable(tf.truncated_normal([784,128], 0.,0.5))b1 = tf.Variable(tf.zeros([128]) + 0.1)L1 = tf.nn.relu(tf.matmul(input_x, W1) + b1)W2 = tf.Variable(tf.truncated_normal([128,10], 0.,0.5))b2 = tf.Variable(tf.zeros([10]) + 0.1)L2 = tf.matmul(L1, W2) + b2loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=L2, labels=input_y))train = tf.train.GradientDescentOptimizer(0.2).minimize(loss)correct_indices = tf.equal(tf.argmax(input_y, 1), tf.argmax(L2, 1))accuracy = tf.reduce_mean(tf.cast(correct_indices, tf.float32))# STEP 1:定义saver对象saver = tf.train.Saver()init = tf.global_variables_initializer()with tf.Session() as sess: sess.run(init) for epoch in range(11): for batch in range(batch_num): batch_xs, batch_ys = mnist.train.next_batch(batch_size) sess.run(train, feed_dict=&#123;input_x:batch_xs, input_y:batch_ys&#125;) acc = sess.run(accuracy, feed_dict=&#123;input_x:mnist.test.images, input_y:mnist.test.labels&#125;) print("epoch:"+ str(epoch) + ", accuracy:"+ str(acc)) # STEP2:用saver保存模型 saver.save(sess, MOD_DIR + "test_net.ckpt") Extracting MNIST_data\train-images-idx3-ubyte.gz Extracting MNIST_data\train-labels-idx1-ubyte.gz Extracting MNIST_data\t10k-images-idx3-ubyte.gz Extracting MNIST_data\t10k-labels-idx1-ubyte.gz epoch:0, accuracy:0.9057 epoch:1, accuracy:0.9222 epoch:2, accuracy:0.9296 epoch:3, accuracy:0.9394 epoch:4, accuracy:0.9423 epoch:5, accuracy:0.9448 epoch:6, accuracy:0.9489 epoch:7, accuracy:0.9479 epoch:8, accuracy:0.9516 epoch:9, accuracy:0.9507 epoch:10, accuracy:0.9552 123456789101112131415161718192021222324252627282930313233mnist = input_data.read_data_sets("MNIST_data", one_hot=True)batch_size = 100batch_num = mnist.train.num_examples // batch_sizeinput_x = tf.placeholder(tf.float32, [None, 784])input_y = tf.placeholder(tf.float32, [None, 10])W1 = tf.Variable(tf.truncated_normal([784,128], 0.,0.5))b1 = tf.Variable(tf.zeros([128]) + 0.1)L1 = tf.nn.relu(tf.matmul(input_x, W1) + b1)W2 = tf.Variable(tf.truncated_normal([128,10], 0.,0.5))b2 = tf.Variable(tf.zeros([10]) + 0.1)L2 = tf.matmul(L1, W2) + b2loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=L2, labels=input_y))train = tf.train.GradientDescentOptimizer(0.2).minimize(loss)correct_indices = tf.equal(tf.argmax(input_y, 1), tf.argmax(L2, 1))accuracy = tf.reduce_mean(tf.cast(correct_indices, tf.float32))# STEP 1:定义saver对象saver = tf.train.Saver()init = tf.global_variables_initializer()with tf.Session() as sess: sess.run(init) print(sess.run(accuracy, feed_dict=&#123;input_x:mnist.test.images, input_y:mnist.test.labels&#125;)) # STEP2:用saver加载模型，打印载入训练模型后的准确率变化 saver.restore(sess, MOD_DIR + "test_net.ckpt") print(sess.run(accuracy, feed_dict=&#123;input_x:mnist.test.images, input_y:mnist.test.labels&#125;)) Extracting MNIST_data\train-images-idx3-ubyte.gz Extracting MNIST_data\train-labels-idx1-ubyte.gz Extracting MNIST_data\t10k-images-idx3-ubyte.gz Extracting MNIST_data\t10k-labels-idx1-ubyte.gz 0.1311 INFO:tensorflow:Restoring parameters from D:/Tensorflow/models/test_net.ckpt 0.9552 总结：如果是.pb模型文件， 保存用 graph_def() 导入用 tf.import_graph_def()]]></content>
      <categories>
        <category>tensorflow</category>
      </categories>
      <tags>
        <tag>model</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tensorflow GPU版本安装]]></title>
    <url>%2F2018%2F03%2F20%2Finstall%20and%20config%2FTensorflow%20GPU%E7%89%88%E6%9C%AC%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[第一步，下载安装CUDA1 Tensorflow 1.8.0，对应CUDA9.0版。 2 安装前如果C:\Program Files下有NVIDIA Corporation等文件夹，先删除。 3 添加环境变量：12C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.0\binC:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.0\lib\x64 4 注销后，重新登录让环境变量生效。 第二步，下载安装cudnn1 这个相当于CUDA补丁，下载需要注册NVIDIA账号。 2 下载完后解压，拷贝到CUDA安装目录。 第三步，安装Tensorflow GPU版本1 先卸载旧版，防止冲突。 2 然后，执行下面命令，下载最新版1.8.0 的Tensorflow1pip install tensorflow-gpu]]></content>
      <categories>
        <category>install and config</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[ssr+vultr翻墙]]></title>
    <url>%2F2018%2F03%2F20%2Fgoogle%2Fssr%2Bvultr%E7%BF%BB%E5%A2%99%2F</url>
    <content type="text"><![CDATA[要用到的工具：1.Sstap（工具已经放到网盘了，需要的直接下载安装即可。）2.Xshell远程代理工具。工具下载链接: https://pan.baidu.com/s/1-1k63VJZmLGdvTUYvyKNBw 密码: gunv3.国外的服务器一台。服务器推荐使用vultr,这家的服务器比较便宜。 教程开始： 第一步、服务器购买1.进入官网，点击“create Account”创建账号，创建过程挺简单的，需要注意的是：密码需要大写+小写+数字。2.进入首页点击“Service”服务器3 选择服务器的位置，我选择的纽约的服务器（日本离中国最近，但是日本的服务器，最便宜也要5美元一个月，所以选择了纽约的服务器） Service type服务器系统选择默认第一个就好了。 Service Size我选择的最便宜的，如果不是用于建站，数据过大的话，购买2.5美刀/月的就足够了。 付款使用支付宝支付就行了，最低充值10美刀，充值之后再点Deploy Now购买，vultr是按小时来计算收费的，如何使用不方便可以删除，重新换个地区的服务器来部署。 第二步：部署我们刚刚买的vps服务器1.安装好Xshell，来远程部署服务器，点击文件-新建。随便取个名字，然后把你的服务器ip填上接国外ip即服务器时，软件会先后提醒你输入用户名和密码，就是你服务的账号密码，在vultr里面可以看到。出现图上情况，代理连接成功。3.安装SSR执行以下命令 1wget --no-check-certificate https://freed.ga/github/shadowsocksR.sh; bash shadowsocksR.sh 若提示：wget :command not found请执行：yum install wget -y然后再按照提示走就行了这张图注意保存走到这一步，还有些不够，网速很慢，几乎慢到连不到网，所以进行第三步。 第三步：锐速脚本安装一键更换内核脚本（Vultr需先执行此脚本） 1wget -N --no-check-certificate https://freed.ga/kernel/ruisu.sh &amp;&amp; bash ruisu.sh 脚本安装需要1-3分钟，耐心等待服务器重启。在服务器重启之后，重新连接继续下面安装 锐速安装脚本 1wget -N --no-check-certificate https://github.com/91yun/serverspeeder/raw/master/serverspeeder.sh &amp;&amp; bash serverspeeder.sh 备用脚本 1wget -N --no-check-certificate https://raw.githubusercontent.com/91yun/serverspeeder/master/serverspeeder-all.sh &amp;&amp; bash serverspeeder-all.sh 出现这些就算大功告成了。 第四步：下载sstap,进行连接。（不要进行升级）钩选“编辑并激活使用”保存。 模式选择，就可以使用了最后附上Google一张。]]></content>
      <categories>
        <category>google</category>
      </categories>
      <tags>
        <tag>翻墙</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tensorflow学习4： MNIST数据集手写数字识别]]></title>
    <url>%2F2018%2F02%2F04%2Ftensorflow%2FTensorflow%E5%AD%A6%E4%B9%A04%EF%BC%9AMNIST%E6%95%B0%E6%8D%AE%E9%9B%86%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E8%AF%86%E5%88%AB%2F</url>
    <content type="text"><![CDATA[1 MNIST数据集手写数字识别（简单版）12import tensorflow as tffrom tensorflow.examples.tutorials.mnist import input_data 12345678910111213141516171819202122232425262728293031323334353637#载入数据集mnist = input_data.read_data_sets("MNIST_data", one_hot=True)#每个批次大小batch_size = 100#计算有多少批次batch_num = mnist.train.num_examples // batch_sizeinput_x = tf.placeholder(tf.float32, [None, 784])input_y = tf.placeholder(tf.float32, [None, 10])#创建神经网络模型W1 = tf.Variable(tf.truncated_normal([784,128], 0.,0.5))b1 = tf.Variable(tf.zeros([128]) + 0.1)L1 = tf.nn.relu(tf.matmul(input_x, W1) + b1)W2 = tf.Variable(tf.truncated_normal([128,10], 0.,0.5))b2 = tf.Variable(tf.zeros([10]) + 0.1)L2 = tf.matmul(L1, W2) + b2#tf.nn.softmax_cross_entropy_with_logitsloss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=L2, labels=input_y))#loss = tf.reduce_mean(tf.square(L2 - input_y))train = tf.train.GradientDescentOptimizer(0.2).minimize(loss)#获取用于显示的精度——优化效果correct_indices = tf.equal(tf.argmax(input_y, 1), tf.argmax(L2, 1))accuracy = tf.reduce_mean(tf.cast(correct_indices, tf.float32))init = tf.global_variables_initializer()with tf.Session() as sess: sess.run(init) for epoch in range(20): for batch in range(batch_num): batch_xs, batch_ys = mnist.train.next_batch(batch_size) sess.run(train, feed_dict=&#123;input_x:batch_xs, input_y:batch_ys&#125;) acc = sess.run(accuracy, feed_dict=&#123;input_x:mnist.test.images, input_y:mnist.test.labels&#125;) print("epoch:"+ str(epoch) + ", accuracy:"+ str(acc)) Extracting MNIST_data\train-images-idx3-ubyte.gzExtracting MNIST_data\train-labels-idx1-ubyte.gzExtracting MNIST_data\t10k-images-idx3-ubyte.gzExtracting MNIST_data\t10k-labels-idx1-ubyte.gzepoch:0, accuracy:0.9072epoch:1, accuracy:0.9248epoch:2, accuracy:0.9319epoch:3, accuracy:0.9375epoch:4, accuracy:0.9432epoch:5, accuracy:0.9469epoch:6, accuracy:0.9502epoch:7, accuracy:0.9534epoch:8, accuracy:0.9533epoch:9, accuracy:0.9574epoch:10, accuracy:0.9557epoch:11, accuracy:0.9577epoch:12, accuracy:0.9558epoch:13, accuracy:0.9588epoch:14, accuracy:0.9593epoch:15, accuracy:0.9595epoch:16, accuracy:0.9604epoch:17, accuracy:0.9613epoch:18, accuracy:0.9608epoch:19, accuracy:0.962 总结：1 整除 //2 最后一层不使用激活函数，可以使用softmax 2 MNIST数据集手写数字识别（优化版）12import tensorflow as tffrom tensorflow.examples.tutorials.mnist import input_data 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849#载入数据集mnist = input_data.read_data_sets("MNIST_data", one_hot=True)#每个批次大小batch_size = 100#计算有多少批次batch_num = mnist.train.num_examples // batch_sizekeep_prob = tf.constant(1., tf.float32)lr = tf.Variable(0.01, dtype=tf.float32)input_x = tf.placeholder(tf.float32, [None, 784])input_y = tf.placeholder(tf.float32, [None, 10])#创建神经网络模型W1 = tf.Variable(tf.truncated_normal([784,300], 0.,0.5))b1 = tf.Variable(tf.zeros([300]) + 0.1)L1 = tf.nn.relu(tf.matmul(input_x, W1) + b1)L1_drop = tf.nn.dropout(L1, keep_prob=keep_prob)W2 = tf.Variable(tf.truncated_normal([300,100], 0.,0.5))b2 = tf.Variable(tf.zeros([100]) + 0.1)L2 = tf.nn.relu(tf.matmul(L1_drop, W2) + b2)W3 = tf.Variable(tf.truncated_normal([100,10], 0.,0.5))b3 = tf.Variable(tf.zeros([10]) + 0.1)L3 = tf.matmul(L2, W3) + b3#tf.nn.softmax_cross_entropy_with_logitsloss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=L3, labels=input_y))# train = tf.train.RMSPropOptimizer(lr).minimize(loss)train = tf.train.RMSPropOptimizer(lr).minimize(loss)#获取用于显示的精度——优化效果correct_indices = tf.equal(tf.argmax(input_y, 1), tf.argmax(L3, 1))accuracy = tf.reduce_mean(tf.cast(correct_indices, tf.float32))init = tf.global_variables_initializer()with tf.Session() as sess: sess.run(init) for epoch in range(20): #每次迭代更新学习率 sess.run(tf.assign(lr, 0.01*(0.9**epoch) )) for batch in range(batch_num): batch_xs, batch_ys = mnist.train.next_batch(batch_size) sess.run(train, feed_dict=&#123;input_x:batch_xs, input_y:batch_ys&#125;) _accuracy = sess.run(accuracy, feed_dict=&#123;input_x:mnist.test.images, input_y:mnist.test.labels&#125;) print("epoch:"+ str(epoch) + ", accuracy:"+ str(_accuracy)) Extracting MNIST_data\train-images-idx3-ubyte.gzExtracting MNIST_data\train-labels-idx1-ubyte.gzExtracting MNIST_data\t10k-images-idx3-ubyte.gzExtracting MNIST_data\t10k-labels-idx1-ubyte.gzepoch:0, accuracy:0.9192epoch:1, accuracy:0.9496epoch:2, accuracy:0.9543epoch:3, accuracy:0.951epoch:4, accuracy:0.9553epoch:5, accuracy:0.9631epoch:6, accuracy:0.9695epoch:7, accuracy:0.9679epoch:8, accuracy:0.968epoch:9, accuracy:0.9705epoch:10, accuracy:0.9659epoch:11, accuracy:0.9745epoch:12, accuracy:0.9742epoch:13, accuracy:0.9757epoch:14, accuracy:0.9765epoch:15, accuracy:0.9769epoch:16, accuracy:0.9768epoch:17, accuracy:0.9771epoch:18, accuracy:0.977epoch:19, accuracy:0.9771 总结：1 动态学习率2 dropout3 加1个隐层4 使用RMSPropOptimizer优化器]]></content>
      <categories>
        <category>tensorflow</category>
      </categories>
      <tags>
        <tag>MNIST</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tensorflow学习3：非线性回归]]></title>
    <url>%2F2018%2F02%2F03%2Ftensorflow%2FTensorflow%E5%AD%A6%E4%B9%A03%EF%BC%9A%E9%9D%9E%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%2F</url>
    <content type="text"><![CDATA[123import tensorflow as tfimport numpy as npimport matplotlib.pyplot as plt 123456789101112131415161718192021222324252627282930313233# 用numpy生成200个随机点x_data = np.linspace(-0.5, 0.5, 200)[:,np.newaxis] #增加一个维度，变成单列矩阵noise = np.random.normal(0,0.02,x_data.shape)y_data = np.square(x_data) + noise#x = tf.placeholder(tf.float32,[None,1])y = tf.placeholder(tf.float32,[None,1])Weights_L1 = tf.Variable(tf.random_normal((1,10)))biases_L1 = tf.Variable(tf.zeros((1,10)))Wx_plus_b_L1 = tf.matmul(x, Weights_L1) + biases_L1L1 = tf.nn.softplus(Wx_plus_b_L1)Weights_L2 = tf.Variable(tf.random_normal((10,1)))biases_L2 = tf.Variable(tf.zeros((1,1)))Wx_plus_Biases_L2 = tf.matmul(L1, Weights_L2) + biases_L2#predict_y = tf.nn.tanh(Wx_plus_Biases_L2)predict_y = Wx_plus_Biases_L2loss = tf.reduce_mean(tf.square(predict_y - y))train = tf.train.GradientDescentOptimizer(0.1).minimize(loss)with tf.Session() as sess: sess.run(tf.global_variables_initializer()) for _ in range(1000): sess.run(train, feed_dict=&#123;x:x_data, y:y_data&#125;) _predict_y = sess.run(predict_y, feed_dict=&#123;x:x_data&#125;) plt.figure() plt.scatter(x_data, y_data) plt.plot(x_data, _predict_y, "red", lw=5) plt.show()]]></content>
      <categories>
        <category>tensorflow</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Tensorflow学习2：线性回归]]></title>
    <url>%2F2018%2F02%2F02%2Ftensorflow%2FTensorflow%E5%AD%A6%E4%B9%A02%EF%BC%9A%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%2F</url>
    <content type="text"><![CDATA[123import tensorflow as tfimport numpy as npimport matplotlib.pyplot as plt 12345678910111213141516171819202122232425262728293031323334#使用numpy生成100个随机点#y_data = 3*x_datax_data = []y_data = []for i in range(100): x_data.append(np.random.normal(0.0, 0.5)) y_data.append( x_data[i]*0.2 + 0.3 + np.random.normal(0,0.03) )#构建一个线性模型b = tf.Variable(0.)k = tf.Variable(0.)y = k*x_data + b#均方误差作为损失函数loss = tf.reduce_mean(tf.square(y - y_data))#定义一个梯度下降优化器optimizer = tf.train.GradientDescentOptimizer(0.2)#最小化损失函数train = optimizer.minimize(loss)init = tf.global_variables_initializer()with tf.Session() as sess: sess.run(init) for step in range(40): sess.run(train) if step%4 == 0: # print(sess.run([k,b])) y_predict = sess.run(y) plt.plot(x_data, y_predict, color="red", lw=3) plt.scatter(x_data, y_data, color="blue") plt.show() 总结：1 数值0 用0.表示当作float2 训练目标变量用tf.Variable，其他的用numpy普通变量]]></content>
      <categories>
        <category>tensorflow</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Tensorflow学习1：基本使用]]></title>
    <url>%2F2018%2F02%2F01%2Ftensorflow%2FTensorflow%E5%AD%A6%E4%B9%A01%EF%BC%9A%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[.eval() 和 sess.run()区别：最主要的区别就在于使用.eval()只能获取这个tensor的值，而使用sess.run()可以同时获取多个tensor中的值，12345678t = tf.constant(42.0)u = tf.constant(37.0)tu = tf.mul(t, u)ut = tf.mul(u, t)with sess.as_default(): tu.eval() # runs one step ut.eval() # runs one step sess.run([tu, ut]) # evaluates both tensors in a single step 变量 和 Session()定义1234567891011121314import tensorflow as tfa = 3# Create a variable.w = tf.Variable([[0.5,1.0]])x = tf.Variable([[2.0],[1.0]]) y = tf.matmul(w, x) #variables have to be explicitly initialized before you can run Opsinit_op = tf.global_variables_initializer()with tf.Session() as sess: #第一种 session()定义方式 sess.run(init_op) print (y.eval()) Tensor(&quot;Variable_11/read:0&quot;, shape=(1, 2), dtype=float32) [[ 2.]] 计算操作需要在 Session()——会话（计算图的区域） 中进行；而且在使用 variable 时还需要 初始化全局变量操作 矩阵 和 常量1234567891011121314151617181920212223# float32tf.zeros([3, 4], int32) ==&gt; [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]]# 'tensor' is [[1, 2, 3], [4, 5, 6]]tf.zeros_like(tensor) ==&gt; [[0, 0, 0], [0, 0, 0]]tf.ones([2, 3], int32) ==&gt; [[1, 1, 1], [1, 1, 1]]# 'tensor' is [[1, 2, 3], [4, 5, 6]]tf.ones_like(tensor) ==&gt; [[1, 1, 1], [1, 1, 1]]# Constant 1-D Tensor populated with value list.tensor = tf.constant([1, 2, 3, 4, 5, 6, 7]) =&gt; [1 2 3 4 5 6 7]# Constant 2-D tensor populated with scalar value -1.tensor = tf.constant(-1.0, shape=[2, 3]) =&gt; [[-1. -1. -1.] [-1. -1. -1.]]tf.linspace(10.0, 12.0, 3, name="linspace") =&gt; [ 10.0 11.0 12.0]# 令'start' is 3# 'limit' is 18# 'delta' is 3tf.range(start, limit, delta) ==&gt; [3, 6, 9, 12, 15] 对比 Numpy 操作，发现区别并不大 另一种 Session() 定义12345678910111213import tensorflow as tf# 初始化生成一个 均值为-1，方差为4的矩阵norm = tf.random_normal([2, 3], mean=-1, stddev=4)# Shuffle the first dimension of a tensorc = tf.constant([[1, 2], [3, 4], [5, 6]])shuff = tf.random_shuffle(c)# Each time we run these ops, different results are generatedsess = tf.Session() #第二种 session()定义方式print (sess.run(norm))print (sess.run(shuff))sess.close() #这种方法没有明确的区域，所以要关闭来指定作用域 [[ 3.78657341 -3.94182277 -3.65419507] [-7.0396409 -5.51102114 3.56082773]] [[5 6] [1 2] [3 4]] 累加 演示12345678910state = tf.Variable(0)new_value = tf.add(state, tf.constant(1))update = tf.assign(state, new_value)with tf.Session() as sess: sess.run(tf.global_variables_initializer()) print(sess.run(state)) for _ in range(3): sess.run(update) print(sess.run(state)) 0 1 2 3 train.Saver() 保存 Session() 操作123456789101112#tf.train.Saverw = tf.Variable([[0.5,1.0]])x = tf.Variable([[2.0],[1.0]])y = tf.matmul(w, x)init_op = tf.global_variables_initializer()saver = tf.train.Saver()with tf.Session() as sess: sess.run(init_op)# Do some work with the model.# Save the variables to disk. save_path = saver.save(sess, "C://tensorflow//model//test") print ("Model saved in file: ", save_path) Model saved in file: C://tensorflow//model//test convert_to_tensor() 转换 numpy 的语法为 tensorflow 类型12345import numpy as npa = np.zeros((3,3))ta = tf.convert_to_tensor(a)with tf.Session() as sess: print(sess.run(ta)) [[ 0. 0. 0.] [ 0. 0. 0.] [ 0. 0. 0.]] 不推荐使用，还是建议使用 tensorflow 语法 Fetch and FeedFetch 把多个op放在一个数组中一起run 1234567891011# Fetchimput1 = tf.constant(3.0)imput2 = tf.constant(2.0)imput3 = tf.constant(1.0)add = tf.add(imput1, imput2)mul = tf.multiply(imput1, imput2)with tf.Session() as sess: res = sess.run([add, mul]) print(res) [5.0, 6.0] Feed 在运行时，用字典结构给占位符赋值 123456789imput1 = tf.placeholder(tf.float32)imput2 = tf.placeholder(tf.float32)output = tf.add(imput1, imput2)with tf.Session() as sess: print(sess.run(output, feed_dict=&#123; imput1:[1], imput2:[2] &#125;)) 使用flags定义命令行参数tf定义了tf.app.flags，用于支持接受命令行传递参数，相当于接受argv。1234567891011121314151617import tensorflow as tf#第一个是参数名称，第二个参数是默认值，第三个是参数描述tf.app.flags.DEFINE_string('str_name', 'def_v_1',"descrip1")tf.app.flags.DEFINE_integer('int_name', 10,"descript2")tf.app.flags.DEFINE_boolean('bool_name', False, "descript3")FLAGS = tf.app.flags.FLAGS#必须带参数，否则：'TypeError: main() takes no arguments (1 given)'; main的参数名随意定义，无要求def main(_): print(FLAGS.str_name) print(FLAGS.int_name) print(FLAGS.bool_name)if __name__ == '__main__': tf.app.run() #执行main函数 执行：12345678910//不带参数时：[root@test]# python tt.pydef_v_110False//给定参数时：[root@test]# python tt.py --str_name test_str --int_name 99 --bool_name Truetest_str99True]]></content>
      <categories>
        <category>tensorflow</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Tensorflow专题4：迁移学习(transfer learning)和微调fine-tune的区别及迁移学习代码实现]]></title>
    <url>%2F2018%2F01%2F04%2Ftensorflow%2FTensorflow%E4%B8%93%E9%A2%984%EF%BC%9A%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0(transfer%20learning)%E5%92%8C%E5%BE%AE%E8%B0%83fine-tune%E7%9A%84%E5%8C%BA%E5%88%AB%E5%8F%8A%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[一：区别 1：迁移学习是将已经学习到的知识应用到其他领域，比如通用的语音模型迁移到某个人的语音模型上。 ​ 迁移学习就是将一个问题上训练好的模型通过简单的调整使其适用于一个新的问题。 ​ 例如利用ImageNet数据集上训练好的Inception-V3模型来解决一个新的图像分类问题，可以保留训练好的Inception-v3模型中所有卷积层的参数，只是替换最后一层全连接层，在最后这一层全连接层之前的网络层称为瓶颈层。而将新的图像通过训练好的卷积神经网络直到瓶颈层的过程可以看成是对图像进行特征提取的过程，瓶颈层输出再通过一个单层的全连接层神经网络可以很好的区分类别，所以有理由相信将瓶颈层的输出的节点向量可以被称为任何图像的更加精简且表达能力更强的特征向量。所以可以直接利用这个训练好的神经网络对图像进行特征提取，然后再将提取得到特征向量作为输入来训练一个新的单层全连接网络来处理分类问题。 ​ 但是在数据量足够的情况下，迁移学习的效果不如完全重新训练，但是迁移学习所需要的训练时间和训练样本要远远小于训练完整的模型。 ​ 比如把已经训练好的模型的某一层的输出拿出来，然后用一个svm、LR等分类，更好的去利用从某一层输出的特征（也叫知识），这也还是迁移学习的思想，如下前三个是transfer learning经常用到的方法。最后一个是finetune的思想。 把Alexnet里卷积层最后一层输出的特征拿出来，然后直接用SVM分类。这是Transfer Learning，因为你用到了Alexnet中已经学到了的 “知识”。把Vggnet卷积层最后的输出拿出来，用贝叶斯分类器分类。思想基本同上。甚至你可以把Alexnet、Vggnet的输出拿出来进行组合，自己设计一个分类器分类。这个过程中你不仅用了Alexnet的“知识”，也用了Vggnet的“知识”。https://github.com/Gogul09/flower-recognition（此方法实现代码）最后，你也可以直接使用 fine-tune这种方法，在Alexnet的基础上，重新加上全连接层，再去训练网络。 2：finetune（微调）：例子：在Alexnet的基础上，我们重新加上一个层再去训练网络，比如再加入一个全连接层，那就是先 固定前面的层，让新加的fc层的loss值降低到一个很低的值，再调低学习率，放开所有层一块去训练这样可以收敛到一个不错的效果。 3：所以我个人认为迁移学习直接将现有的或者从现有的模型中提取出来的有用的东西应用的另一个领域，不在进行训练之前的网络部分，只需要训练我们添加部分网络的部分，将迁移过来的模型的某一层的输出作为我们新增加网络部分的输入。 而finetune就是微调，思想是：利用原有模型的参数信息，作为我们要训练的新的模型的初始化参数，这个新的模型可以和原来一样也可以增添几个层（进行适当的调整）。 4：传统的机器学习框架下，学习的任务是在给定充分训练数据集的基础上学习一个分类模型；然后利用这个学习到的模型来对测试文档进行分类和预测。然而，我们看到机器学习算法在当前web挖掘应用领域存在一个关键问题：一些新出现的领域中的大量训练数据非常难得到。web领域中大量新的数据不断涌现，从传统的新闻，网页，到图片，再到博客，播客等。传统的机器学习需要对每个领域都标定大量训练数据，这将会耗费大量的人力物力，而没有大量的标注数据，会使得很多与学习相关研究与应用无法开展，其次传统的机器学习假设训练数据与测试数据服从相同的数据分布。然而在很多情况下，这种相同分布不满足，通常可能发生的情况如训练数据过期。如果我们有大量的，在不同分布下的训练数据，完全丢弃这些数据也是非常浪费的，如何利用这些数据就是迁移学习主要解决的问题。 迁移学习可以从现有的数据中迁移知识，用来帮助将来的学习。 迁移学习的目标是将从一个环境中学到的知识用来帮助新环境中的学习任务，因此迁移学习不会想传统机器学习那样作同分布假设。 例子：一个会下象棋的人可以更容易的学会下围棋。 迁移学习目前分为一下三个部分：同构空间下基于实例的迁移学习；同构空间下基于特征的迁移学习；异构空间下的迁移学习。 基于实例的迁移学习有更强的知识迁移能力，基于特征的迁移学习具有更广规范的知识迁移能力；异构空间的迁移具有广泛的学习与扩展能力。 迁移学习即一种学习对另一种学习的影响，它广泛的存在于知识技能态度和行为的规范的学习中，任何一种学习都将受先验知识的影响，只要有学习就有迁移，迁移是学习的继续和巩固，优势提高和深化学习的条件，学习与迁移不可分割。 二：迁移学习实例 为了能够快速地训练好自己的花朵图片分类器，我们可以使用别人已经训练好的模型参数，在此基础之上训练我们的模型。这个便属于迁移学习。本文提供训练数据集和代码下载。 原理：卷积神经网络模型总体上可以分为两部分，前面的卷积层和后面的全连接层。卷积层的作用是图片特征的提取，全连接层作用是特征的分类。我们的思路便是在inception-v3网络模型上，修改全连接层，保留卷积层。卷积层的参数使用的是别人已经训练好的，全连接层的参数需要我们初始化并使用我们自己的数据来训练和学习。 和微调fine-tune的区别及迁移学习代码实现/2018-07-15-19-26-42.png) 上面inception-v3模型图红色箭头前面部分是卷积层，后面是全连接层。我们需要修改修改全连接层，同时把模型的最终输出改为5。 由于这里使用了tensorflow框架，所以，我们需要获取上图红色箭头所在位置的张量BOTTLENECK_TENSOR_NAME（最后一个卷积层激活函数的输出值，个数为2048）以及模型最开始的输入数据的张量JPEG_DATA_TENSOR_NAME。获取这两个张量的作用是，图片训练数据通过JPEG_DATA_TENSOR_NAME张量输入模型，通过BOTTLENECK_TENSOR_NAME张量获取通过卷积层之后的图片特征。 BOTTLENECK_TENSOR_SIZE = 2048 BOTTLENECK_TENSOR_NAME = ‘pool_3/_reshape:0’ JPEG_DATA_TENSOR_NAME = ‘DecodeJpeg/contents:0’ 通过下面的代码加载模型，同时获取上面所述的两个张量。 和微调fine-tune的区别及迁移学习代码实现/2018-07-15-19-27-04.png) 最后便是定义交叉熵损失函数。模型使用反向传播训练，而训练的参数并不是模型的所有参数，仅仅是全连接层的参数，卷积层的参数是不变的。 定义交叉熵损失函数。 cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=ground_truth_input) cross_entropy_mean = tf.reduce_mean(cross_entropy) train_step = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(cross_entropy_mean) 那么接下来的是如何给我们的模型输入数据了，这里提供了几个操作数据的函数。由于训练数据集比较小， 先把所有的图片通过JPEG_DATA_TENSOR_NAME张量输入模型，然后获取BOTTLENECK_TENSOR_NAME张量的值并保存到硬盘中。 在模型训练的时候，从硬盘中读取所保存的BOTTLENECK_TENSOR_NAME张量的值作为全连接层的输入数据。因为一张图片可能会被使用多次。 和微调fine-tune的区别及迁移学习代码实现/2018-07-15-19-27-25.png) 运行代码在到时候再去看我的pycharm中的trasform这个项目。 这个代码实现部分参考的是https://blog.csdn.net/liangyihuai/article/details/79219457这个博客的内容 完整代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231# coding=utf8import globimport os.pathimport randomimport numpy as npimport tensorflow as tffrom tensorflow.python.platform import gfileBOTTLENECK_TENSOR_SIZE = 2048#最后一个卷积层激活函数输出值，个数是2048个1×1的BOTTLENECK_TENSOR_NAME = 'pool_3/_reshape:0'#张量获取通过卷积层之后的图片特征JPEG_DATA_TENSOR_NAME = 'DecodeJpeg/contents:0'#模型开始的输入数据的张量，张量输入MODEL_DIR = './inception_dec_2015'#inception模型的闻之MODEL_FILE= 'tensorflow_inception_graph.pb'#模型文件的准确位置CACHE_DIR = './bottleneck'#最后一个卷积层输出的每一类的特征，这个要输入到fc中然后进行分类用的。INPUT_DATA = './flower_photos'VALIDATION_PERCENTAGE = 10#TEST_PERCENTAGE = 10#验证集测试集都占10%LEARNING_RATE = 0.01STEPS = 4000BATCH = 100def create_image_lists(testing_percentage, validation_percentage): result = &#123;&#125; sub_dirs = [x[0] for x in os.walk(INPUT_DATA)]#os.walk()方法的作用是在目录树中游走输出在目录中的文件名，向上或者向下。 is_root_dir = True for sub_dir in sub_dirs: if is_root_dir: is_root_dir = False continue extensions = ['jpg', 'jpeg', 'JPG', 'JPEG'] file_list = [] dir_name = os.path.basename(sub_dir) for extension in extensions: file_glob = os.path.join(INPUT_DATA, dir_name, '*.' + extension) file_list.extend(glob.glob(file_glob)) if not file_list: continue label_name = dir_name.lower() #初始化 training_images = [] testing_images = [] validation_images = [] for file_name in file_list: base_name = os.path.basename(file_name)#这个只是取回去文件名，去掉其路径 #basename的作用是去掉目录的路径，只返回文件名，而dirname用于 去掉文件名，只返回目录所在的路径。os.split()的作用是返回路径名和文件名的元组 # 随机划分数据 chance = np.random.randint(100) if chance &lt; validation_percentage: validation_images.append(base_name) elif chance &lt; (testing_percentage + validation_percentage): testing_images.append(base_name) else: training_images.append(base_name) result[label_name] = &#123; 'dir': dir_name, 'training': training_images, 'testing': testing_images, 'validation': validation_images, &#125; return resultdef get_image_path(image_lists, image_dir, label_name, index, category): label_lists = image_lists[label_name] category_list = label_lists[category] mod_index = index % len(category_list) base_name = category_list[mod_index] sub_dir = label_lists['dir'] full_path = os.path.join(image_dir, sub_dir, base_name) return full_pathdef get_bottleneck_path(image_lists, label_name, index, category): return get_image_path(image_lists, CACHE_DIR, label_name, index, category) + '.txt'def run_bottleneck_on_image(sess, image_data, image_data_tensor, bottleneck_tensor): bottleneck_values = sess.run(bottleneck_tensor, &#123;image_data_tensor: image_data&#125;) bottleneck_values = np.squeeze(bottleneck_values) return bottleneck_valuesdef get_or_create_bottleneck(sess, image_lists, label_name, index, category, jpeg_data_tensor, bottleneck_tensor): label_lists = image_lists[label_name] sub_dir = label_lists['dir'] sub_dir_path = os.path.join(CACHE_DIR, sub_dir) if not os.path.exists(sub_dir_path): os.makedirs(sub_dir_path) bottleneck_path = get_bottleneck_path(image_lists, label_name, index, category) if not os.path.exists(bottleneck_path): image_path = get_image_path(image_lists, INPUT_DATA, label_name, index, category) image_data = gfile.FastGFile(image_path, 'rb').read() bottleneck_values = run_bottleneck_on_image(sess, image_data, jpeg_data_tensor, bottleneck_tensor) bottleneck_string = ','.join(str(x) for x in bottleneck_values) with open(bottleneck_path, 'w') as bottleneck_file: bottleneck_file.write(bottleneck_string) else: with open(bottleneck_path, 'r') as bottleneck_file: bottleneck_string = bottleneck_file.read() bottleneck_values = [float(x) for x in bottleneck_string.split(',')] return bottleneck_valuesdef get_random_cached_bottlenecks(sess, n_classes, image_lists, how_many, category, jpeg_data_tensor, bottleneck_tensor): bottlenecks = [] ground_truths = [] for _ in range(how_many): label_index = random.randrange(n_classes) label_name = list(image_lists.keys())[label_index] image_index = random.randrange(65536) bottleneck = get_or_create_bottleneck( sess, image_lists, label_name, image_index, category, jpeg_data_tensor, bottleneck_tensor) ground_truth = np.zeros(n_classes, dtype=np.float32) ground_truth[label_index] = 1.0 bottlenecks.append(bottleneck) ground_truths.append(ground_truth) return bottlenecks, ground_truthsdef get_test_bottlenecks(sess, image_lists, n_classes, jpeg_data_tensor, bottleneck_tensor): bottlenecks = [] ground_truths = [] label_name_list = list(image_lists.keys()) for label_index, label_name in enumerate(label_name_list): category = 'testing' for index, unused_base_name in enumerate(image_lists[label_name][category]): bottleneck = get_or_create_bottleneck(sess, image_lists, label_name, index, category,jpeg_data_tensor, bottleneck_tensor) ground_truth = np.zeros(n_classes, dtype=np.float32) ground_truth[label_index] = 1.0 bottlenecks.append(bottleneck) ground_truths.append(ground_truth) return bottlenecks, ground_truthsdef main(): image_lists = create_image_lists(TEST_PERCENTAGE, VALIDATION_PERCENTAGE) n_classes = len(image_lists.keys()) # 读取已经训练好的Inception-v3的模型 with gfile.FastGFile(os.path.join(MODEL_DIR, MODEL_FILE), 'rb') as f: graph_def = tf.GraphDef() graph_def.ParseFromString(f.read()) bottleneck_tensor, jpeg_data_tensor = tf.import_graph_def( graph_def, return_elements=[BOTTLENECK_TENSOR_NAME, JPEG_DATA_TENSOR_NAME]) # 定义新的神经网络输入 bottleneck_input = tf.placeholder(tf.float32, [None, BOTTLENECK_TENSOR_SIZE], name='BottleneckInputPlaceholder') ground_truth_input = tf.placeholder(tf.float32, [None, n_classes], name='GroundTruthInput') # 定义一个权链接层 with tf.name_scope('final_training_ops'): weights = tf.Variable(tf.truncated_normal([BOTTLENECK_TENSOR_SIZE, n_classes], stddev=0.001)) biases = tf.Variable(tf.zeros([n_classes])) logits = tf.matmul(bottleneck_input, weights) + biases final_tensor = tf.nn.softmax(logits) # 定义交叉商损失函数 cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=ground_truth_input) cross_entropy_mean = tf.reduce_mean(cross_entropy) train_step = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(cross_entropy_mean) # 计算准确率 with tf.name_scope('evaluation'): correct_prediction = tf.equal(tf.argmax(final_tensor, 1), tf.argmax(ground_truth_input, 1)) #tf.argmax()的用法，tf.argmax(final_tensor,1)返回的是final_tensor中，值最大的一个值，1在这里是指的返回一个值 #final_tensor返回的是概率的大小，返回的是概率的最大值作为最后的值 #tf.equal()的用法是比较tf.argmax(final_tensor, 1)和 tf.argmax(ground_truth_input, 1)对应位置的值是否相等， #相等的时候返回true，否则返回false，然后统计true多占的比列就是最后的准确率。 evaluation_step = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) #tf.cast()的作用是改变数据类型的，改变correct_prediction的数据类型为float32 #tf.reduce_mean（）的用法，在tensor的某一维度上，计算元素的平均值，由于输出的维度比原tensor降低了，所以也叫做降为。 with tf.Session() as sess: sess.run(tf.global_variables_initializer()) # 训练过程 for i in range(STEPS): train_bottlenecks, train_ground_truth = get_random_cached_bottlenecks( sess, n_classes, image_lists, BATCH, 'training', jpeg_data_tensor, bottleneck_tensor) sess.run(train_step, feed_dict=&#123;bottleneck_input: train_bottlenecks, ground_truth_input: train_ground_truth&#125;) #这里是train_bottlenecks，从磁盘读入的张量值作为输入向量，来训练全链接层， if i % 100 == 0 or i + 1 == STEPS: validation_bottlenecks, validation_ground_truth = get_random_cached_bottlenecks( sess, n_classes, image_lists, BATCH, 'validation', jpeg_data_tensor, bottleneck_tensor) validation_accuracy = sess.run(evaluation_step, feed_dict=&#123; bottleneck_input: validation_bottlenecks, ground_truth_input: validation_ground_truth&#125;) print('Step %d: Validation accuracy on random sampled %d examples = %.1f%%' % (i, BATCH, validation_accuracy * 100)) # 在最后的测试数据上测试正确率 test_bottlenecks, test_ground_truth = get_test_bottlenecks( sess, image_lists, n_classes, jpeg_data_tensor, bottleneck_tensor) test_accuracy = sess.run(evaluation_step, feed_dict=&#123; bottleneck_input: test_bottlenecks, ground_truth_input: test_ground_truth&#125;) print('Final test accuracy = %.1f%%' % (test_accuracy * 100))if __name__ == '__main__':#防止导入的包中的内容，也就是import 后面的内容也被运行 main() #python是脚本语言，不像编译语言一样，先将程序编译成二进制再运行 ，而是动态的逐行解释运行，也就是从脚本的第一行开始运行，没有统一的入口。 #一个python源码除了可以直接运行外，还可以最为模块，也就是库导入，不管是导入还是运行，最顶层的代码都会被运行，python用缩进来区分代码层次，而 #实际上在导入的时候，有一部分代码我们是不希望被运行的。 #if __name__ =='main'就相当于程序的入口，python本身并没有规定这莫写，这只是一种编程习惯，由于模块之间相互引用，不同模块可能有这样的定义 #，而入口程序只能有一个，到底那一额入口程序被选中，这就取决于 __name__的值。 #__name__可以清晰的反映一个模块在包中的层次，其实，所谓模块的在包中的层次。__name__是内置变量，用于表示当前模块的名字，同时还能反映一个 #包的结构，如果模块直接运行的，则代码被运行，如果模块被导入的，则代码不能运行。]]></content>
      <categories>
        <category>tensorflow</category>
      </categories>
      <tags>
        <tag>transfer learning</tag>
        <tag>fine-tune</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tensorflow专题3：模型优化策略]]></title>
    <url>%2F2018%2F01%2F03%2Ftensorflow%2FTensorflow%E4%B8%93%E9%A2%983%EF%BC%9A%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96%E7%AD%96%E7%95%A5%2F</url>
    <content type="text"><![CDATA[初始化优化 权值W使用：tf.random_truncated偏置值b使用：0.112W1 = tf.Variable(tf.truncated_normal([784,128], 0.,0.5))b1 = tf.Variable(tf.zeros([128]) + 0.1) 过拟合优化 增加数据集 正则化 dropout：每次停用部分神经元12# keep_prob是设置层L1工作神经元的百分比tf.nn.dopout(L1, keep_prob) 收敛速度优化 更改损失函数 loss，如交叉熵 更改优化器算法 optimizer，如自适应优化算法 更改学习率。如动态学习率]]></content>
      <categories>
        <category>tensorflow</category>
      </categories>
      <tags>
        <tag>模型优化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tensorflow专题2：优化器Optimizer]]></title>
    <url>%2F2018%2F01%2F02%2Ftensorflow%2FTensorflow%E4%B8%93%E9%A2%982%EF%BC%9A%E4%BC%98%E5%8C%96%E5%99%A8Optimizer%2F</url>
    <content type="text"><![CDATA[一、优化器介绍tensorflow优化器12345678910tf.train.GradientDescentOptimizertf.train.AdadeltaOptimizertf.train.AdagradOptimizertf.train.AdagradDAOptimizertf.train.MomentumOptimizertf.train.AdamOptimizertf.train.FtrlOptimizertf.train.ProximalGradientDescentOptimizertf.train.ProximalAdagradOptimizertf.train.RMSPropOptimizer1234567891011 1）GradientDescent梯度下降法TF函数实现：tf.train.GradientDescentOptimizer 设：$h_W(x)=W_1x$，其参数是W_1，则其损失函数是：$J(W_1)=\frac{1}{2m}\sum^{m}_{i=1}(h_W(x^{i})-y^{i})^2$则W1W1通过如下求得：$minimize\ J(W_1)$ 标准梯度下降法：标准梯度下降先计算所有样本汇总误差，然后根据总误差来更新权值。随机梯度下降法：随机梯度下降随机抽取一个样本来计算误差，然后更新权值。批量梯度下降法：批量梯度下降算是一种折中的方案，从总样本中选取一个批次（比如一共有10000个样本，随机选取100个样本作为一个batch），然后计算这个batch的总误差，根据总误差来更新权值。 问题1.合适的学习率，α 比较难获得α 过大导致震荡无法得到最优解，过小导致学习过程漫长。2.对所有参数学习率只有一个，如果数据是稀疏的，并且特征具有不同的频率时，更倾向于对不同频率特征使用不同的学习率，对很少发生的特征采用较大的学习率。3.目标函数门限需要提前定义，一旦计算中小于门限就停止，数据调度训练的选择对其有影响，通常使用shuffle打断以减小这种影响。4.高维非凸误差函数最小求解技术难度大。 2）MomentumTF函数实现：tf.train.MomentumOptimizer动量法ρ：动力通常设为 0.9 $W_t=ρW_{t-1}-η\nabla_W {J(W)}$当前权值的改变会受到上一次全职改变的影响，类似小球向下滚动的时候带上了惯性。这样可以加快小球向下的速度。 3）NAG（Nesterov accelerated gradient）TF函数实现：tf.train.MomentumOptimizer $W_t=ρW_{t-1}-η\nabla_W {J(W-ρW_{t-1})}$NAG在TF中跟Momentum合并在同一个函数tf.train.MomentumOptimizer中，可以通过参数配置启用。在Momentum中小球会盲目跟从下坡的梯度，容易发生错误，所以我们需要一个更聪明的小球，这个小球提前知道它要去哪里，它还要知道走到坡底的时候速度慢下来而不是又冲上另一个坡。我们可以提前计算下一个位置的梯度，然后使用到当前位置。 4）AdagradTF函数实现：tf.train.AdagradOptimizertf.train.AdagradDAOptimizer Adagrad会累加之前所有的梯度平方。$ΔW_t=-\frac η{\sqrt{\sum_{τ}^{t}g_τ^2+ϵ}}⨀g_t$$W_{t+1}=W_t-\frac η{\sqrt{\sum_{τ}^{t}g_τ^2+ϵ}}⨀g_t$通常上述的求和针对一个窗长w求解，以减小运算量。 它是基于SGD的一种算法，它的核心思想是对比比较常见的数据给予它比较小的学习率去调整参数，对于比较罕见的数据给予它较大的学习率去调整参数。它很适合应用于数据稀疏的数据集（比如一个图片的数据集，有10000张狗的照片，10000张猫的照片，只有100张大象的照片）。Adagrad主要的优势在于不需要人为的调节学习率，它可以自动调节。它的缺点在于，随着迭代次数的增多，学习率也会越来越低，最终会趋向于0。 5）RMSpropTF函数实现：tf.train.RMSPropOptimizer $E|g^2|_t=0.9E|g^2|_{t-1}+0.1g^2_t$$W_{t+1}=W_t-\frac η{\sqrt {E|g2|_t+ϵ}}⨀g_t$学习速率梯度均方根均值指数衰减。 RMSprop借鉴了一些Adagrad的思想，不过这里RMSprop只用到了前t-1次梯度平方的平均值加上当前梯度的平方的和的开平方作为学习率的分母。这样RMSprop不会出现学习率越来越低的问题，而且也能自己调节学习率，并且可以有一个比较好的效果。 6）AdadeltaTF函数实现：tf.train.AdadeltaOptimizer 该算法不需要手动调优学习速率α，抗噪声能力强，可以选择不同的模型结构。Adadelta是对Adagrad的扩展。Adadelta只累加固定大小的项，并且也不直接存储这些项，仅仅是计算对应的平均值。上一节的参数更新模型是：$W_{t+1}=W_t - α\frac{dJ(W)}{dW_t}$为方便，把$\frac{dJ(W)}{dW_t}$记作$g_t$，把平滑后的梯度记作$E|g|_t$,则其平方表示如下：$E|g^2|_t = ρE|g^2|_{t-1}+(1-ρ)g^2_t$其中ρ是平滑/衰减因子。其均方根得到的该值如下：$RMS|g|_t=\sqrt {E|g^2|_t+ϵ}$其中ϵ是为了防止后续计算分母为零而引入的参数常量。$ΔW=-\frac{RMS|ΔW|_{t-1}}{RMS|g|_t}g_t$$W_t+1=W_t+ΔW_t$ Adadelta t时刻跟新过程如下：前提衰减因子ρ，常数ϵ，初始的W值。 1 计算变量E|g2|0=0E|g2|0=0，E|ΔW2|0=0E|ΔW2|0=02: for t=1:T do %%Loop over #of updates 3: 计算梯度：$g_t $4: 滑动平均梯度:$E|g^2|_t = ρE|g^2|_{t-1}+(1-ρ)g^2_t$ 计算参数跟新$ΔW=-\frac{RMS|ΔW|_{t-1}}{RMS|g|_t}g_t$ 计算更新$E|Δx^2|_t=ρE|ΔW^2|_{t-1} + (1-ρ)W^2$ 更新参数$W_{t+1}=W_t+ΔW_t$ 8.end for 7）AdamTF函数实现：tf.train.AdamOptimizer Adam(Adaptive Moment Estimation)加上了bias校正和momentum，在优化末期，梯度更稀疏时，它比RMSprop稍微好点。$m_t=β_1m_{t-1}+(1-β_1)g_t$$v_t=β_2v_{t-1}+(1-β_2)g^2_t$其中$m_t$是梯度均值，$v_t$是梯度偏方差。这两个值初始化时为0的张量。在训练开始时，$m_t$和$v_t$趋向于零。可以使用如下估计方法抵消：$\hat m_t=\frac{m_t}{1-β^t_1}$$\hat v_t=\frac {v_t}{1-β^t_2}$$W_{t+1}=W_t-\frac η{\sqrt{\hat v_t+ϵ}}\hat m_t$就像Adadelta和RMSprop一样Adam会存储之前衰减的平方梯度，同时它也会保存之前衰减的梯度。经过一些处理之后再使用类似Adadelta和RMSprop的方式更新参数。 tf.train.FtrlOptimizertf.train.ProximalGradientDescentOptimizertf.train.ProximalAdagradOptimizer其它梯度优化方法1.数据重拍(shuffle函数)和数据多次重复训练2.批量归一化，防止逐级训练中的梯度消失和溢出3.提前终止，防止过拟合，监控验证数据集在训练中的损失，合适时提前终止。4.增加高斯分布的梯度噪声，$g_{t,i}=g_{t,i}+N(0,δ^2)$$δ^2_t=\frac η{(1+t)^γ}$这使得网络对初始化不敏感。 二、总结如何选用optimizer 对于稀疏数据：使用学习率可自适应的优化方法如：RMSprop，不用手动调节，而且最好采用默认值 追求精度：SGD。虽然通常训练时间更长，容易陷入鞍点，但是在好的初始化和学习率调度方案的情况下，结果更可靠 如果在意更快的收敛，并且需要训练较深较复杂的网络时：推荐使用学习率自适应的优化方法，如：Adadelta，RMSprop，Adam是比较相近的算法，在相似的情况下表现差不多。NAG收敛也比较快。]]></content>
      <categories>
        <category>tensorflow</category>
      </categories>
      <tags>
        <tag>Optimizer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tensorflow专题1：生成随机数]]></title>
    <url>%2F2018%2F01%2F01%2Ftensorflow%2FTensorflow%E4%B8%93%E9%A2%981%EF%BC%9A%E7%94%9F%E6%88%90%E9%9A%8F%E6%9C%BA%E6%95%B0%2F</url>
    <content type="text"><![CDATA[随机数函数 tf.random_normal &amp; tf.random_uniform &amp; tf.truncated_normal &amp; tf.random_shuffle tf.random_normal从正态分布输出随机值。 1random_normal(shape,mean=0.0,stddev=1.0,dtype=tf.float32,seed=None,name=None) shape：一个一维整数张量或Python数组。代表张量的形状。mean：数据类型为dtype的张量值或Python值。是正态分布的均值。stddev：数据类型为dtype的张量值或Python值。是正态分布的标准差dtype： 输出的数据类型。seed：一个Python整数。是随机种子。name： 操作的名称(可选) tf.random_uniform从均匀分布中返回随机值。 12345678random_uniform( shape,# 生成的张量的形状 minval=0, maxval=None, dtype=tf.float32, seed=None, name=None ) 返回值的范围默认是0到1的左闭右开区间，即[0，1)。minval为指定最小边界，默认为1。maxval为指定的最大边界，如果是数据浮点型则默认为1，如果数据为整形则必须指定。 tf.truncated_normal截断的正态分布函数。生成的值遵循一个正态分布，但如果生成的值大于平均值2个标准偏差的值则丢弃重新选择。故，只取横轴区间（μ-2σ，μ+2σ）内的值，面积为95.449974% 12345678truncated_normal( shape,#一个一维整数张量或Python数组。代表张量的形状。 mean=0.0,#数据类型为dtype的张量值或Python值。是正态分布的均值。 stddev=1.0,#数据类型为dtype的张量值或Python值。是正态分布的标准差 dtype=tf.float32,#输出的数据类型。 seed=None,#一个Python整数。是随机种子。 name=None#操作的名称(可选) ) tf.random_shuffle沿着要被洗牌的张量的第一个维度，随机打乱。 12345random_shuffle( value,# 要被洗牌的张量 seed=None, name=None ) 即下面这种效果：123[[1, 2], [[5, 6], [3, 4], ==&gt; [1, 2], [5, 6]] [3, 4]] 附录1：生成随机数的操作的源码random_ops.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454# Copyright 2015 The TensorFlow Authors. All Rights Reserved. # # Licensed under the Apache License, Version 2.0 (the "License"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an "AS IS" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. # ============================================================================== """Operations for generating random numbers.""" from __future__ import absolute_import from __future__ import division from __future__ import print_function import numpy as np from tensorflow.python.framework import dtypes from tensorflow.python.framework import ops from tensorflow.python.framework import random_seed from tensorflow.python.ops import array_ops from tensorflow.python.ops import control_flow_ops from tensorflow.python.ops import gen_random_ops from tensorflow.python.ops import math_ops # go/tf-wildcard-import # pylint: disable=wildcard-import from tensorflow.python.ops.gen_random_ops import * # pylint: enable=wildcard-import def _ShapeTensor(shape): """Convert to an int32 or int64 tensor, defaulting to int32 if empty.""" if isinstance(shape, (tuple, list)) and not shape: dtype = dtypes.int32 else: dtype = None return ops.convert_to_tensor(shape, dtype=dtype, name="shape") # pylint: disable=protected-access def random_normal(shape, mean=0.0, stddev=1.0, dtype=dtypes.float32, seed=None, name=None): """Outputs random values from a normal distribution. Args: shape: A 1-D integer Tensor or Python array. The shape of the output tensor. mean: A 0-D Tensor or Python value of type `dtype`. The mean of the normal distribution. stddev: A 0-D Tensor or Python value of type `dtype`. The standard deviation of the normal distribution. dtype: The type of the output. seed: A Python integer. Used to create a random seed for the distribution. See @&#123;tf.set_random_seed&#125; for behavior. name: A name for the operation (optional). Returns: A tensor of the specified shape filled with random normal values. """ with ops.name_scope(name, "random_normal", [shape, mean, stddev]) as name: shape_tensor = _ShapeTensor(shape) mean_tensor = ops.convert_to_tensor(mean, dtype=dtype, name="mean") stddev_tensor = ops.convert_to_tensor(stddev, dtype=dtype, name="stddev") seed1, seed2 = random_seed.get_seed(seed) rnd = gen_random_ops._random_standard_normal( shape_tensor, dtype, seed=seed1, seed2=seed2) mul = rnd * stddev_tensor value = math_ops.add(mul, mean_tensor, name=name) return value ops.NotDifferentiable("RandomStandardNormal") def parameterized_truncated_normal(shape, means=0.0, stddevs=1.0, minvals=-2.0, maxvals=2.0, dtype=dtypes.float32, seed=None, name=None): """Outputs random values from a truncated normal distribution. The generated values follow a normal distribution with specified mean and standard deviation, except that values whose magnitude is more than 2 standard deviations from the mean are dropped and re-picked. Args: shape: A 1-D integer Tensor or Python array. The shape of the output tensor. means: A 0-D Tensor or Python value of type `dtype`. The mean of the truncated normal distribution. stddevs: A 0-D Tensor or Python value of type `dtype`. The standard deviation of the truncated normal distribution. minvals: A 0-D Tensor or Python value of type `dtype`. The minimum value of the truncated normal distribution. maxvals: A 0-D Tensor or Python value of type `dtype`. The maximum value of the truncated normal distribution. dtype: The type of the output. seed: A Python integer. Used to create a random seed for the distribution. See @&#123;tf.set_random_seed&#125; for behavior. name: A name for the operation (optional). Returns: A tensor of the specified shape filled with random truncated normal values. """ with ops.name_scope(name, "parameterized_truncated_normal", [shape, means, stddevs, minvals, maxvals]) as name: shape_tensor = _ShapeTensor(shape) means_tensor = ops.convert_to_tensor(means, dtype=dtype, name="means") stddevs_tensor = ops.convert_to_tensor(stddevs, dtype=dtype, name="stddevs") minvals_tensor = ops.convert_to_tensor(minvals, dtype=dtype, name="minvals") maxvals_tensor = ops.convert_to_tensor(maxvals, dtype=dtype, name="maxvals") seed1, seed2 = random_seed.get_seed(seed) rnd = gen_random_ops._parameterized_truncated_normal( shape_tensor, means_tensor, stddevs_tensor, minvals_tensor, maxvals_tensor, seed=seed1, seed2=seed2) return rnd def truncated_normal(shape, mean=0.0, stddev=1.0, dtype=dtypes.float32, seed=None, name=None): """Outputs random values from a truncated normal distribution. The generated values follow a normal distribution with specified mean and standard deviation, except that values whose magnitude is more than 2 standard deviations from the mean are dropped and re-picked. Args: shape: A 1-D integer Tensor or Python array. The shape of the output tensor. mean: A 0-D Tensor or Python value of type `dtype`. The mean of the truncated normal distribution. stddev: A 0-D Tensor or Python value of type `dtype`. The standard deviation of the truncated normal distribution. dtype: The type of the output. seed: A Python integer. Used to create a random seed for the distribution. See @&#123;tf.set_random_seed&#125; for behavior. name: A name for the operation (optional). Returns: A tensor of the specified shape filled with random truncated normal values. """ with ops.name_scope(name, "truncated_normal", [shape, mean, stddev]) as name: shape_tensor = _ShapeTensor(shape) mean_tensor = ops.convert_to_tensor(mean, dtype=dtype, name="mean") stddev_tensor = ops.convert_to_tensor(stddev, dtype=dtype, name="stddev") seed1, seed2 = random_seed.get_seed(seed) rnd = gen_random_ops._truncated_normal( shape_tensor, dtype, seed=seed1, seed2=seed2) mul = rnd * stddev_tensor value = math_ops.add(mul, mean_tensor, name=name) return value ops.NotDifferentiable("ParameterizedTruncatedNormal") ops.NotDifferentiable("TruncatedNormal") def random_uniform(shape, minval=0, maxval=None, dtype=dtypes.float32, seed=None, name=None): """Outputs random values from a uniform distribution. The generated values follow a uniform distribution in the range `[minval, maxval)`. The lower bound `minval` is included in the range, while the upper bound `maxval` is excluded. For floats, the default range is `[0, 1)`. For ints, at least `maxval` must be specified explicitly. In the integer case, the random integers are slightly biased unless `maxval - minval` is an exact power of two. The bias is small for values of `maxval - minval` significantly smaller than the range of the output (either `2**32` or `2**64`). Args: shape: A 1-D integer Tensor or Python array. The shape of the output tensor. minval: A 0-D Tensor or Python value of type `dtype`. The lower bound on the range of random values to generate. Defaults to 0. maxval: A 0-D Tensor or Python value of type `dtype`. The upper bound on the range of random values to generate. Defaults to 1 if `dtype` is floating point. dtype: The type of the output: `float32`, `float64`, `int32`, or `int64`. seed: A Python integer. Used to create a random seed for the distribution. See @&#123;tf.set_random_seed&#125; for behavior. name: A name for the operation (optional). Returns: A tensor of the specified shape filled with random uniform values. Raises: ValueError: If `dtype` is integral and `maxval` is not specified. """ dtype = dtypes.as_dtype(dtype) if maxval is None: if dtype.is_integer: raise ValueError("Must specify maxval for integer dtype %r" % dtype) maxval = 1 with ops.name_scope(name, "random_uniform", [shape, minval, maxval]) as name: shape = _ShapeTensor(shape) minval = ops.convert_to_tensor(minval, dtype=dtype, name="min") maxval = ops.convert_to_tensor(maxval, dtype=dtype, name="max") seed1, seed2 = random_seed.get_seed(seed) if dtype.is_integer: return gen_random_ops._random_uniform_int( shape, minval, maxval, seed=seed1, seed2=seed2, name=name) else: rnd = gen_random_ops._random_uniform( shape, dtype, seed=seed1, seed2=seed2) return math_ops.add(rnd * (maxval - minval), minval, name=name) ops.NotDifferentiable("RandomUniform") def random_shuffle(value, seed=None, name=None): """Randomly shuffles a tensor along its first dimension. The tensor is shuffled along dimension 0, such that each `value[j]` is mapped to one and only one `output[i]`. For example, a mapping that might occur for a 3x2 tensor is: ```python [[1, 2], [[5, 6], [3, 4], ==&gt; [1, 2], [5, 6]] [3, 4]] ``` Args: value: A Tensor to be shuffled. seed: A Python integer. Used to create a random seed for the distribution. See @&#123;tf.set_random_seed&#125; for behavior. name: A name for the operation (optional). Returns: A tensor of same shape and type as `value`, shuffled along its first dimension. """ seed1, seed2 = random_seed.get_seed(seed) return gen_random_ops._random_shuffle( value, seed=seed1, seed2=seed2, name=name) def random_crop(value, size, seed=None, name=None): """Randomly crops a tensor to a given size. Slices a shape `size` portion out of `value` at a uniformly chosen offset. Requires `value.shape &gt;= size`. If a dimension should not be cropped, pass the full size of that dimension. For example, RGB images can be cropped with `size = [crop_height, crop_width, 3]`. Args: value: Input tensor to crop. size: 1-D tensor with size the rank of `value`. seed: Python integer. Used to create a random seed. See @&#123;tf.set_random_seed&#125; for behavior. name: A name for this operation (optional). Returns: A cropped tensor of the same rank as `value` and shape `size`. """ # TODO(shlens): Implement edge case to guarantee output size dimensions. # If size &gt; value.shape, zero pad the result so that it always has shape # exactly size. with ops.name_scope(name, "random_crop", [value, size]) as name: value = ops.convert_to_tensor(value, name="value") size = ops.convert_to_tensor(size, dtype=dtypes.int32, name="size") shape = array_ops.shape(value) check = control_flow_ops.Assert( math_ops.reduce_all(shape &gt;= size), ["Need value.shape &gt;= size, got ", shape, size], summarize=1000) shape = control_flow_ops.with_dependencies([check], shape) limit = shape - size + 1 offset = random_uniform( array_ops.shape(shape), dtype=size.dtype, maxval=size.dtype.max, seed=seed) % limit return array_ops.slice(value, offset, size, name=name) def multinomial(logits, num_samples, seed=None, name=None): """Draws samples from a multinomial distribution. Example: ```python # samples has shape [1, 5], where each value is either 0 or 1 with equal # probability. samples = tf.multinomial(tf.log([[10., 10.]]), 5) ``` Args: logits: 2-D Tensor with shape `[batch_size, num_classes]`. Each slice `[i, :]` represents the log-odds for all classes. num_samples: 0-D. Number of independent samples to draw for each row slice. seed: A Python integer. Used to create a random seed for the distribution. See @&#123;tf.set_random_seed&#125; for behavior. name: Optional name for the operation. Returns: The drawn samples of shape `[batch_size, num_samples]`. """ with ops.name_scope(name, "multinomial", [logits]): logits = ops.convert_to_tensor(logits, name="logits") seed1, seed2 = random_seed.get_seed(seed) return gen_random_ops.multinomial( logits, num_samples, seed=seed1, seed2=seed2) ops.NotDifferentiable("Multinomial") def random_gamma(shape, alpha, beta=None, dtype=dtypes.float32, seed=None, name=None): """Draws `shape` samples from each of the given Gamma distribution(s). `alpha` is the shape parameter describing the distribution(s), and `beta` is the inverse scale parameter(s). Example: samples = tf.random_gamma([10], [0.5, 1.5]) # samples has shape [10, 2], where each slice [:, 0] and [:, 1] represents # the samples drawn from each distribution samples = tf.random_gamma([7, 5], [0.5, 1.5]) # samples has shape [7, 5, 2], where each slice [:, :, 0] and [:, :, 1] # represents the 7x5 samples drawn from each of the two distributions samples = tf.random_gamma([30], [[1.],[3.],[5.]], beta=[[3., 4.]]) # samples has shape [30, 3, 2], with 30 samples each of 3x2 distributions. Note: Because internal calculations are done using `float64` and casting has `floor` semantics, we must manually map zero outcomes to the smallest possible positive floating-point value, i.e., `np.finfo(dtype).tiny`. This means that `np.finfo(dtype).tiny` occurs more frequently than it otherwise should. This bias can only happen for small values of `alpha`, i.e., `alpha &lt;&lt; 1` or large values of `beta`, i.e., `beta &gt;&gt; 1`. Args: shape: A 1-D integer Tensor or Python array. The shape of the output samples to be drawn per alpha/beta-parameterized distribution. alpha: A Tensor or Python value or N-D array of type `dtype`. `alpha` provides the shape parameter(s) describing the gamma distribution(s) to sample. Must be broadcastable with `beta`. beta: A Tensor or Python value or N-D array of type `dtype`. Defaults to 1. `beta` provides the inverse scale parameter(s) of the gamma distribution(s) to sample. Must be broadcastable with `alpha`. dtype: The type of alpha, beta, and the output: `float16`, `float32`, or `float64`. seed: A Python integer. Used to create a random seed for the distributions. See @&#123;tf.set_random_seed&#125; for behavior. name: Optional name for the operation. Returns: samples: a `Tensor` of shape `tf.concat(shape, tf.shape(alpha + beta))` with values of type `dtype`. """ with ops.name_scope(name, "random_gamma", [shape, alpha, beta]): shape = ops.convert_to_tensor(shape, name="shape", dtype=dtypes.int32) alpha = ops.convert_to_tensor(alpha, name="alpha", dtype=dtype) beta = ops.convert_to_tensor( beta if beta is not None else 1, name="beta", dtype=dtype) alpha_broadcast = alpha + array_ops.zeros_like(beta) seed1, seed2 = random_seed.get_seed(seed) return math_ops.maximum( np.finfo(dtype.as_numpy_dtype).tiny, gen_random_ops._random_gamma( shape, alpha_broadcast, seed=seed1, seed2=seed2) / beta) ops.NotDifferentiable("RandomGamma") def random_poisson(lam, shape, dtype=dtypes.float32, seed=None, name=None): """Draws `shape` samples from each of the given Poisson distribution(s). `lam` is the rate parameter describing the distribution(s). Example: samples = tf.random_poisson([0.5, 1.5], [10]) # samples has shape [10, 2], where each slice [:, 0] and [:, 1] represents # the samples drawn from each distribution samples = tf.random_poisson([12.2, 3.3], [7, 5]) # samples has shape [7, 5, 2], where each slice [:, :, 0] and [:, :, 1] # represents the 7x5 samples drawn from each of the two distributions Args: lam: A Tensor or Python value or N-D array of type `dtype`. `lam` provides the rate parameter(s) describing the poisson distribution(s) to sample. shape: A 1-D integer Tensor or Python array. The shape of the output samples to be drawn per "rate"-parameterized distribution. dtype: The type of `lam` and the output: `float16`, `float32`, or `float64`. seed: A Python integer. Used to create a random seed for the distributions. See @&#123;tf.set_random_seed&#125; for behavior. name: Optional name for the operation. Returns: samples: a `Tensor` of shape `tf.concat(shape, tf.shape(lam))` with values of type `dtype`. """ with ops.name_scope(name, "random_poisson", [lam, shape]): lam = ops.convert_to_tensor(lam, name="lam", dtype=dtype) shape = ops.convert_to_tensor(shape, name="shape", dtype=dtypes.int32) seed1, seed2 = random_seed.get_seed(seed) return gen_random_ops._random_poisson(shape, lam, seed=seed1, seed2=seed2)]]></content>
      <categories>
        <category>tensorflow</category>
      </categories>
      <tags>
        <tag>tf.random</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tensorflow函数整理]]></title>
    <url>%2F2017%2F12%2F19%2Ftensorflow%2FTensorflow%E5%87%BD%E6%95%B0%E6%95%B4%E7%90%86%2F</url>
    <content type="text"><![CDATA[属性类.eval() 取op的值 函数类计算函数tf.nn.dopout(L1, keep_prob)：keep_prob是设置层L1工作神经元的百分比tf.nn.sigmoid()tf.nn.softmax()tf.nn.softmax_cross_entropy_with_logits(y_hat, y)：交叉熵函数。tf.add()：加减乘除tf.subtract()tf.multiply()tf.divide() tf.matmul()tf.reshape(labels, [128, 1])tf.reduce_max()tf.reduce_mean() tf.square()：平方tf.sqrt()：开方 tf.squeeze()：去掉张量中长度为1的维度，相当于降维 算法函数Optimizertf.train.GradientDescentOptimizer()：梯度下降tf.train.AdadeItaOptimizer()tf.train.AdagradOptimizer()tf.train.AdagradDAOptimizer()tf.train.MomentumOptimizer()tf.train.AdamOptimizer()tf.train.FtriOptimizer()tf.train.ProximalGradientDescentOptimizer()tf.train.ProximalAdagradOptimizer()tf.train.RMSPropOptimizer() 功能函数 tf.equal(a, b) ：比较是否相等 tf.cast(a, “float”) :类型转换 tf.placeholder(“float”, [None, 10]) ：None is for infinite 这是一个n行10列的动态矩阵 tf.InteractiveSession(): 它能让你在运行图的时候，插入一些计算图 tf.Session(): 需要在启动session之前构建整个计算图，然后启动该计算图。 tf.random_normal([2, 3], mean=-1, stddev=4) ：高斯初始化生成一个 均值为-1，方差为4的矩阵 经验总结类Tensorflow 定义的任何数据类型都推荐用 float32]]></content>
      <categories>
        <category>tensorflow</category>
      </categories>
      <tags>
        <tag>tensorflow</tag>
      </tags>
  </entry>
</search>
