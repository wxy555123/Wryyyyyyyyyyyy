<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[欢迎光临，BLOG刚弄不久正在施工 文章陆续上传中...]]></title>
    <url>%2F2018%2F04%2F29%2FOTHERS%2F%E7%BD%AE%E9%A1%B6%E4%BF%A1%E6%81%AF%2F</url>
    <content type="text"></content>
      <categories>
        <category>其他</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[4线性方程求解]]></title>
    <url>%2F2018%2F04%2F29%2Fmath%2Flinear_algebra%2F4%E7%BA%BF%E6%80%A7%E6%96%B9%E7%A8%8B%E6%B1%82%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[线性方程求解方法Gauss消去法1转化为（同解）的三角形方程组2化阶梯形矩阵 且 要保证解不变，所以需要组成增广阵再进行 初等行变换（同解变换）实际过程就是一行一行消元，用上面行消去下面行第一个项，有利于解出最后一个回代 举例：适用性：系数矩阵A规模比较小的，否则很慢系数矩阵A是非奇异的，否则没有唯一解 Jacobi迭代法 雅克比迭代法矩阵描述 矩阵迭代公式DX = (L+U)X + bL和U都没更新，写在右边 Gauss-Seildel迭代法及时更新下半三角系数的迭代 矩阵迭代公式一(D-L)X = UX + b只有上半部的U没更新，写在右边 矩阵迭代公式二这样D的逆更方便求出来 迭代法的收敛严格对角占优矩阵 定义：对角线元素的绝对值和 &gt; 其所在 行/列 元素的绝对值和 的矩阵]]></content>
      <categories>
        <category>math</category>
        <category>linear_algebra</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[jupyter设置]]></title>
    <url>%2F2018%2F04%2F25%2Fanaconda%2Fjupyter%E8%AE%BE%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[更改启动目录右键jupyter notebook快捷方式属性，把“目标”属性最后的变量改为自己的路径，如下：1&quot;C:\\Users\\lenovo\\Desktop\\Python WORK SPACE\\&quot; 注意前面一个空格要保留]]></content>
      <categories>
        <category>anaconda</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[anaconda安装命令整理]]></title>
    <url>%2F2018%2F04%2F25%2Fanaconda%2Fanaconda%E5%AE%89%E8%A3%85%E5%91%BD%E4%BB%A4%E6%95%B4%E7%90%86%2F</url>
    <content type="text"><![CDATA[安装命令整理Tensorflow 安装（以windows版本为例）S1. 查找所有Tensorflow版本：1anaconda search -t conda tensorflow 找到windows版本S2. 显示该版本的安装命令：1anaconda show dhirschfeld/tensorflow S3. 使用所提示的安装命令：1conda install --channel https://conda.anaconda.org/dhirschfeld tensorflow]]></content>
      <categories>
        <category>anaconda</category>
      </categories>
      <tags>
        <tag>anaconda</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[markdown语法]]></title>
    <url>%2F2018%2F04%2F24%2Fhexo%2Fmarkdown%E8%AF%AD%E6%B3%95%2F</url>
    <content type="text"><![CDATA[markdown语法空格：输入法全角状态下space]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>markdown</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2模型评估与选择]]></title>
    <url>%2F2018%2F04%2F21%2Fmachine_learning_theory%2F2%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E4%B8%8E%E9%80%89%E6%8B%A9%2F</url>
    <content type="text"><![CDATA[性能度量二分类任务的 混淆矩阵 和 其衍生的度量指标 True Positive （真正, TP） 被模型预测为正样本，是真的判断正确。所以就是正样本，也称作正的数。 True Negative（真负 , TN）被模型判断为负样本，是真的判断正确。所以就是负样本，也称作负的数。 False Positive （假正, FP）被模型判断为正样本，是假的判断错误。所以应该是负样本，也称作误报数。 False Negative（假负 , FN）被模型判断为负样本，是假的判断错误。所以应该是正样本，也称作漏报数。 1）常用的3个指标（多用于交叉验证） accuracy（准确率）——检验模型预测的正确率A=\frac {TP+TN}{ALL} 预测正确个数/全部样本数 precision（精确率）——检验模型预测正例的正确率P=\frac {TP}{TP+FP} 预测正确的正样本数 / 预测为的正样本数 recall/TPR（召回率/真正率）——检验模型正例预测的全面性R\ /\ TPR=\frac {TP}{TP+FN} 预测正确的正样本数 / 真实的正样本数 2）不常用的3个指标（多用于绘图） specificity（特异性/真负率）——检验模型负例预测的正确率S=\frac {TN}{TN+FP} 预测正确的负样本数 / 真实的负样本数 FPR（假正率）——用于和TPR一起绘制ROC曲线FPR=\frac {FP}{TN+FP} 预测错误的正样本数 / 真实的负样本数 FNR（假负率）——用的少 FNR=\frac {FN}{TP+FN} 预测错误的负样本数 / 真实的正样本数]]></content>
      <categories>
        <category>machine_learning_theory</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[2矩阵的运算、行列式]]></title>
    <url>%2F2018%2F04%2F20%2Fmath%2Flinear_algebra%2F2%E7%9F%A9%E9%98%B5%E7%9A%84%E8%BF%90%E7%AE%97%E3%80%81%E8%A1%8C%E5%88%97%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[矩阵的运算1 矩阵乘法的具体应用总结：A中每个元素和B中每个元素相乘是有意义的；B矩阵和最终C矩阵指标数相等，相当于对应指标类元素的求和 几种特殊的矩阵1.对角矩阵2.数量矩阵★3.单矩阵4.三角矩阵5.对称矩阵 分块矩阵和其运算1 简介：2 分块矩阵相加和相乘A+B 和 AB相加：要求每个子块矩阵有相同的行数和列数相乘：要求A的列 = B的行 行列式1 矩阵的行列式和他的转置的行列式相等]]></content>
      <categories>
        <category>math</category>
        <category>linear_algebra</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[直线和平面方程]]></title>
    <url>%2F2018%2F04%2F20%2Fmath%2Fcalculus%2F%E7%9B%B4%E7%BA%BF%E5%92%8C%E5%B9%B3%E9%9D%A2%E6%96%B9%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[一、直线 直线方程 方程名称 形式 说明 一般式 ax+by+c=0 优点：可以表示平面上的任意一条直线缺点：要确定的常数较多 斜截式 y=kx+b 优点：只需要斜率和截距缺点：不能表示垂直x轴的直线x=a 点斜式 y-y0=k(x-x0) 优点：只需要一个点和斜率缺点：不能表示垂直x轴的直线x=a 两点式 (y-y1)/(y2-y1)=(x-x1)/(x2-x1) 优点：只需要2个点缺点：不能表示两点x1=x2或y1=y2时的直线（即垂直或水平直线） 截距式 x/a+y/b=1 优点：只需要x轴截距a和y轴截距b缺点：不能表示截距为0时的直线,比如正比例直线 二、平面 平面方程常用4种 方程名称 形式 说明 一般式 Ax+By+Cz+D=0 截距式 x/a+y/b+z/c=1 点法式 A(x-x0)+B(y-y0)+C(z-z0)=0 向量(A,B,C)为平面的法向量 法线式 xcosα+ycosβ+zcosγ=p 其中cosα、cosβ、cosγ是平面法矢量的方向余弦，p为原点到平面的距离。 ​ 平面方程全部7种 三、超平面 二维空间的超平面是一条直线，三维空间的超平面是一个平面，而N维空间的超平面则是N-1维的仿射空间。]]></content>
      <categories>
        <category>math</category>
        <category>calculus</category>
      </categories>
      <tags>
        <tag>平面方程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[贷款分析]]></title>
    <url>%2F2018%2F04%2F19%2Fmachine_learning_in_action%2F%E8%B4%B7%E6%AC%BE%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[123456import pandas as pdloans_2007 = pd.read_csv('LoanStats3a.csv', skiprows=1)half_count = len(loans_2007) / 2loans_2007 = loans_2007.dropna(thresh=half_count, axis=1)loans_2007 = loans_2007.drop(['desc', 'url'],axis=1)loans_2007.to_csv('loans_2007.csv', index=False) D:\Anaconda3\lib\site-packages\IPython\core\interactiveshell.py:2717: DtypeWarning: Columns (0,47) have mixed types. Specify dtype option on import or set low_memory=False. interactivity=interactivity, compiler=compiler, result=result) 12345import pandas as pdloans_2007 = pd.read_csv("loans_2007.csv")#loans_2007.drop_duplicates()print(loans_2007.iloc[0].head(15))print(loans_2007.shape[1]) #共52个特征 id 1077501 member_id 1.2966e+06 loan_amnt 5000 funded_amnt 5000 funded_amnt_inv 4975 term 36 months int_rate 10.65% installment 162.87 grade B sub_grade B2 emp_title NaN emp_length 10+ years home_ownership RENT annual_inc 24000 verification_status Verified Name: 0, dtype: object 52 D:\Anaconda3\lib\site-packages\IPython\core\interactiveshell.py:2717: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False. interactivity=interactivity, compiler=compiler, result=result) 预处理——数据清洗先直观上 去掉不需要的特征。包括： 1.预测后才出现的特征（比如：实际发放的贷款）、2.高度相关的特征（比如：123和ABC）、3。关系不大的特征（比如：ID等） 12loans_2007 = loans_2007.drop(["id", "member_id", "funded_amnt", "funded_amnt_inv", "grade", "sub_grade", "emp_title", "issue_d"], axis=1)loans_2007 = loans_2007.drop(["zip_code", "out_prncp", "out_prncp_inv", "total_pymnt", "total_pymnt_inv", "total_rec_prncp"], axis=1) 123loans_2007 = loans_2007.drop(["total_rec_int", "total_rec_late_fee", "recoveries", "collection_recovery_fee", "last_pymnt_d", "last_pymnt_amnt"], axis=1)print(loans_2007.iloc[0])print(loans_2007.shape[1]) loan_amnt 5000 term 36 months int_rate 10.65% installment 162.87 emp_length 10+ years home_ownership RENT annual_inc 24000 verification_status Verified loan_status Fully Paid pymnt_plan n purpose credit_card title Computer addr_state AZ dti 27.65 delinq_2yrs 0 earliest_cr_line Jan-1985 inq_last_6mths 1 open_acc 3 pub_rec 0 revol_bal 13648 revol_util 83.7% total_acc 9 initial_list_status f last_credit_pull_d Nov-2016 collections_12_mths_ex_med 0 policy_code 1 application_type INDIVIDUAL acc_now_delinq 0 chargeoff_within_12_mths 0 delinq_amnt 0 pub_rec_bankruptcies 0 tax_liens 0 Name: 0, dtype: object 32 预处理——label列属性数字替换对label属性进行统计，选择适合用于分类的属性 1print(loans_2007['loan_status'].value_counts()) Fully Paid 33902 Charged Off 5658 Does not meet the credit policy. Status:Fully Paid 1988 Does not meet the credit policy. Status:Charged Off 761 Current 201 Late (31-120 days) 10 In Grace Period 9 Late (16-30 days) 5 Default 1 Name: loan_status, dtype: int64 12345678910111213# 属性为Fully Paid 和 Charged off 的替换为 1 和 0# 只取这部分数据loans_2007 = loans_2007[(loans_2007['loan_status'] == "Fully Paid") | (loans_2007['loan_status'] == "Charged Off")]# 替换指定列的 指定属性为 指定的值★status_replace = &#123; "loan_status" : &#123; "Fully Paid": 1, "Charged Off": 0, &#125;&#125;loans_2007 = loans_2007.replace(status_replace) 预处理——最后再去掉列属性只有一个的列123456789101112#let's look for any columns that contain only one unique value and remove themorig_columns = loans_2007.columnsdrop_columns = []for col in orig_columns: col_series = loans_2007[col].dropna().unique() #这里要去掉空值nan以后 再判定列的属性是否只有一个 if len(col_series) == 1: drop_columns.append(col)loans_2007 = loans_2007.drop(drop_columns, axis=1)print(drop_columns)print(loans_2007.shape)loans_2007.to_csv('filtered_loans_2007.csv', index=False) [&#39;initial_list_status&#39;, &#39;collections_12_mths_ex_med&#39;, &#39;policy_code&#39;, &#39;application_type&#39;, &#39;acc_now_delinq&#39;, &#39;chargeoff_within_12_mths&#39;, &#39;delinq_amnt&#39;, &#39;tax_liens&#39;] (39560, 24) 处理完毕，最终得到24列数据 预处理——缺失值先查找列缺失值多的，去掉这些列，只剩下缺失值少的列。之后直接去掉那些样本行即可 1234import pandas as pdloans = pd.read_csv('filtered_loans_2007.csv')null_counts = loans.isnull().sum()print(null_counts) loan_amnt 0 term 0 int_rate 0 installment 0 emp_length 0 home_ownership 0 annual_inc 0 verification_status 0 loan_status 0 pymnt_plan 0 purpose 0 title 10 addr_state 0 dti 0 delinq_2yrs 0 earliest_cr_line 0 inq_last_6mths 0 open_acc 0 pub_rec 0 revol_bal 0 revol_util 50 total_acc 0 last_credit_pull_d 2 pub_rec_bankruptcies 697 dtype: int64 12loans = loans.drop("pub_rec_bankruptcies", axis=1)loans = loans.dropna(axis=0) object 12 float64 10 int64 1 dtype: int64 12# 处理完缺失值，统计每种数据类型的列 有几个print(loans.dtypes.value_counts()) 预处理——字符串值转换12object_columns_df = loans.select_dtypes(include=["object"])print(object_columns_df.iloc[0]) term 36 months int_rate 10.65% emp_length 10+ years home_ownership RENT verification_status Verified pymnt_plan n purpose credit_card title Computer addr_state AZ earliest_cr_line Jan-1985 revol_util 83.7% last_credit_pull_d Nov-2016 Name: 0, dtype: object 123cols = ['home_ownership', 'verification_status', 'emp_length', 'term', 'addr_state']for c in cols: print(loans[c].value_counts()) RENT 18780 MORTGAGE 17574 OWN 3045 OTHER 96 NONE 3 Name: home_ownership, dtype: int64 Not Verified 16856 Verified 12705 Source Verified 9937 Name: verification_status, dtype: int64 10+ years 8821 &lt; 1 year 4563 2 years 4371 3 years 4074 4 years 3409 5 years 3270 1 year 3227 6 years 2212 7 years 1756 8 years 1472 9 years 1254 n/a 1069 Name: emp_length, dtype: int64 36 months 29041 60 months 10457 Name: term, dtype: int64 CA 7070 NY 3788 FL 2856 TX 2714 NJ 1838 IL 1517 PA 1504 VA 1400 GA 1393 MA 1336 OH 1208 MD 1049 AZ 874 WA 834 CO 786 NC 780 CT 747 MI 722 MO 682 MN 611 NV 492 SC 470 WI 453 AL 446 OR 445 LA 435 KY 325 OK 298 KS 269 UT 256 AR 243 DC 211 RI 198 NM 188 WV 176 HI 172 NH 172 DE 113 MT 84 WY 83 AK 79 SD 63 VT 54 MS 19 TN 17 IN 9 ID 6 IA 5 NE 5 ME 3 Name: addr_state, dtype: int64 123# 这2个特征内容差不多，选择去掉title列print(loans["purpose"].value_counts())print(loans["title"].value_counts()) debt_consolidation 18533 credit_card 5099 other 3963 home_improvement 2965 major_purchase 2181 small_business 1815 car 1544 wedding 945 medical 692 moving 581 vacation 379 house 378 educational 320 renewable_energy 103 Name: purpose, dtype: int64 Debt Consolidation 2168 Debt Consolidation Loan 1706 Personal Loan 658 Consolidation 509 debt consolidation 502 Credit Card Consolidation 356 Home Improvement 354 Debt consolidation 333 Small Business Loan 322 Credit Card Loan 313 Personal 308 Consolidation Loan 255 Home Improvement Loan 246 personal loan 234 personal 220 Loan 212 Wedding Loan 209 consolidation 200 Car Loan 200 Other Loan 190 Credit Card Payoff 155 Wedding 152 Major Purchase Loan 144 Credit Card Refinance 143 Consolidate 127 Medical 122 Credit Card 117 home improvement 111 My Loan 94 Credit Cards 93 ... DebtConsolidationn 1 Freedom 1 Credit Card Consolidation Loan - SEG 1 SOLAR PV 1 Pay on Credit card 1 To pay off balloon payments due 1 Paying off the debt 1 Payoff ING PLOC 1 Josh CC Loan 1 House payoff 1 Taking care of Business 1 Gluten Free Bakery in ideal town for it 1 Startup Money for Small Business 1 FundToFinanceCar 1 getting ready for Baby 1 Dougs Wedding Loan 1 d rock 1 LC Loan 2 1 swimming pool repair 1 engagement 1 Cut the credit cards Loan 1 vinman 1 working hard to get out of debt 1 consolidate the rest of my debt 1 Medical/Vacation 1 2BDebtFree 1 Paying Off High Interest Credit Cards! 1 Baby on the way! 1 cart loan 1 Consolidaton 1 Name: title, dtype: int64 1234567891011121314151617181920mapping_dict = &#123; "emp_length": &#123; "10+ years": 10, "9 years": 9, "8 years": 8, "7 years": 7, "6 years": 6, "5 years": 5, "4 years": 4, "3 years": 3, "2 years": 2, "1 year": 1, "&lt; 1 year": 0, "n/a": 0 &#125;&#125;loans = loans.drop(["last_credit_pull_d", "earliest_cr_line", "addr_state", "title"], axis=1)loans["int_rate"] = loans["int_rate"].str.rstrip("%").astype("float")loans["revol_util"] = loans["revol_util"].str.rstrip("%").astype("float")loans = loans.replace(mapping_dict) 123456cat_columns = ["home_ownership", "verification_status", "emp_length", "purpose", "term"]dummy_df = pd.get_dummies(loans[cat_columns])loans = pd.concat([loans, dummy_df], axis=1)loans = loans.drop(cat_columns, axis=1)loans = loans.drop("pymnt_plan", axis=1) 1loans.to_csv('cleaned_loans2007.csv', index=False) 模型训练、评估、调参——训练目标是盈利最大化模型训练目标：是确保TP的数量至少是FP的10倍。然而这里不适合使用精度，故我们选择 TPR 和 FPR ，要使 TPR尽可能大， FPR 尽可能小 123import pandas as pdloans = pd.read_csv("cleaned_loans2007.csv")print(loans.info()) &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 39498 entries, 0 to 39497 Data columns (total 37 columns): loan_amnt 39498 non-null float64 int_rate 39498 non-null float64 installment 39498 non-null float64 annual_inc 39498 non-null float64 loan_status 39498 non-null int64 dti 39498 non-null float64 delinq_2yrs 39498 non-null float64 inq_last_6mths 39498 non-null float64 open_acc 39498 non-null float64 pub_rec 39498 non-null float64 revol_bal 39498 non-null float64 revol_util 39498 non-null float64 total_acc 39498 non-null float64 home_ownership_MORTGAGE 39498 non-null int64 home_ownership_NONE 39498 non-null int64 home_ownership_OTHER 39498 non-null int64 home_ownership_OWN 39498 non-null int64 home_ownership_RENT 39498 non-null int64 verification_status_Not Verified 39498 non-null int64 verification_status_Source Verified 39498 non-null int64 verification_status_Verified 39498 non-null int64 purpose_car 39498 non-null int64 purpose_credit_card 39498 non-null int64 purpose_debt_consolidation 39498 non-null int64 purpose_educational 39498 non-null int64 purpose_home_improvement 39498 non-null int64 purpose_house 39498 non-null int64 purpose_major_purchase 39498 non-null int64 purpose_medical 39498 non-null int64 purpose_moving 39498 non-null int64 purpose_other 39498 non-null int64 purpose_renewable_energy 39498 non-null int64 purpose_small_business 39498 non-null int64 purpose_vacation 39498 non-null int64 purpose_wedding 39498 non-null int64 term_ 36 months 39498 non-null int64 term_ 60 months 39498 non-null int64 dtypes: float64(12), int64(25) memory usage: 11.1 MB None 12345678910111213141516import pandas as pd# False positives.fp_filter = (predictions == 1) &amp; (loans["loan_status"] == 0)fp = len(predictions[fp_filter])# True positives.tp_filter = (predictions == 1) &amp; (loans["loan_status"] == 1)tp = len(predictions[tp_filter])# False negatives.fn_filter = (predictions == 0) &amp; (loans["loan_status"] == 1)fn = len(predictions[fn_filter])# True negativestn_filter = (predictions == 0) &amp; (loans["loan_status"] == 0)tn = len(predictions[tn_filter]) 12345678from sklearn.linear_model import LogisticRegressionlr = LogisticRegression()cols = loans.columnstrain_cols = cols.drop("loan_status")features = loans[train_cols]target = loans["loan_status"]lr.fit(features, target)predictions = lr.predict(features) 第一次，逻辑回归123456789101112131415161718192021222324252627282930from sklearn.linear_model import LogisticRegressionfrom sklearn.cross_validation import cross_val_predict, KFoldlr = LogisticRegression()kf = KFold(features.shape[0], random_state=1)predictions = cross_val_predict(lr, features, target, cv=kf)predictions = pd.Series(predictions)# False positives.fp_filter = (predictions == 1) &amp; (loans["loan_status"] == 0)fp = len(predictions[fp_filter])# True positives.tp_filter = (predictions == 1) &amp; (loans["loan_status"] == 1)tp = len(predictions[tp_filter])# False negatives.fn_filter = (predictions == 0) &amp; (loans["loan_status"] == 1)fn = len(predictions[fn_filter])# True negativestn_filter = (predictions == 0) &amp; (loans["loan_status"] == 0)tn = len(predictions[tn_filter])# Ratestpr = tp / float((tp + fn))fpr = fp / float((fp + tn))print(tpr)print(fpr)print predictions[:20] 0.999084438406 0.998049299521 0 1 1 1 2 1 3 1 4 1 5 1 6 1 7 1 8 1 9 1 10 1 11 1 12 1 13 1 14 1 15 1 16 1 17 1 18 1 19 1 dtype: int64 因为样本不均衡导致效果不好。 第二次，逻辑回归，设置模型参数 class_weight=”balanced” ，让模型自动添加惩罚权重123456789101112131415161718192021222324252627282930from sklearn.linear_model import LogisticRegressionfrom sklearn.cross_validation import cross_val_predictlr = LogisticRegression(class_weight="balanced")kf = KFold(features.shape[0], random_state=1)predictions = cross_val_predict(lr, features, target, cv=kf)predictions = pd.Series(predictions)# False positives.fp_filter = (predictions == 1) &amp; (loans["loan_status"] == 0)fp = len(predictions[fp_filter])# True positives.tp_filter = (predictions == 1) &amp; (loans["loan_status"] == 1)tp = len(predictions[tp_filter])# False negatives.fn_filter = (predictions == 0) &amp; (loans["loan_status"] == 1)fn = len(predictions[fn_filter])# True negativestn_filter = (predictions == 0) &amp; (loans["loan_status"] == 0)tn = len(predictions[tn_filter])# Ratestpr = tp / float((tp + fn))fpr = fp / float((fp + tn))print(tpr)print(fpr)print predictions[:20] 0.670781771464 0.400780280192 0 1 1 0 2 0 3 1 4 1 5 0 6 0 7 0 8 0 9 0 10 1 11 0 12 1 13 1 14 0 15 0 16 1 17 1 18 1 19 0 dtype: int64 模型终于起了效果，但还是不理想 第三次，逻辑回归，手动调节 cclass_weight 参数设置惩罚系数12345678910111213141516171819202122232425262728293031323334from sklearn.linear_model import LogisticRegressionfrom sklearn.cross_validation import cross_val_predictpenalty = &#123; 0: 5, 1: 1&#125;lr = LogisticRegression(class_weight=penalty)kf = KFold(features.shape[0], random_state=1)predictions = cross_val_predict(lr, features, target, cv=kf)predictions = pd.Series(predictions)# False positives.fp_filter = (predictions == 1) &amp; (loans["loan_status"] == 0)fp = len(predictions[fp_filter])# True positives.tp_filter = (predictions == 1) &amp; (loans["loan_status"] == 1)tp = len(predictions[tp_filter])# False negatives.fn_filter = (predictions == 0) &amp; (loans["loan_status"] == 1)fn = len(predictions[fn_filter])# True negativestn_filter = (predictions == 0) &amp; (loans["loan_status"] == 0)tn = len(predictions[tn_filter])# Ratestpr = tp / float((tp + fn))fpr = fp / float((fp + tn))print(tpr)print(fpr) 0.731799521545 0.478985635751 效果又好了一些 第四次，随机森林123456789101112131415161718192021222324252627from sklearn.ensemble import RandomForestClassifierfrom sklearn.cross_validation import cross_val_predictrf = RandomForestClassifier(n_estimators=10,class_weight="balanced", random_state=1)#print help(RandomForestClassifier)kf = KFold(features.shape[0], random_state=1)predictions = cross_val_predict(rf, features, target, cv=kf)predictions = pd.Series(predictions)# False positives.fp_filter = (predictions == 1) &amp; (loans["loan_status"] == 0)fp = len(predictions[fp_filter])# True positives.tp_filter = (predictions == 1) &amp; (loans["loan_status"] == 1)tp = len(predictions[tp_filter])# False negatives.fn_filter = (predictions == 0) &amp; (loans["loan_status"] == 1)fn = len(predictions[fn_filter])# True negativestn_filter = (predictions == 0) &amp; (loans["loan_status"] == 0)tn = len(predictions[tn_filter])# Ratestpr = tp / float((tp + fn))fpr = fp / float((fp + tn))]]></content>
      <categories>
        <category>machine_learning_in_action</category>
      </categories>
      <tags>
        <tag>pandas列属性数字替换</tag>
        <tag>pandas缺失值处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[1机器学习流程]]></title>
    <url>%2F2018%2F04%2F19%2Fmachine_learning_theory%2F1%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%B5%81%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[一、数据收集并给定标签数据预处理1）归一化2）数据清洗先直观上 去掉不需要的列特征。 包括：1.预测后才出现的特征（比如：实际发放的贷款）、2.高度相关的特征（比如：123和ABC）、3.关系不大的特征（比如：ID等）4.列属性只有一个值的 需要先排除列中 nan 值，再用 unique() 判定 3）缺失值处理先查找列缺失值多的，去掉这些列，只剩下缺失值少的列。之后直接去掉那些样本行即可 4）字符串值处理把 object 类型转为 int 和 float 类型。包括：1.one-hot 编码2.列属性数字替换 5）数据样本均衡分析样本分布不均衡：指的是 label 不同的样本数量差距很大。如果数据样本不均衡，容易导致分类器效果很差。为了解决，有以下2种方案： P1：oversample 或 undersampleP2：用模型参数 调节分类惩罚权重比参数： class_weight，适用于所有分类算法 特征提取数据集划分二、训练一个分类器选择模型训练模型三、模型测试、评估选择模型评估方法1.交叉验证 + 指标（比如：精度、TPR）2.均方误差3.交叉熵 模型评估结果分析如果分类效果不佳，可采取以下措施： 调节分类惩罚系数比（可在模型参数中调节） 调节模型其他参数 考虑 过拟合 可能，去掉一些列 集成多个模型 尝试其他模型]]></content>
      <categories>
        <category>machine_learning_theory</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PCA 手写主成分分析]]></title>
    <url>%2F2018%2F04%2F19%2Fmachine_learning_in_action%2FPCA%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[PCA 手写主成分分析1234import numpy as npimport pandas as pddf = pd.read_csv('iris.data')df.head() .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } 5.1 3.5 1.4 0.2 Iris-setosa 0 4.9 3.0 1.4 0.2 Iris-setosa 1 4.7 3.2 1.3 0.2 Iris-setosa 2 4.6 3.1 1.5 0.2 Iris-setosa 3 5.0 3.6 1.4 0.2 Iris-setosa 4 5.4 3.9 1.7 0.4 Iris-setosa 1 数据预处理123#加上列名df.columns=['sepal_len', 'sepal_wid', 'petal_len', 'petal_wid', 'class']df.head() .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } sepal_len sepal_wid petal_len petal_wid class 0 4.9 3.0 1.4 0.2 Iris-setosa 1 4.7 3.2 1.3 0.2 Iris-setosa 2 4.6 3.1 1.5 0.2 Iris-setosa 3 5.0 3.6 1.4 0.2 Iris-setosa 4 5.4 3.9 1.7 0.4 Iris-setosa 2 画图，进行降维特征分析1234# split data table into data X and class labels yX = df.iloc[:,0:4].valuesy = df.iloc[:,4].values 123456789101112131415161718192021222324252627# 把每个特征用于分类的结果，都画成条形图，观察哪个特征更容易划分种类from matplotlib import pyplot as pltimport mathlabel_dict = &#123;1: 'Iris-Setosa', 2: 'Iris-Versicolor', 3: 'Iris-Virgnica'&#125;feature_dict = &#123;0: 'sepal length [cm]', 1: 'sepal width [cm]', 2: 'petal length [cm]', 3: 'petal width [cm]'&#125;plt.figure(figsize=(8, 6))for cnt in range(4): plt.subplot(2, 2, cnt+1) for lab in ('Iris-setosa', 'Iris-versicolor', 'Iris-virginica'): plt.hist(X[y==lab, cnt], label=lab, bins=10, alpha=0.3,) plt.xlabel(feature_dict[cnt]) plt.legend(loc='upper right', fancybox=True, fontsize=8)plt.tight_layout()plt.show() 123# 特征 归一化from sklearn.preprocessing import StandardScalerX_std = StandardScaler().fit_transform(X) 3 协方差分析（发现有2个有用特征，决定从4维降到2维）1 计算样本X的 协方差矩阵（有4个特征，所以是4x4）1234# 自己算 协方差矩阵mean_vec = np.mean(X_std, axis=0)cov_mat = (X_std - mean_vec).T.dot((X_std - mean_vec)) / (X_std.shape[0]-1)print('Covariance matrix \n%s' %cov_mat) Covariance matrix [[ 1.00675676 -0.10448539 0.87716999 0.82249094] [-0.10448539 1.00675676 -0.41802325 -0.35310295] [ 0.87716999 -0.41802325 1.00675676 0.96881642] [ 0.82249094 -0.35310295 0.96881642 1.00675676]] 12# numpy算 协方差矩阵print('NumPy covariance matrix: \n%s' %np.cov(X_std.T)) NumPy covariance matrix: [[ 1.00675676 -0.10448539 0.87716999 0.82249094] [-0.10448539 1.00675676 -0.41802325 -0.35310295] [ 0.87716999 -0.41802325 1.00675676 0.96881642] [ 0.82249094 -0.35310295 0.96881642 1.00675676]] 2 对协方差矩阵进行 特征值分解123456cov_mat = np.cov(X_std.T)eig_vals, eig_vecs = np.linalg.eig(cov_mat)print('Eigenvectors \n%s' %eig_vecs)print('\nEigenvalues \n%s' %eig_vals) Eigenvectors [[ 0.52308496 -0.36956962 -0.72154279 0.26301409] [-0.25956935 -0.92681168 0.2411952 -0.12437342] [ 0.58184289 -0.01912775 0.13962963 -0.80099722] [ 0.56609604 -0.06381646 0.63380158 0.52321917]] Eigenvalues [ 2.92442837 0.93215233 0.14946373 0.02098259] 3 把特征值从大到小排列，并配对特征向量1234567891011# Make a list of (eigenvalue, eigenvector) tupleseig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:,i]) for i in range(len(eig_vals))]print (eig_pairs)print ('----------')# Sort the (eigenvalue, eigenvector) tuples from high to loweig_pairs.sort(key=lambda x: x[0], reverse=True)# Visually confirm that the list is correctly sorted by decreasing eigenvaluesprint('Eigenvalues in descending order:')for i in eig_pairs: print(i[0],"对应",i[1]) [(2.9244283691111144, array([ 0.52308496, -0.25956935, 0.58184289, 0.56609604])), (0.93215233025350641, array([-0.36956962, -0.92681168, -0.01912775, -0.06381646])), (0.14946373489813314, array([-0.72154279, 0.2411952 , 0.13962963, 0.63380158])), (0.020982592764270606, array([ 0.26301409, -0.12437342, -0.80099722, 0.52321917]))] ---------- Eigenvalues in descending order: 2.92442836911 对应 [ 0.52308496 -0.25956935 0.58184289 0.56609604] 0.932152330254 对应 [-0.36956962 -0.92681168 -0.01912775 -0.06381646] 0.149463734898 对应 [-0.72154279 0.2411952 0.13962963 0.63380158] 0.0209825927643 对应 [ 0.26301409 -0.12437342 -0.80099722 0.52321917] 4 通过前面特征值累加所占比重 的图像，判断取前多少特征值合适，组成投影矩阵W12345tot = sum(eig_vals)var_exp = [(i / tot)*100 for i in sorted(eig_vals, reverse=True)]print (var_exp)cum_var_exp = np.cumsum(var_exp)cum_var_exp [72.620033326920336, 23.147406858644135, 3.7115155645845164, 0.52104424985101538] array([ 72.62003333, 95.76744019, 99.47895575, 100. ]) 1234a = np.array([1,2,3,4])print (a)print ('-----------')print (np.cumsum(a)) [1 2 3 4] ----------- [ 1 3 6 10] 123456789101112plt.figure(figsize=(6, 4))plt.bar(range(4), var_exp, alpha=0.5, align='center', label='individual explained variance')plt.step(range(4), cum_var_exp, where='mid', label='cumulative explained variance')plt.ylabel('Explained variance ratio')plt.xlabel('Principal components')plt.legend(loc='best')plt.tight_layout()plt.show() 1234matrix_w = np.hstack((eig_pairs[0][1].reshape(4,1), eig_pairs[1][1].reshape(4,1)))print('Matrix W:\n', matrix_w) Matrix W: [[ 0.52308496 -0.36956962] [-0.25956935 -0.92681168] [ 0.58184289 -0.01912775] [ 0.56609604 -0.06381646]] 4 开始降维——用投影矩阵降维样本矩阵X1Y = X_std.dot(matrix_w) 5 画图观察 降维前 和 降维后的样本分布123456789101112plt.figure(figsize=(6, 4))for lab, col in zip(('Iris-setosa', 'Iris-versicolor', 'Iris-virginica'), ('blue', 'red', 'green')): plt.scatter(X[y==lab, 0], X[y==lab, 1], label=lab, c=col)plt.xlabel('sepal_len')plt.ylabel('sepal_wid')plt.legend(loc='best')plt.tight_layout()plt.show() 123456789101112plt.figure(figsize=(6, 4))for lab, col in zip(('Iris-setosa', 'Iris-versicolor', 'Iris-virginica'), ('blue', 'red', 'green')): plt.scatter(Y[y==lab, 0], Y[y==lab, 1], label=lab, c=col)plt.xlabel('Principal Component 1')plt.ylabel('Principal Component 2')plt.legend(loc='lower center')plt.tight_layout()plt.show() 12 12 12 12]]></content>
      <categories>
        <category>machine_learning_in_action</category>
      </categories>
      <tags>
        <tag>PCA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pandas常用功能、函数]]></title>
    <url>%2F2018%2F04%2F19%2Fpandas%2FPandas%E5%B8%B8%E7%94%A8%E5%8A%9F%E8%83%BD%E3%80%81%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"></content>
      <categories>
        <category>pandas</category>
      </categories>
      <tags>
        <tag>pandas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于NLP的股价预测]]></title>
    <url>%2F2018%2F04%2F19%2Fmachine_learning_in_action%2F%E5%9F%BA%E4%BA%8ENLP%E7%9A%84%E8%82%A1%E4%BB%B7%E9%A2%84%E6%B5%8B%2F</url>
    <content type="text"><![CDATA[基于NLP的股价预测123import pandas as pdfrom sklearn.feature_extraction.text import CountVectorizerfrom sklearn.linear_model import LogisticRegression 1data = pd.read_csv('Combined_News_DJIA.csv') 每行是某公司 这一天股市数据；label表示当天涨/跌，Top表示依重要程度排列的当天新闻事件 通过NLP处理可以把这些字符串转换为 机器认识的语言 1data.head() .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } Date Label Top1 Top2 Top3 Top4 Top5 Top6 Top7 Top8 ... Top16 Top17 Top18 Top19 Top20 Top21 Top22 Top23 Top24 Top25 0 2008-08-08 0 b"Georgia 'downs two Russian warplanes' as cou... b'BREAKING: Musharraf to be impeached.' b'Russia Today: Columns of troops roll into So... b'Russian tanks are moving towards the capital... b"Afghan children raped with 'impunity,' U.N. ... b'150 Russian tanks have entered South Ossetia... b"Breaking: Georgia invades South Ossetia, Rus... b"The 'enemy combatent' trials are nothing but... ... b'Georgia Invades South Ossetia - if Russia ge... b'Al-Qaeda Faces Islamist Backlash' b'Condoleezza Rice: "The US would not act to p... b'This is a busy day: The European Union has ... b"Georgia will withdraw 1,000 soldiers from Ir... b'Why the Pentagon Thinks Attacking Iran is a ... b'Caucasus in crisis: Georgia invades South Os... b'Indian shoe manufactory - And again in a se... b'Visitors Suffering from Mental Illnesses Ban... b"No Help for Mexico's Kidnapping Surge" 1 2008-08-11 1 b'Why wont America and Nato help us? If they w... b'Bush puts foot down on Georgian conflict' b"Jewish Georgian minister: Thanks to Israeli ... b'Georgian army flees in disarray as Russians ... b"Olympic opening ceremony fireworks 'faked'" b'What were the Mossad with fraudulent New Zea... b'Russia angered by Israeli military sale to G... b'An American citizen living in S.Ossetia blam... ... b'Israel and the US behind the Georgian aggres... b'"Do not believe TV, neither Russian nor Geor... b'Riots are still going on in Montreal (Canada... b'China to overtake US as largest manufacturer' b'War in South Ossetia [PICS]' b'Israeli Physicians Group Condemns State Tort... b' Russia has just beaten the United States ov... b'Perhaps *the* question about the Georgia - R... b'Russia is so much better at war' b"So this is what it's come to: trading sex fo... 2 2008-08-12 0 b'Remember that adorable 9-year-old who sang a... b"Russia 'ends Georgia operation'" b'"If we had no sexual harassment we would hav... b"Al-Qa'eda is losing support in Iraq because ... b'Ceasefire in Georgia: Putin Outmaneuvers the... b'Why Microsoft and Intel tried to kill the XO... b'Stratfor: The Russo-Georgian War and the Bal... b"I'm Trying to Get a Sense of This Whole Geor... ... b'U.S. troops still in Georgia (did you know t... b'Why Russias response to Georgia was right' b'Gorbachev accuses U.S. of making a "serious ... b'Russia, Georgia, and NATO: Cold War Two' b'Remember that adorable 62-year-old who led y... b'War in Georgia: The Israeli connection' b'All signs point to the US encouraging Georgi... b'Christopher King argues that the US and NATO... b'America: The New Mexico?' b"BBC NEWS | Asia-Pacific | Extinction 'by man... 3 2008-08-13 0 b' U.S. refuses Israel weapons to attack Iran:... b"When the president ordered to attack Tskhinv... b' Israel clears troops who killed Reuters cam... b'Britain\'s policy of being tough on drugs is... b'Body of 14 year old found in trunk; Latest (... b'China has moved 10 *million* quake survivors... b"Bush announces Operation Get All Up In Russi... b'Russian forces sink Georgian ships ' ... b'Elephants extinct by 2020?' b'US humanitarian missions soon in Georgia - i... b"Georgia's DDOS came from US sources" b'Russian convoy heads into Georgia, violating... b'Israeli defence minister: US against strike ... b'Gorbachev: We Had No Choice' b'Witness: Russian forces head towards Tbilisi... b' Quarter of Russians blame U.S. for conflict... b'Georgian president says US military will ta... b'2006: Nobel laureate Aleksander Solzhenitsyn... 4 2008-08-14 1 b'All the experts admit that we should legalis... b'War in South Osetia - 89 pictures made by a ... b'Swedish wrestler Ara Abrahamian throws away ... b'Russia exaggerated the death toll in South O... b'Missile That Killed 9 Inside Pakistan May Ha... b"Rushdie Condemns Random House's Refusal to P... b'Poland and US agree to missle defense deal. ... b'Will the Russians conquer Tblisi? Bet on it,... ... b'Bank analyst forecast Georgian crisis 2 days... b"Georgia confict could set back Russia's US r... b'War in the Caucasus is as much the product o... b'"Non-media" photos of South Ossetia/Georgia ... b'Georgian TV reporter shot by Russian sniper ... b'Saudi Arabia: Mother moves to block child ma... b'Taliban wages war on humanitarian aid workers' b'Russia: World "can forget about" Georgia\'s... b'Darfur rebels accuse Sudan of mounting major... b'Philippines : Peace Advocate say Muslims nee... 5 rows × 27 columns 1 数据简单预处理、划分123# 根据日期划分 训练集 测试集train = data[data['Date'] &lt; '2015-01-01']test = data[data['Date'] &gt; '2014-12-31'] 12example = train.iloc[3,10]print(example) b&quot;The commander of a Navy air reconnaissance squadron that provides the President and the defense secretary the airborne ability to command the nation&#39;s nuclear weapons has been relieved of duty&quot; 12example2 = example.lower()print(example2) b&quot;the commander of a navy air reconnaissance squadron that provides the president and the defense secretary the airborne ability to command the nation&#39;s nuclear weapons has been relieved of duty&quot; 12example3 = CountVectorizer().build_tokenizer()(example2)print(example3) [&#39;the&#39;, &#39;commander&#39;, &#39;of&#39;, &#39;navy&#39;, &#39;air&#39;, &#39;reconnaissance&#39;, &#39;squadron&#39;, &#39;that&#39;, &#39;provides&#39;, &#39;the&#39;, &#39;president&#39;, &#39;and&#39;, &#39;the&#39;, &#39;defense&#39;, &#39;secretary&#39;, &#39;the&#39;, &#39;airborne&#39;, &#39;ability&#39;, &#39;to&#39;, &#39;command&#39;, &#39;the&#39;, &#39;nation&#39;, &#39;nuclear&#39;, &#39;weapons&#39;, &#39;has&#39;, &#39;been&#39;, &#39;relieved&#39;, &#39;of&#39;, &#39;duty&#39;] 1pd.DataFrame([[x,example3.count(x)] for x in set(example3)], columns = ['Word', 'Count']) .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } Word Count 0 the 5 1 command 1 2 secretary 1 3 weapons 1 4 has 1 5 defense 1 6 commander 1 7 squadron 1 8 relieved 1 9 navy 1 10 of 2 11 air 1 12 reconnaissance 1 13 provides 1 14 president 1 15 been 1 16 to 1 17 and 1 18 ability 1 19 nation 1 20 that 1 21 duty 1 22 nuclear 1 23 airborne 1 2 基于词频的特征提取——构造词频矩阵1）构造一个字符串数组StringList，每个元素是对应行所有top特征字符串拼成的长字符串 1234trainheadlines = [] for row in range(0,len(train.index)): trainheadlines.append(' '.join(str(x) for x in train.iloc[row,2:27]))print(trainheadlines[0:1]) [&#39;b&quot;Georgia \&#39;downs two Russian warplanes\&#39; as countries move to brink of war&quot; b\&#39;BREAKING: Musharraf to be impeached.\&#39; b\&#39;Russia Today: Columns of troops roll into South Ossetia; footage from fighting (YouTube)\&#39; b\&#39;Russian tanks are moving towards the capital of South Ossetia, which has reportedly been completely destroyed by Georgian artillery fire\&#39; b&quot;Afghan children raped with \&#39;impunity,\&#39; U.N. official says - this is sick, a three year old was raped and they do nothing&quot; b\&#39;150 Russian tanks have entered South Ossetia whilst Georgia shoots down two Russian jets.\&#39; b&quot;Breaking: Georgia invades South Ossetia, Russia warned it would intervene on SO\&#39;s side&quot; b&quot;The \&#39;enemy combatent\&#39; trials are nothing but a sham: Salim Haman has been sentenced to 5 1/2 years, but will be kept longer anyway just because they feel like it.&quot; b\&#39;Georgian troops retreat from S. Osettain capital, presumably leaving several hundred people killed. [VIDEO]\&#39; b\&#39;Did the U.S. Prep Georgia for War with Russia?\&#39; b\&#39;Rice Gives Green Light for Israel to Attack Iran: Says U.S. has no veto over Israeli military ops\&#39; b\&#39;Announcing:Class Action Lawsuit on Behalf of American Public Against the FBI\&#39; b&quot;So---Russia and Georgia are at war and the NYT\&#39;s top story is opening ceremonies of the Olympics? What a fucking disgrace and yet further proof of the decline of journalism.&quot; b&quot;China tells Bush to stay out of other countries\&#39; affairs&quot; b\&#39;Did World War III start today?\&#39; b\&#39;Georgia Invades South Ossetia - if Russia gets involved, will NATO absorb Georgia and unleash a full scale war?\&#39; b\&#39;Al-Qaeda Faces Islamist Backlash\&#39; b\&#39;Condoleezza Rice: &quot;The US would not act to prevent an Israeli strike on Iran.&quot; Israeli Defense Minister Ehud Barak: &quot;Israel is prepared for uncompromising victory in the case of military hostilities.&quot;\&#39; b\&#39;This is a busy day: The European Union has approved new sanctions against Iran in protest at its nuclear programme.\&#39; b&quot;Georgia will withdraw 1,000 soldiers from Iraq to help fight off Russian forces in Georgia\&#39;s breakaway region of South Ossetia&quot; b\&#39;Why the Pentagon Thinks Attacking Iran is a Bad Idea - US News &amp;amp; World Report\&#39; b\&#39;Caucasus in crisis: Georgia invades South Ossetia\&#39; b\&#39;Indian shoe manufactory - And again in a series of &quot;you do not like your work?&quot;\&#39; b\&#39;Visitors Suffering from Mental Illnesses Banned from Olympics\&#39; b&quot;No Help for Mexico\&#39;s Kidnapping Surge&quot;&#39;] 2）将这个字符串数组 转换成 词频矩阵，以便可以作为训练集 123basicvectorizer = CountVectorizer()basictrain = basicvectorizer.fit_transform(trainheadlines)print(basictrain.shape) # 生成了一个词频矩阵，总共1611个样本，31675个不重复的单词 (1611, 31675) 3 用逻辑回归 进行训练，查看训练结果精度 和 每个单词的权重参数ceof_123# 逻辑回归 fit 训练集词频矩阵basicmodel = LogisticRegression()basicmodel = basicmodel.fit(basictrain, train["Label"]) 123456testheadlines = []for row in range(0,len(test.index)): testheadlines.append(' '.join(str(x) for x in test.iloc[row,2:27]))basictest = basicvectorizer.transform(testheadlines)# 逻辑回归 predict 测试集词频矩阵predictions = basicmodel.predict(basictest) 123# 构造简易 混淆矩阵pd.crosstab(test["Label"], predictions, rownames=["Actual"], colnames=["Predicted"])#0.42 .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } Predicted 0 1 Actual 0 61 125 1 92 100 1观察：用精度做的混淆矩阵，精度只有42% 不理想 123456basicwords = basicvectorizer.get_feature_names() # 得到分词模型中所有单词（特征）basiccoeffs = basicmodel.coef_.tolist()[0] # 得到logistic模型中所有单词对应的 权重参数coeffdf = pd.DataFrame(&#123;'Word' : basicwords, 'Coefficient' : basiccoeffs&#125;)coeffdf = coeffdf.sort_values(['Coefficient', 'Word'], ascending=[0, 1]) # 从大到小排序coeffdf.head(10) # 前面的正相关 Coefficient Word 19419 0.497924 nigeria 25261 0.452526 self 29286 0.428011 tv 15998 0.425863 korea 20135 0.425716 olympics 15843 0.411636 kills 26323 0.411267 so 29256 0.394855 turn 10874 0.388555 fears 28274 0.384031 territory 1coeffdf.tail(10) # 前面的负相关 Coefficient Word 27299 -0.424441 students 8478 -0.427079 did 6683 -0.431925 congo 12818 -0.444069 hacking 7139 -0.448570 country 16949 -0.463116 low 3651 -0.470454 begin 25433 -0.494555 sex 24754 -0.549725 sanctions 24542 -0.587794 run 4 改进特征选择方法。用2个单词的词组 进行分词提取特征，构造新的频率矩阵12advancedvectorizer = CountVectorizer(ngram_range=(2,2))advancedtrain = advancedvectorizer.fit_transform(trainheadlines) 1print(advancedtrain.shape) (1611, 366721) 12advancedmodel = LogisticRegression()advancedmodel = advancedmodel.fit(advancedtrain, train["Label"]) 12345testheadlines = []for row in range(0,len(test.index)): testheadlines.append(' '.join(str(x) for x in test.iloc[row,2:27]))advancedtest = advancedvectorizer.transform(testheadlines)advpredictions = advancedmodel.predict(advancedtest) 12pd.crosstab(test["Label"], advpredictions, rownames=["Actual"], colnames=["Predicted"])#.57 Predicted 0 1 Actual 0 66 120 1 45 147 123456advwords = advancedvectorizer.get_feature_names()advcoeffs = advancedmodel.coef_.tolist()[0]advcoeffdf = pd.DataFrame(&#123;'Words' : advwords, 'Coefficient' : advcoeffs&#125;)advcoeffdf = advcoeffdf.sort_values(['Coefficient', 'Words'], ascending=[0, 1])advcoeffdf.head(10) Coefficient Words 272047 0.286533 right to 24710 0.275274 and other 285392 0.274698 set to 316194 0.262873 the first 157511 0.227943 in china 159522 0.224184 in south 125870 0.219130 found in 124411 0.216726 forced to 173246 0.211137 it has 322590 0.209239 this is 1advcoeffdf.tail(10) Coefficient Words 326846 -0.198495 to help 118707 -0.201654 fire on 155038 -0.209702 if he 242528 -0.211303 people are 31669 -0.213362 around the 321333 -0.215699 there is 327113 -0.221812 to kill 340714 -0.226289 up in 358917 -0.227516 with iran 315485 -0.331153 the country 12]]></content>
      <categories>
        <category>machine_learning_in_action</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>CountVectorizer</tag>
        <tag>LogisticRegression</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据挖掘——信用卡欺诈检测]]></title>
    <url>%2F2018%2F04%2F19%2Fmachine_learning_in_action%2FCreditCard%2F</url>
    <content type="text"><![CDATA[案例：用 逻辑回归 预测 信用卡欺诈12345import pandas as pdimport matplotlib.pyplot as pltimport numpy as np%matplotlib inline 12data = pd.read_csv("creditcard.csv")data.head() .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } Time V1 V2 V3 V4 V5 V6 V7 V8 V9 ... V21 V22 V23 V24 V25 V26 V27 V28 Amount Class 0 0.0 -1.359807 -0.072781 2.536347 1.378155 -0.338321 0.462388 0.239599 0.098698 0.363787 ... -0.018307 0.277838 -0.110474 0.066928 0.128539 -0.189115 0.133558 -0.021053 149.62 0 1 0.0 1.191857 0.266151 0.166480 0.448154 0.060018 -0.082361 -0.078803 0.085102 -0.255425 ... -0.225775 -0.638672 0.101288 -0.339846 0.167170 0.125895 -0.008983 0.014724 2.69 0 2 1.0 -1.358354 -1.340163 1.773209 0.379780 -0.503198 1.800499 0.791461 0.247676 -1.514654 ... 0.247998 0.771679 0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752 378.66 0 3 1.0 -0.966272 -0.185226 1.792993 -0.863291 -0.010309 1.247203 0.237609 0.377436 -1.387024 ... -0.108300 0.005274 -0.190321 -1.175575 0.647376 -0.221929 0.062723 0.061458 123.50 0 4 2.0 -1.158233 0.877737 1.548718 0.403034 -0.407193 0.095921 0.592941 -0.270533 0.817739 ... -0.009431 0.798278 -0.137458 0.141267 -0.206010 0.502292 0.219422 0.215153 69.99 0 5 rows × 31 columns 1数据预处理——归一化、去掉不用的列123456789count_classes = pd.value_counts(data['Class'], sort = True).sort_index()count_classes.plot(kind = 'bar')plt.title("Fraud class histogram")plt.xlabel("Class")plt.ylabel("Frequency")#发现样本分布十分不均衡#策略：统一不同类别样本总数 #1）oversample——过采样，把少的增多 #2) undersample——欠采样，把多的减少 &lt;matplotlib.text.Text at 0x5d32f27ef0&gt; 123456#用sklearn 函数来进行归一化(自带合并到原dataframe功能)from sklearn.preprocessing import StandardScaler data['normAmount'] = StandardScaler().fit_transform(data['Amount'].values.reshape(-1, 1))data = data.drop(['Time','Amount'],axis=1)data.head() .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } V1 V2 V3 V4 V5 V6 V7 V8 V9 V10 ... V21 V22 V23 V24 V25 V26 V27 V28 Class normAmount 0 -1.359807 -0.072781 2.536347 1.378155 -0.338321 0.462388 0.239599 0.098698 0.363787 0.090794 ... -0.018307 0.277838 -0.110474 0.066928 0.128539 -0.189115 0.133558 -0.021053 0 0.244964 1 1.191857 0.266151 0.166480 0.448154 0.060018 -0.082361 -0.078803 0.085102 -0.255425 -0.166974 ... -0.225775 -0.638672 0.101288 -0.339846 0.167170 0.125895 -0.008983 0.014724 0 -0.342475 2 -1.358354 -1.340163 1.773209 0.379780 -0.503198 1.800499 0.791461 0.247676 -1.514654 0.207643 ... 0.247998 0.771679 0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752 0 1.160686 3 -0.966272 -0.185226 1.792993 -0.863291 -0.010309 1.247203 0.237609 0.377436 -1.387024 -0.054952 ... -0.108300 0.005274 -0.190321 -1.175575 0.647376 -0.221929 0.062723 0.061458 0 0.140534 4 -1.158233 0.877737 1.548718 0.403034 -0.407193 0.095921 0.592941 -0.270533 0.817739 0.753074 ... -0.009431 0.798278 -0.137458 0.141267 -0.206010 0.502292 0.219422 0.215153 0 -0.073403 5 rows × 30 columns 1数据预处理——解决样本分布不均衡问题之undersample1234567891011121314151617181920212223242526272829303132# 方式一：采用“undersample”构建模型#把数据集切分为 样本 和 标记 存变量X = data.loc[:, data.columns != 'Class']y = data.loc[:, data.columns == 'Class']#计算欺诈样本总数number_records_fraud = len(data[data.Class == 1]) #取得欺诈行为的样本indexfraud_indices = np.array(data[data.Class == 1].index) #取得正常的样本indexnormal_indices = data[data.Class == 0].index # 随机选出 和 欺诈类数量相同的 正常Indexrandom_normal_indices = np.random.choice(normal_indices, number_records_fraud, replace = False)random_normal_indices = np.array(random_normal_indices)# 合并取得的两组index，作为欠采样indexunder_sample_indices = np.concatenate([fraud_indices,random_normal_indices])# 取得欠采样datasetunder_sample_data = data.iloc[under_sample_indices,:]#把undersample数据集切分为 样本 和 标记 存变量X_undersample = under_sample_data.loc[:, under_sample_data.columns != 'Class']y_undersample = under_sample_data.loc[:, under_sample_data.columns == 'Class']# 显示处理结果print("Percentage of normal transactions: ", len(under_sample_data[under_sample_data.Class == 0])/len(under_sample_data))print("Percentage of fraud transactions: ", len(under_sample_data[under_sample_data.Class == 1])/len(under_sample_data))print("Total number of transactions in resampled data: ", len(under_sample_data)) Percentage of normal transactions: 0.5 Percentage of fraud transactions: 0.5 Total number of transactions in resampled data: 984 1数据预处理——划分训练和测试集1234567891011121314151617181920#引入数据集切分函数from sklearn.cross_validation import train_test_split# Whole dataset 划分全部数据X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.3, random_state = 0)print("Number transactions train dataset: ", len(X_train))print("Number transactions test dataset: ", len(X_test))print("Total number of transactions: ", len(X_train)+len(X_test))# Undersampled dataset 划分欠采样数据X_train_undersample, X_test_undersample, y_train_undersample, y_test_undersample = train_test_split(X_undersample ,y_undersample ,test_size = 0.3 ,random_state = 0)print("")print("Number transactions train dataset: ", len(X_train_undersample))print("Number transactions test dataset: ", len(X_test_undersample))print("Total number of transactions: ", len(X_train_undersample)+len(X_test_undersample)) Number transactions train dataset: 199364 Number transactions test dataset: 85443 Total number of transactions: 284807 Number transactions train dataset: 688 Number transactions test dataset: 296 Total number of transactions: 984 2 交叉验证——在训练集上做，找最好的逻辑回归正则惩罚系数C1234#Recall = TP/(TP+FN) 这里适用召回率来检测from sklearn.linear_model import LogisticRegressionfrom sklearn.cross_validation import KFold, cross_val_scorefrom sklearn.metrics import confusion_matrix,recall_score,classification_report 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657# 自己实现召回率的 K=5的交叉验证函数（注意：此处是在训练集上的交叉验证）def printing_Kfold_scores(x_train_data,y_train_data): fold = KFold(len(y_train_data),5,shuffle=False) # Different C parameters #在sklearn里面，惩罚系数是倒数，比如100其实是0.01 #每个都试一遍，看哪个模型最好 c_param_range = [0.01,0.1,1,10,100] results_table = pd.DataFrame(index = range(len(c_param_range),2), columns = ['C_parameter','Mean recall score']) results_table['C_parameter'] = c_param_range # the k-fold will give 2 lists: train_indices = indices[0], test_indices = indices[1] j = 0 for c_param in c_param_range: print('-------------------------------------------') print('C parameter: ', c_param) print('-------------------------------------------') print('') recall_accs = [] # iteration：迭代轮数1-5 # indices：[0]表示训练集索引集合，[1]表示测试集索引集合 for iteration, indices in enumerate(fold,start=1): # Call the logistic regression model with a certain C parameter # C：指定惩罚项的参数 # penalty：指定惩罚项的算法 lr = LogisticRegression(C = c_param, penalty = 'l1') # Use the training data to fit the model. In this case, we use the portion of the fold to train the model # with indices[0]. We then predict on the portion assigned as the 'test cross validation' with indices[1] lr.fit(x_train_data.iloc[indices[0],:],y_train_data.iloc[indices[0],:].values.ravel()) # Predict values using the test indices in the training data y_pred_undersample = lr.predict(x_train_data.iloc[indices[1],:].values) # Calculate the recall score and append it to a list for recall scores representing the current c_parameter recall_acc = recall_score(y_train_data.iloc[indices[1],:].values,y_pred_undersample) recall_accs.append(recall_acc) print('Iteration ', iteration,': recall score = ', recall_acc) # The mean value of those recall scores is the metric we want to save and get hold of. results_table.loc[j,'Mean recall score'] = np.mean(recall_accs) j += 1 print('') print('Mean recall score ', np.mean(recall_accs)) print('') best_c = results_table.loc[results_table['Mean recall score'].idxmax()]['C_parameter'] # Finally, we can check which C parameter is the best amongst the chosen. print('*********************************************************************************') print('Best model to choose from cross validation is with C parameter = ', best_c) print('*********************************************************************************') return best_c 1best_c = printing_Kfold_scores(X_train_undersample,y_train_undersample) ------------------------------------------- C parameter: 0.01 ------------------------------------------- Iteration 1 : recall score = 0.931506849315 Iteration 2 : recall score = 0.931506849315 Iteration 3 : recall score = 1.0 Iteration 4 : recall score = 0.972972972973 Iteration 5 : recall score = 0.969696969697 Mean recall score 0.96113672826 ------------------------------------------- C parameter: 0.1 ------------------------------------------- Iteration 1 : recall score = 0.849315068493 Iteration 2 : recall score = 0.86301369863 Iteration 3 : recall score = 0.932203389831 Iteration 4 : recall score = 0.945945945946 Iteration 5 : recall score = 0.893939393939 Mean recall score 0.896883499368 ------------------------------------------- C parameter: 1 ------------------------------------------- Iteration 1 : recall score = 0.86301369863 Iteration 2 : recall score = 0.890410958904 Iteration 3 : recall score = 0.983050847458 Iteration 4 : recall score = 0.945945945946 Iteration 5 : recall score = 0.909090909091 Mean recall score 0.918302472006 ------------------------------------------- C parameter: 10 ------------------------------------------- Iteration 1 : recall score = 0.86301369863 Iteration 2 : recall score = 0.904109589041 Iteration 3 : recall score = 0.983050847458 Iteration 4 : recall score = 0.945945945946 Iteration 5 : recall score = 0.909090909091 Mean recall score 0.921042198033 ------------------------------------------- C parameter: 100 ------------------------------------------- Iteration 1 : recall score = 0.876712328767 Iteration 2 : recall score = 0.890410958904 Iteration 3 : recall score = 0.983050847458 Iteration 4 : recall score = 0.945945945946 Iteration 5 : recall score = 0.909090909091 Mean recall score 0.921042198033 ********************************************************************************* Best model to choose from cross validation is with C parameter = 0.01 ********************************************************************************* 3训练 + 测试——用best_C在训练集上重新训练一遍，再在undersample测试集上预测用 混淆矩阵 计算recall值1234567891011121314151617181920212223import itertoolsdef plot_confusion_matrix(cm, classes, title='Confusion matrix', cmap=plt.cm.Blues): """ This function prints and plots the confusion matrix. """ plt.imshow(cm, interpolation='nearest', cmap=cmap) plt.title(title) plt.colorbar() tick_marks = np.arange(len(classes)) plt.xticks(tick_marks, classes, rotation=0) plt.yticks(tick_marks, classes) thresh = cm.max() / 2. for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])): plt.text(j, i, cm[i, j], horizontalalignment="center", color="white" if cm[i, j] &gt; thresh else "black") plt.tight_layout() plt.ylabel('True label') plt.xlabel('Predicted label') 123456789101112131415161718lr = LogisticRegression(C = best_c, penalty = 'l1')lr.fit(X_train_undersample,y_train_undersample.values.ravel())y_pred_undersample = lr.predict(X_test_undersample.values)# Compute confusion matrixcnf_matrix = confusion_matrix(y_test_undersample,y_pred_undersample)np.set_printoptions(precision=2)print("Recall metric in the testing dataset: ", cnf_matrix[1,1]/(cnf_matrix[1,0]+cnf_matrix[1,1]))# Plot non-normalized confusion matrixclass_names = [0,1]plt.figure()plot_confusion_matrix(cnf_matrix , classes=class_names , title='Confusion matrix')plt.show()#混淆矩阵 显示模型分类效果 Recall metric in the testing dataset: 0.931972789116 3训练 + 测试——用best_C在训练集上重新训练一遍，再在 完整 测试集上预测用 混淆矩阵 计算recall值1234567891011121314151617lr = LogisticRegression(C = best_c, penalty = 'l1')lr.fit(X_train_undersample,y_train_undersample.values.ravel())y_pred = lr.predict(X_test.values)# Compute confusion matrixcnf_matrix = confusion_matrix(y_test,y_pred)np.set_printoptions(precision=2)print("Recall metric in the testing dataset: ", cnf_matrix[1,1]/(cnf_matrix[1,0]+cnf_matrix[1,1]))# Plot non-normalized confusion matrixclass_names = [0,1]plt.figure()plot_confusion_matrix(cnf_matrix , classes=class_names , title='Confusion matrix')plt.show() Recall metric in the testing dataset: 0.918367346939 12这里发现虽然recall值还可以，但是误伤了8581个（被检测成欺诈了），也就是精度accuracy有点低。故要权衡两者，都要较高才行 这里展示的是：不做样本平衡处理，直接把所有样本做交叉验证，发现效果很差1best_c = printing_Kfold_scores(X_train,y_train) ------------------------------------------- C parameter: 0.01 ------------------------------------------- Iteration 1 : recall score = 0.492537313433 Iteration 2 : recall score = 0.602739726027 Iteration 3 : recall score = 0.683333333333 Iteration 4 : recall score = 0.569230769231 Iteration 5 : recall score = 0.45 Mean recall score 0.559568228405 ------------------------------------------- C parameter: 0.1 ------------------------------------------- Iteration 1 : recall score = 0.567164179104 Iteration 2 : recall score = 0.616438356164 Iteration 3 : recall score = 0.683333333333 Iteration 4 : recall score = 0.584615384615 Iteration 5 : recall score = 0.525 Mean recall score 0.595310250644 ------------------------------------------- C parameter: 1 ------------------------------------------- Iteration 1 : recall score = 0.55223880597 Iteration 2 : recall score = 0.616438356164 Iteration 3 : recall score = 0.716666666667 Iteration 4 : recall score = 0.615384615385 Iteration 5 : recall score = 0.5625 Mean recall score 0.612645688837 ------------------------------------------- C parameter: 10 ------------------------------------------- Iteration 1 : recall score = 0.55223880597 Iteration 2 : recall score = 0.616438356164 Iteration 3 : recall score = 0.733333333333 Iteration 4 : recall score = 0.615384615385 Iteration 5 : recall score = 0.575 Mean recall score 0.61847902217 ------------------------------------------- C parameter: 100 ------------------------------------------- Iteration 1 : recall score = 0.55223880597 Iteration 2 : recall score = 0.616438356164 Iteration 3 : recall score = 0.733333333333 Iteration 4 : recall score = 0.615384615385 Iteration 5 : recall score = 0.575 Mean recall score 0.61847902217 ********************************************************************************* Best model to choose from cross validation is with C parameter = 10.0 ********************************************************************************* 1234567891011121314151617lr = LogisticRegression(C = best_c, penalty = 'l1')lr.fit(X_train,y_train.values.ravel())y_pred_undersample = lr.predict(X_test.values)# Compute confusion matrixcnf_matrix = confusion_matrix(y_test,y_pred_undersample)np.set_printoptions(precision=2)print("Recall metric in the testing dataset: ", cnf_matrix[1,1]/(cnf_matrix[1,0]+cnf_matrix[1,1]))# Plot non-normalized confusion matrixclass_names = [0,1]plt.figure()plot_confusion_matrix(cnf_matrix , classes=class_names , title='Confusion matrix')plt.show() Recall metric in the testing dataset: 0.619047619048 4 用predict_proba来测试 最好的逻辑回归 阈值1234567891011121314151617181920212223242526lr = LogisticRegression(C = 0.01, penalty = 'l1')lr.fit(X_train_undersample,y_train_undersample.values.ravel())y_pred_undersample_proba = lr.predict_proba(X_test_undersample.values)thresholds = [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]plt.figure(figsize=(10,10))j = 1for threshold in thresholds: y_test_predictions_high_recall = y_pred_undersample_proba[:,1] &gt; threshold plt.subplot(3,3,j) j += 1 # Compute confusion matrix cnf_matrix = confusion_matrix(y_test_undersample,y_test_predictions_high_recall) np.set_printoptions(precision=2) print("Recall metric in the testing dataset: ", cnf_matrix[1,1]/(cnf_matrix[1,0]+cnf_matrix[1,1])) # Plot non-normalized confusion matrix class_names = [0,1] plot_confusion_matrix(cnf_matrix , classes=class_names , title='Threshold &gt;= %s'%threshold) Recall metric in the testing dataset: 1.0 Recall metric in the testing dataset: 1.0 Recall metric in the testing dataset: 1.0 Recall metric in the testing dataset: 0.993197278912 Recall metric in the testing dataset: 0.931972789116 Recall metric in the testing dataset: 0.884353741497 Recall metric in the testing dataset: 0.843537414966 Recall metric in the testing dataset: 0.748299319728 Recall metric in the testing dataset: 0.578231292517 1数据预处理——解决样本分布不均衡问题之oversample123456import pandas as pd# 安装命令：pip install imblearnfrom imblearn.over_sampling import SMOTEfrom sklearn.ensemble import RandomForestClassifierfrom sklearn.metrics import confusion_matrixfrom sklearn.model_selection import train_test_split 12345678credit_cards=pd.read_csv('creditcard.csv')columns=credit_cards.columns# The labels are in the last column ('Class'). Simply remove it to obtain features columnsfeatures_columns=columns.delete(len(columns)-1)features=credit_cards[features_columns]labels=credit_cards['Class'] 12345#划分数据集features_train, features_test, labels_train, labels_test = train_test_split(features, labels, test_size=0.2, random_state=0) 123#★SMOTE算法通过给定的训练集，生成新的随机扩充训练集oversampler=SMOTE(random_state=0)os_features,os_labels=oversampler.fit_sample(features_train,labels_train) 12345#生成前，label=0print(len(labels_train[labels_train == 0]))#生成以后，label=1的变成和=0的一样多print(len(os_labels[os_labels == 1])) 227454 227454 123os_features = pd.DataFrame(os_features)os_labels = pd.DataFrame(os_labels)best_c = printing_Kfold_scores(os_features,os_labels) ------------------------------------------- C parameter: 0.01 ------------------------------------------- Iteration 1 : recall score = 0.890322580645 Iteration 2 : recall score = 0.894736842105 Iteration 3 : recall score = 0.968617904172 Iteration 4 : recall score = 0.944471922709 Iteration 5 : recall score = 0.958397907255 Mean recall score 0.931309431377 ------------------------------------------- C parameter: 0.1 ------------------------------------------- Iteration 1 : recall score = 0.890322580645 Iteration 2 : recall score = 0.894736842105 Iteration 3 : recall score = 0.970255615802 Iteration 4 : recall score = 0.959991646608 Iteration 5 : recall score = 0.96051922929 Mean recall score 0.93516518289 ------------------------------------------- C parameter: 1 ------------------------------------------- Iteration 1 : recall score = 0.890322580645 Iteration 2 : recall score = 0.894736842105 Iteration 3 : recall score = 0.970211353325 Iteration 4 : recall score = 0.960134533584 Iteration 5 : recall score = 0.960442290148 Mean recall score 0.935169519962 ------------------------------------------- C parameter: 10 ------------------------------------------- Iteration 1 : recall score = 0.890322580645 Iteration 2 : recall score = 0.894736842105 Iteration 3 : recall score = 0.970322009516 Iteration 4 : recall score = 0.95977182049 Iteration 5 : recall score = 0.960783020631 Mean recall score 0.935187254678 ------------------------------------------- C parameter: 100 ------------------------------------------- Iteration 1 : recall score = 0.890322580645 Iteration 2 : recall score = 0.894736842105 Iteration 3 : recall score = 0.969635941131 Iteration 4 : recall score = 0.960255437949 Iteration 5 : recall score = 0.960398324925 Mean recall score 0.935069825351 ********************************************************************************* Best model to choose from cross validation is with C parameter = 10.0 ********************************************************************************* 1234567891011121314151617lr = LogisticRegression(C = best_c, penalty = 'l1')lr.fit(os_features,os_labels.values.ravel())y_pred = lr.predict(features_test.values)# Compute confusion matrixcnf_matrix = confusion_matrix(labels_test,y_pred)np.set_printoptions(precision=2)print("Recall metric in the testing dataset: ", cnf_matrix[1,1]/(cnf_matrix[1,0]+cnf_matrix[1,1]))# Plot non-normalized confusion matrixclass_names = [0,1]plt.figure()plot_confusion_matrix(cnf_matrix , classes=class_names , title='Confusion matrix')plt.show() Recall metric in the testing dataset: 0.910891089109 12 12 12]]></content>
      <categories>
        <category>machine_learning_in_action</category>
      </categories>
      <tags>
        <tag>usersample</tag>
        <tag>oversample</tag>
        <tag>K折交叉验证</tag>
        <tag>混淆矩阵</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python常用功能、函数]]></title>
    <url>%2F2018%2F04%2F19%2Fpython%2FPython%E5%B8%B8%E7%94%A8%E5%8A%9F%E8%83%BD%E3%80%81%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[open()1简单的文件读写 123456789f = open("123.txt", "r");content = f.read();print(content);f.close();f = open("123.txt", "a+");f.write("\n");f.write("222222222222");f.close(); 2读取数据到矩阵 123456789101112#读取txt二维数据到矩阵def loadDataSet(fileName): dataMat = np.mat([0,0]) f = open(fileName) for line in f.readlines(): curLine = line.strip().split('\t') if len(curLine)==1 : continue curLineMat = np.mat(curLine) dataMat = np.vstack((dataMat, curLineMat)) #拼接矩阵 dataMat = dataMat[1:,:].astype(float) #不要第一行；转为纯数字 return dataMat set()将一个字符串拆成 单个字符 组成的字符集，可进行关系测试，删除重复数据，还可以计算交集、差集、并集等。 1234567&gt;&gt;&gt; x = set('runoob') &gt;&gt;&gt; y = set('google') &gt;&gt;&gt; x, y (set(['b', 'r', 'u', 'o', 'n']), set(['e', 'o', 'g', 'l'])) # 重复的被删除 &gt;&gt;&gt; x &amp; y # 交集 set(['o']) &gt;&gt;&gt; x | y # 并集 set(['b', 'e', 'g', 'l', 'o', 'n', 'r', 'u']) &gt;&gt;&gt; x - y # 差集 set(['r', 'b', 'u', 'n']) &gt;&gt;&gt; copy() = 赋值 传引用 =》内存不独立 =》 同步跟随变化 copy 浅拷贝 只拷贝父对象 =》父对象内存独立 =》只有子对象跟随变化 deepcopy 深拷贝 拷贝对象及其子对象 =》全部内存独立 =》 不跟随变化（深拷贝 和 浅拷贝——只对数组结构有用，int之类的没用）123456789101112131415161718a = [1, 2, 3, 4, ['a', 'b']] #原始对象 b = a #赋值，传对象的引用c = copy.copy(a) #对象拷贝，浅拷贝d = copy.deepcopy(a) #对象拷贝，深拷贝 a.append(5) #修改对象aa[4].append('c') #修改对象a中的['a', 'b']数组对象 print 'a = ', aprint 'b = ', bprint 'c = ', cprint 'd = ', d输出结果：a = [1, 2, 3, 4, ['a', 'b', 'c'], 5]b = [1, 2, 3, 4, ['a', 'b', 'c'], 5]c = [1, 2, 3, 4, ['a', 'b', 'c']]d = [1, 2, 3, 4, ['a', 'b']] zip()zip函数接受任意多个序列作为参数，返回一个tuple列表123456789101112print(zip(range(3),range(5)))[(0, 0), (1, 1), (2, 2)]for i,j in zip(range(3),range(5)): print(i) print(j)001122 运算符and和or 注意：and or 是python特有的短路运算符 表达式从左至右运算，若 or 的左侧逻辑值为 True ，则短路 or 后所有的表达式（不管是 and 还是 or），直接输出 or 左侧表达式 。 表达式从左至右运算，若 and 的左侧逻辑值为 False ，则短路其后所有 and 表达式，直到有 or 出现，输出 and 左侧表达式到 or 的左侧，参与接下来的逻辑运算。 若 or 的左侧为 False ，或者 and 的左侧为 True 则不能使用短路逻辑。]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[1向量、矩阵、向量范数、矩阵范数]]></title>
    <url>%2F2018%2F04%2F19%2Fmath%2Flinear_algebra%2F1%E5%90%91%E9%87%8F%E3%80%81%E7%9F%A9%E9%98%B5%E3%80%81%E5%90%91%E9%87%8F%E8%8C%83%E6%95%B0%E3%80%81%E7%9F%A9%E9%98%B5%E8%8C%83%E6%95%B0%2F</url>
    <content type="text"><![CDATA[向量向量内积 和 投影内积：1用点乘：a•b2用转置乘：a^T b3向量的模是范数的一种4 wT w《=》向量自己做内积 = 自身长度（模）²，因为投影结果还是w向量本身 投影：1 b在a上的投影 = |b|cosθ2 内积 = 向量1在向量2上的投影 * 他的长度（模，绝对值符号）；3 由2又有：b在a上的投影 = $\frac {a^T b} {||a||}$ ★ 向量外积向量范数 向量范数的定义和性质： 齐次性：数乘以后会放大相应的倍数 1-范数、2-范数、无穷范数： 稀疏性 和 0-范数： 稀疏性用到的范数：特殊的0-范数，他不满足齐次性；所以需要1-范数来辅助解决； 范数的几何意义： 向量组 初始单位向两组 向量组等价 线性相关 和 线性无关 施密特正交化由施密特正交化生成的 正交向量组 和 之前的线性无关向量组 可以互相线性表示 矩阵 O矩阵所有元素都为0的矩阵 一阶矩阵(a)等同于数a 矩阵的内积内积 A·B &lt;=&gt; ${A^T}B$矩阵范数矩阵的范数比向量的范数多一条相容性]]></content>
      <categories>
        <category>math</category>
        <category>linear_algebra</category>
      </categories>
      <tags>
        <tag>范数</tag>
        <tag>外积</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tensorflow整理]]></title>
    <url>%2F2018%2F04%2F19%2Ftensorflow%2F0_Tensorflow%E6%95%B4%E7%90%86%2F</url>
    <content type="text"><![CDATA[整理特色语法类 输出数值要加 .eval() 计算类 tf.nn.sigmoid() tf.nn.softmax tf.nn.softmax_cross_entropy_with_logits(y_hat, y)：交叉熵函数。 tf.add() tf.matmul() tf.reduce_mean() tf.square()平方算法类 优化器tf.train.GradientDescentOptimizer()：梯度下降 功能类 tf.equal(a, b) ：比较是否相等 tf.cast(a, “float”) :类型转换 tf.placeholder(“float”, [None, 10]) ：None is for infinite 这是一个n行10列的动态矩阵 tf.InteractiveSession(): 它能让你在运行图的时候，插入一些计算图 tf.Session(): 需要在启动session之前构建整个计算图，然后启动该计算图。 tf.random_normal([2, 3], mean=-1, stddev=4) ：高斯初始化生成一个 均值为-1，方差为4的矩阵 经验总结类 Tensorflow 定义的任何数据类型都推荐用 float32]]></content>
      <categories>
        <category>tensorflow</category>
      </categories>
      <tags>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tensorflow基本语法]]></title>
    <url>%2F2018%2F04%2F19%2Ftensorflow%2F1_Tensorflow%E5%9F%BA%E6%9C%AC%E7%94%A8%E6%B3%95%2F</url>
    <content type="text"><![CDATA[变量 和 Session()定义1234567891011121314import tensorflow as tfa = 3# Create a variable.w = tf.Variable([[0.5,1.0]])x = tf.Variable([[2.0],[1.0]]) y = tf.matmul(w, x) #variables have to be explicitly initialized before you can run Opsinit_op = tf.global_variables_initializer()with tf.Session() as sess: #第一种 session()定义方式 sess.run(init_op) print (y.eval()) Tensor(&quot;Variable_11/read:0&quot;, shape=(1, 2), dtype=float32) [[ 2.]] 计算操作需要在 Session()——会话（计算图的区域） 中进行；而且在使用 variable 时还需要 初始化全局变量操作 矩阵 和 常量1234567891011121314151617181920212223# float32tf.zeros([3, 4], int32) ==&gt; [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]]# 'tensor' is [[1, 2, 3], [4, 5, 6]]tf.zeros_like(tensor) ==&gt; [[0, 0, 0], [0, 0, 0]]tf.ones([2, 3], int32) ==&gt; [[1, 1, 1], [1, 1, 1]]# 'tensor' is [[1, 2, 3], [4, 5, 6]]tf.ones_like(tensor) ==&gt; [[1, 1, 1], [1, 1, 1]]# Constant 1-D Tensor populated with value list.tensor = tf.constant([1, 2, 3, 4, 5, 6, 7]) =&gt; [1 2 3 4 5 6 7]# Constant 2-D tensor populated with scalar value -1.tensor = tf.constant(-1.0, shape=[2, 3]) =&gt; [[-1. -1. -1.] [-1. -1. -1.]]tf.linspace(10.0, 12.0, 3, name="linspace") =&gt; [ 10.0 11.0 12.0]# 令'start' is 3# 'limit' is 18# 'delta' is 3tf.range(start, limit, delta) ==&gt; [3, 6, 9, 12, 15] 对比 Numpy 操作，发现区别并不大 另一种 Session() 定义123456789101112import tensorflow as tf# 初始化生成一个 均值为-1，方差为4的矩阵norm = tf.random_normal([2, 3], mean=-1, stddev=4)# Shuffle the first dimension of a tensorc = tf.constant([[1, 2], [3, 4], [5, 6]])shuff = tf.random_shuffle(c)# Each time we run these ops, different results are generatedsess = tf.Session() #第二种 session()定义方式print (sess.run(norm))print (sess.run(shuff)) [[ 3.78657341 -3.94182277 -3.65419507] [-7.0396409 -5.51102114 3.56082773]] [[5 6] [1 2] [3 4]] 累加 演示12345678910state = tf.Variable(0)new_value = tf.add(state, tf.constant(1))update = tf.assign(state, new_value)with tf.Session() as sess: sess.run(tf.global_variables_initializer()) print(sess.run(state)) for _ in range(3): sess.run(update) print(sess.run(state)) 0 1 2 3 train.Saver() 保存 Session() 操作123456789101112#tf.train.Saverw = tf.Variable([[0.5,1.0]])x = tf.Variable([[2.0],[1.0]])y = tf.matmul(w, x)init_op = tf.global_variables_initializer()saver = tf.train.Saver()with tf.Session() as sess: sess.run(init_op)# Do some work with the model.# Save the variables to disk. save_path = saver.save(sess, "C://tensorflow//model//test") print ("Model saved in file: ", save_path) Model saved in file: C://tensorflow//model//test convert_to_tensor() 转换 numpy 的语法为 tensorflow 类型12345import numpy as npa = np.zeros((3,3))ta = tf.convert_to_tensor(a)with tf.Session() as sess: print(sess.run(ta)) [[ 0. 0. 0.] [ 0. 0. 0.] [ 0. 0. 0.]] 不推荐使用，还是建议使用 tensorflow 语法 placeholder() 定义 待输入占位变量12345input1 = tf.placeholder(tf.float32)input2 = tf.placeholder(tf.float32)output = tf.mul(input1, input2)with tf.Session() as sess: print(sess.run([output], feed_dict=&#123;input1:[7.], input2:[2.]&#125;)) [array([ 14.], dtype=float32)] 这些变量可以在使用时再临时 用 feed_dict字典格式 赋值]]></content>
      <categories>
        <category>tensorflow</category>
      </categories>
      <tags>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python面向对象和类]]></title>
    <url>%2F2018%2F04%2F19%2Fpython%2FPython%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%E5%92%8C%E7%B1%BB%2F</url>
    <content type="text"><![CDATA[Python 面向对象Python从设计之初就已经是一门面向对象的语言，正因为如此，在Python中创建一个类和对象是很容易的。本章节我们将详细介绍Python的面向对象编程。 如果你以前没有接触过面向对象的编程语言，那你可能需要先了解一些面向对象语言的一些基本特征，在头脑里头形成一个基本的面向对象的概念，这样有助于你更容易的学习Python的面向对象编程。 接下来我们先来简单的了解下面向对象的一些基本特征。 面向对象技术简介 类(Class): 用来描述具有相同的属性和方法的对象的集合。它定义了该集合中每个对象所共有的属性和方法。对象是类的实例。 类变量：类变量在整个实例化的对象中是公用的。类变量定义在类中且在函数体之外。类变量通常不作为实例变量使用。 数据成员：类变量或者实例变量, 用于处理类及其实例对象的相关的数据。 方法重写：如果从父类继承的方法不能满足子类的需求，可以对其进行改写，这个过程叫方法的覆盖（override），也称为方法的重写。 实例变量：定义在方法中的变量，只作用于当前实例的类。 继承：即一个派生类（derived class）继承基类（base class）的字段和方法。继承也允许把一个派生类的对象作为一个基类对象对待。例如，有这样一个设计：一个Dog类型的对象派生自Animal类，这是模拟”是一个（is-a）”关系（例图，Dog是一个Animal）。 实例化：创建一个类的实例，类的具体对象。 方法：类中定义的函数。 对象：通过类定义的数据结构实例。对象包括两个数据成员（类变量和实例变量）和方法。 创建类使用 class 语句来创建一个新类，class 之后为类的名称并以冒号结尾: 123class ClassName: '类的帮助信息' #类文档字符串 class_suite #类体 类的帮助信息可以通过ClassName.doc查看。 class_suite 由类成员，方法，数据属性组成。 实例以下是一个简单的 Python 类的例子: 实例1234567891011121314151617#!/usr/bin/python# -*- coding: UTF-8 -*- class Employee: '所有员工的基类' empCount = 0 def __init__(self, name, salary): self.name = name self.salary = salary Employee.empCount += 1 def displayCount(self): print "Total Employee %d" % Employee.empCount def displayEmployee(self): print "Name : ", self.name, ", Salary: ", self.salary empCount 变量是一个类变量，它的值将在这个类的所有实例之间共享。你可以在内部类或外部类使用 Employee.empCount 访问。 第一种方法init()方法是一种特殊的方法，被称为类的构造函数或初始化方法，当创建了这个类的实例时就会调用该方法 self 代表类的实例，self 在定义类的方法时是必须有的，虽然在调用时不必传入相应的参数。 self代表类的实例，而非类类的方法与普通的函数只有一个特别的区别——它们必须有一个额外的第一个参数名称, 按照惯例它的名称是 self。 1234567class Test: def prt(self): print(self) print(self.__class__) t = Test()t.prt() 以上实例执行结果为： 12&lt;__main__.Test instance at 0x10d066878&gt;__main__.Test 从执行结果可以很明显的看出，self 代表的是类的实例，代表当前对象的地址，而 self.class 则指向类。 self 不是 python 关键字，我们把他换成 runoob 也是可以正常执行的: 实例1234567class Test: def prt(runoob): print(runoob) print(runoob.__class__) t = Test()t.prt() 以上实例执行结果为： 12&lt;__main__.Test instance at 0x10d066878&gt;__main__.Test 创建实例对象实例化类其他编程语言中一般用关键字 new，但是在 Python 中并没有这个关键字，类的实例化类似函数调用方式。 以下使用类的名称 Employee 来实例化，并通过 init 方法接收参数。 1234"创建 Employee 类的第一个对象"emp1 = Employee("Zara", 2000)"创建 Employee 类的第二个对象"emp2 = Employee("Manni", 5000) 访问属性您可以使用点号 . 来访问对象的属性。使用如下类的名称访问类变量: 123emp1.displayEmployee()emp2.displayEmployee()print "Total Employee %d" % Employee.empCount 完整实例： 实例12345678910111213141516171819202122class Employee: '所有员工的基类' empCount = 0 def __init__(self, name, salary): self.name = name self.salary = salary Employee.empCount += 1 def displayCount(self): print "Total Employee %d" % Employee.empCount def displayEmployee(self): print "Name : ", self.name, ", Salary: ", self.salary "创建 Employee 类的第一个对象"emp1 = Employee("Zara", 2000)"创建 Employee 类的第二个对象"emp2 = Employee("Manni", 5000)emp1.displayEmployee()emp2.displayEmployee()print "Total Employee %d" % Employee.empCount 执行以上代码输出结果如下： 123Name : Zara ,Salary: 2000Name : Manni ,Salary: 5000Total Employee 2 你可以添加，删除，修改类的属性，如下所示： 123emp1.age = 7 # 添加一个 'age' 属性emp1.age = 8 # 修改 'age' 属性del emp1.age # 删除 'age' 属性 你也可以使用以下函数的方式来访问属性： getattr(obj, name[, default]) : 访问对象的属性。 hasattr(obj,name) : 检查是否存在一个属性。 setattr(obj,name,value) : 设置一个属性。如果属性不存在，会创建一个新属性。 delattr(obj, name) : 删除属性。 12345hasattr(emp1, 'age') # 如果存在 'age' 属性返回 True。getattr(emp1, 'age') # 返回 'age' 属性的值setattr(emp1, 'age', 8) # 添加属性 'age' 值为 8delattr(emp1, 'age') # 删除属性 'age' Python内置类属性 dict : 类的属性（包含一个字典，由类的数据属性组成） doc :类的文档字符串 name: 类名 module: 类定义所在的模块（类的全名是’main.className’，如果类位于一个导入模块mymod中，那么className.module 等于 mymod） bases : 类的所有父类构成元素（包含了一个由所有父类组成的元组） Python内置类属性调用实例如下： 实例1234567891011121314151617181920class Employee: '所有员工的基类' empCount = 0 def __init__(self, name, salary): self.name = name self.salary = salary Employee.empCount += 1 def displayCount(self): print "Total Employee %d" % Employee.empCount def displayEmployee(self): print "Name : ", self.name, ", Salary: ", self.salary print "Employee.__doc__:", Employee.__doc__print "Employee.__name__:", Employee.__name__print "Employee.__module__:", Employee.__module__print "Employee.__bases__:", Employee.__bases__print "Employee.__dict__:", Employee.__dict__ 执行以上代码输出结果如下： 12345Employee.__doc__: 所有员工的基类Employee.__name__: EmployeeEmployee.__module__: __main__Employee.__bases__: ()Employee.__dict__: &#123;'__module__': '__main__', 'displayCount': &lt;function displayCount at 0x10a939c80&gt;, 'empCount': 0, 'displayEmployee': &lt;function displayEmployee at 0x10a93caa0&gt;, '__doc__': '\xe6\x89\x80\xe6\x9c\x89\xe5\x91\x98\xe5\xb7\xa5\xe7\x9a\x84\xe5\x9f\xba\xe7\xb1\xbb', '__init__': &lt;function __init__ at 0x10a939578&gt;&#125; python对象销毁(垃圾回收)Python 使用了引用计数这一简单技术来跟踪和回收垃圾。 在 Python 内部记录着所有使用中的对象各有多少引用。 一个内部跟踪变量，称为一个引用计数器。 当对象被创建时， 就创建了一个引用计数， 当这个对象不再需要时， 也就是说， 这个对象的引用计数变为0 时， 它被垃圾回收。但是回收不是”立即”的， 由解释器在适当的时机，将垃圾对象占用的内存空间回收。 1234567a = 40 # 创建对象 &lt;40&gt;b = a # 增加引用， &lt;40&gt; 的计数c = [b] # 增加引用. &lt;40&gt; 的计数del a # 减少引用 &lt;40&gt; 的计数b = 100 # 减少引用 &lt;40&gt; 的计数c[0] = -1 # 减少引用 &lt;40&gt; 的计数 垃圾回收机制不仅针对引用计数为0的对象，同样也可以处理循环引用的情况。循环引用指的是，两个对象相互引用，但是没有其他变量引用他们。这种情况下，仅使用引用计数是不够的。Python 的垃圾收集器实际上是一个引用计数器和一个循环垃圾收集器。作为引用计数的补充， 垃圾收集器也会留心被分配的总量很大（及未通过引用计数销毁的那些）的对象。 在这种情况下， 解释器会暂停下来， 试图清理所有未引用的循环。 实例析构函数 del ，del在对象销毁的时候被调用，当对象不再被使用时，del方法运行： 实例123456789101112131415class Point: def __init__( self, x=0, y=0): self.x = x self.y = y def __del__(self): class_name = self.__class__.__name__ print class_name, "销毁" pt1 = Point()pt2 = pt1pt3 = pt1print id(pt1), id(pt2), id(pt3) # 打印对象的iddel pt1del pt2del pt3 以上实例运行结果如下： 123083401324 3083401324 3083401324Point 销毁 注意：通常你需要在单独的文件中定义一个类， 类的继承面向对象的编程带来的主要好处之一是代码的重用，实现这种重用的方法之一是通过继承机制。继承完全可以理解成类之间的类型和子类型关系。 需要注意的地方：继承语法 class 派生类名（基类名）：//… 基类名写在括号里，基本类是在类定义的时候，在元组之中指明的。 在python中继承中的一些特点： 1：在继承中基类的构造（init()方法）不会被自动调用，它需要在其派生类的构造中亲自专门调用。 2：在调用基类的方法时，需要加上基类的类名前缀，且需要带上self参数变量。区别在于类中调用普通函数时并不需要带上self参数 3：Python总是首先查找对应类型的方法，如果它不能在派生类中找到对应的方法，它才开始到基类中逐个查找。（先在本类中查找调用的方法，找不到才去基类中找）。 如果在继承元组中列了一个以上的类，那么它就被称作”多重继承” 。 语法： 派生类的声明，与他们的父类类似，继承的基类列表跟在类名之后，如下所示： 123class SubClassName (ParentClass1[, ParentClass2, ...]): 'Optional class documentation string' class_suite 实例1234567891011121314151617181920212223242526class Parent: # 定义父类 parentAttr = 100 def __init__(self): print "调用父类构造函数" def parentMethod(self): print '调用父类方法' def setAttr(self, attr): Parent.parentAttr = attr def getAttr(self): print "父类属性 :", Parent.parentAttr class Child(Parent): # 定义子类 def __init__(self): print "调用子类构造方法" def childMethod(self): print '调用子类方法' c = Child() # 实例化子类c.childMethod() # 调用子类的方法c.parentMethod() # 调用父类方法c.setAttr(200) # 再次调用父类的方法 - 设置属性值c.getAttr() # 再次调用父类的方法 - 获取属性值 以上代码执行结果如下： 1234调用子类构造方法调用子类方法调用父类方法父类属性 : 200 你可以继承多个类 12345678class A: # 定义类 A.....class B: # 定义类 B.....class C(A, B): # 继承类 A 和 B..... 你可以使用issubclass()或者isinstance()方法来检测。 issubclass() - 布尔函数判断一个类是另一个类的子类或者子孙类，语法：issubclass(sub,sup) isinstance(obj, Class) 布尔函数如果obj是Class类的实例对象或者是一个Class子类的实例对象则返回true。 方法重写如果你的父类方法的功能不能满足你的需求，你可以在子类重写你父类的方法： 实例： 实例12345678910class Parent: # 定义父类 def myMethod(self): print '调用父类方法' class Child(Parent): # 定义子类 def myMethod(self): print '调用子类方法' c = Child() # 子类实例c.myMethod() # 子类调用重写方法 执行以上代码输出结果如下： 1调用子类方法 基础重载方法下表列出了一些通用的功能，你可以在自己的类重写： 序号 方法, 描述 &amp; 简单的调用 1 init ( self [,args…] )构造函数简单的调用方法: obj = className(args) 2 del( self )析构方法, 删除一个对象简单的调用方法 : del obj 3 repr( self )转化为供解释器读取的形式简单的调用方法 : repr(obj) 4 str( self )用于将值转化为适于人阅读的形式简单的调用方法 : str(obj) 5 cmp ( self, x )对象比较简单的调用方法 : cmp(obj, x) 运算符重载Python同样支持运算符重载，实例如下： 实例1234567891011121314class Vector: def __init__(self, a, b): self.a = a self.b = b def __str__(self): return 'Vector (%d, %d)' % (self.a, self.b) def __add__(self,other): return Vector(self.a + other.a, self.b + other.b) v1 = Vector(2,10)v2 = Vector(5,-2)print v1 + v2 以上代码执行结果如下所示: 1Vector(7,8) 类属性与方法类的私有属性__private_attrs：两个下划线开头，声明该属性为私有，不能在类的外部被使用或直接访问。在类内部的方法中使用时 self.__private_attrs。 类的方法在类的内部，使用 def 关键字可以为类定义一个方法，与一般函数定义不同，类方法必须包含参数 self,且为第一个参数 类的私有方法__private_method：两个下划线开头，声明该方法为私有方法，不能在类地外部调用。在类的内部调用 self.__private_methods 实例1234567891011121314class JustCounter: __secretCount = 0 # 私有变量 publicCount = 0 # 公开变量 def count(self): self.__secretCount += 1 self.publicCount += 1 print self.__secretCount counter = JustCounter()counter.count()counter.count()print counter.publicCountprint counter.__secretCount # 报错，实例不能访问私有变量 Python 通过改变名称来包含类名: 1234567122Traceback (most recent call last): File "test.py", line 17, in &lt;module&gt; print counter.__secretCount # 报错，实例不能访问私有变量AttributeError: JustCounter instance has no attribute '__secretCount' Python不允许实例化的类访问私有数据，但你可以使用 object._className__attrName 访问属性，将如下代码替换以上代码的最后一行代码： 12.........................print counter._JustCounter__secretCount 执行以上代码，执行结果如下： 12341222 单下划线、双下划线、头尾双下划线说明： foo: 定义的是特殊方法，一般是系统定义名字 ，类似 init() 之类的。 _foo: 以单下划线开头的表示的是 protected 类型的变量，即保护类型只能允许其本身与子类进行访问，不能用于 from module import * __foo: 双下划线的表示的是私有类型(private)的变量, 只能是允许这个类本身进行访问了。]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[备忘记录]]></title>
    <url>%2F2018%2F04%2F19%2FOTHERS%2F%E5%A4%87%E5%BF%98%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[变量命名标准整型：语义名数组：语义名+S 或 语义名+Arr一般：语义名+数据结构名+其他特征 比如：classArrTry 标题title设置标准为了好看，把“库/框架名”放前面，把“算法名”放后面，如：Tensorflow - LinearRegression 标签tags设置标准文章内所包含的 技术点名称]]></content>
      <categories>
        <category>其他</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Numpy常用功能、函数]]></title>
    <url>%2F2018%2F04%2F18%2Fnumpy%2FNumpy%E5%B8%B8%E7%94%A8%E5%8A%9F%E8%83%BD%E3%80%81%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[Numpynp.matrix()：矩阵标准写法1234567891011&gt;&gt;&gt; a = np.matrix('1 2 7; 3 4 8; 5 6 9') #单行写法&gt;&gt;&gt; a #矩阵的换行必须是用分号(;)隔开，内部数据必须为字符串形式(“ ”)，矩matrix([[1, 2, 7], #阵的元素之间必须以空格隔开。[3, 4, 8],[5, 6, 9]])&gt;&gt;&gt; b=np.array([[1,5],[3,2]])&gt;&gt;&gt; x=np.matrix(b) #矩阵中的data可以为数组对象。&gt;&gt;&gt; xmatrix([[1, 5],[3, 2]]) np.inf：无穷大np.set_printoptions(suppress=True)不用科学记数法输出 np.multiply()：矩阵对应元素相乘np.linalg：矩阵运算，中的常用函数diag 以一维数组的形式返回方阵的对角线元素dot 矩阵乘法det 计算矩阵行列式eig 计算方阵的特征值和特征向量inv 计算方阵的逆lstsq 计算Ax=b的最小二乘解norm 计算范数（ord=指定范数），默认为２范数pinv 计算矩阵的Moore-Penrose伪逆qr 计算qr分解svd 计算奇异值分解solve 解线性方程组Ax=b，其中A为一个方阵trace 计算对角线元素的和 np.newaxis： 新增纬度1import numpy as np In [30]: 123#np.newaxis多用于防止取出一行或列后数据降维a = np.arange(6).reshape(2,3);a Out[30]: 12array([[0, 1, 2], [3, 4, 5]]) In [31]: 1234# np.newaxis加在哪个位置，就能在shape里看到相应位置增加了一个纬度c = a[:, np.newaxis] #这里相当于a[:, np.newaxis, :]加在了行的后面列的前面print(c)print("c.shape",c.shape) 1234[[[0 1 2]] [[3 4 5]]]c.shape (2, 1, 3) In [27]: 123d = a[:,np.newaxis, 2] #这里是取2号列print(d)print("d.shape",d.shape) 123[[2] [5]]d.shape (2, 1) In [25]: 12345e = a[1, np.newaxis, :] #这里是取1号行print(e)print("e.shape",e.shape)[[3 4 5]]e.shape (1, 3)]]></content>
      <categories>
        <category>numpy</category>
      </categories>
      <tags>
        <tag>numpy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MathJax数学公式语法]]></title>
    <url>%2F2018%2F04%2F16%2Fhexo%2FMathJax%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F%E8%AF%AD%E6%B3%95%2F</url>
    <content type="text"><![CDATA[概述在Markdown中输入数学公式需要LaTeX语法的支持。 基本语法呈现位置 正文(inline)中的LaTeX公式用\$…$定义 语句为\sum_{i=0}^N\int_{a}^{b}g(t,i)\text{d}t 显示为 $\sum_{i=0}^N\int_{a}^{b}g(t,i)\text{d}t$ 单独显示(display)的LaTeX公式用\$\$…$$定义，此时公式居中并放大显示 语句为\sum_{i=0}^N\int_{a}^{b}g(t,i)\text{d}t 显示为 ​ \sum_{i=0}^N\int_{a}^{b}g(t,i)\text{d}t 下列描述语句中若非特别指出均省略\$…$ 希腊字母 显示 命令 显示 命令 α \alpha β \beta γ \gamma δ \delta ε \epsilon ζ \zeta η \eta θ \theta ι \iota κ \kappa λ \lambda μ \mu ν \nu ξ \xi π \pi ρ \rho σ \sigma τ \tau υ \upsilon φ \phi χ \chi ψ \psi ω \omega - 若需要大写希腊字母，将命令首字母大写即可。 \Gamma呈现为 $\Gamma$- 若需要斜体希腊字母，将命令前加上var前缀即可。 \varGamma呈现为 $\varGamma$ 字母修饰上下标 上标：^ 下标：_ 举例：C_n^2呈现为 $C_n^2$ 矢量 \vec a呈现为 $\vec a$ \overrightarrow{xy}呈现为 $\overrightarrow{xy}$ 字体 Typewriter：\mathtt{A}呈现为 $\mathtt{A}$ $\mathtt{ABCDEFGHIJKLMNOPQRSTUVWXYZABCDEFGHIJKLMNOPQRSTUVWXYZ}$ Blackboard Bold：\mathbb{A}呈现为 $\mathbb{A}$ $\mathbb{ABCDEFGHIJKLMNOPQRSTUVWXYZABCDEFGHIJKLMNOPQRSTUVWXYZ}$ Sans Serif：\mathsf{A}呈现为 $\mathsf{A}$ $\mathsf{ABCDEFGHIJKLMNOPQRSTUVWXYZABCDEFGHIJKLMNOPQRSTUVWXYZ}$ 分组 使用{}将具有相同等级的内容扩入其中，成组处理 举例：10^{10}呈现为 $10^{10}$，而10^10呈现为 $10^10$ 括号 小括号：()呈现为() 中括号：[]呈现为[] 尖括号：\langle,\rangle呈现为⟨⟩ 此处为与分组符号{}相区别，使用转义字符\ 使用\left(或\right)使符号大小与邻近的公式相适应；该语句适用于所有括号类型 (\frac{x}{y})呈现为$(\frac{x}{y})$ 而\left(\frac{x}{y}\right)呈现为$\left(\frac{x}{y}\right)$ 求和、极限与积分 求和：\sum 举例：`\sum_{i=1}^n{a_i}`呈现为$\sum_{i=1}^n{a_i}$ 极限：\lim_{x\to 0}呈现为 $\lim_{x\to 0}$ 积分：\int 举例：`\int_0^\infty{fxdx}`呈现为 $\int_0^\infty{fxdx}$ 分式与根式 分式(fractions)：\frac{公式1}{公式2}呈现为 $\frac{a+b}{a-b}$ 根式：\sqrt[x]{y}呈现为$\sqrt[x]{y}$ 特殊函数 \函数名 举例：\sin x，\ln x，\max(A,B,C)呈现为 $\sin x ,\ln x, \max(A,B,C)$ 特殊符号 显示 命令 ∞ \infty ∪ \cup ∩ \cap ⊂ \subset ⊆ \subseteq ⊃ \supset ∈ \in ∉ \notin ∅ \varnothing ∀ \forall ∃ \exists ¬ \lnot ∇ \nabla ∂ \partial 空格 LaTeX语法本身会忽略空格的存在 小空格：a\ b呈现为 $a\ b$ 4格空格：a\quad b呈现为 $a\quad b$ 矩阵基本语法 起始标记\begin{matrix}``，结束标记``\end{matrix} 每一行末尾标记\\，行间元素之间以&amp;分隔 举例 12345$$\begin&#123;matrix&#125;1&amp;0&amp;0\\0&amp;1&amp;0\\0&amp;0&amp;1\\\end&#123;matrix&#125;$$ 呈现为 ​ \begin{matrix}1&0&0\\0&1&0\\0&0&1\\\end{matrix} 矩阵边框 在起始、结束标记处用下列词替换matrix pmatrix：小括号边框 bmatrix：中括号边框 Bmatrix：大括号边框 vmatrix：单竖线边框 Vmatrix：双竖线边框 省略元素 横省略号：\cdots 竖省略号：\vdots 斜省略号：\ddots 举例 123456$$\begin&#123;bmatrix&#125;&#123;a_&#123;11&#125;&#125;&amp;&#123;a_&#123;12&#125;&#125;&amp;&#123;\cdots&#125;&amp;&#123;a_&#123;1n&#125;&#125;\\&#123;a_&#123;21&#125;&#125;&amp;&#123;a_&#123;22&#125;&#125;&amp;&#123;\cdots&#125;&amp;&#123;a_&#123;2n&#125;&#125;\\&#123;\vdots&#125;&amp;&#123;\vdots&#125;&amp;&#123;\ddots&#125;&amp;&#123;\vdots&#125;\\&#123;a_&#123;m1&#125;&#125;&amp;&#123;a_&#123;m2&#125;&#125;&amp;&#123;\cdots&#125;&amp;&#123;a_&#123;mn&#125;&#125;\\\end&#123;bmatrix&#125;$$ 呈现为 \begin{bmatrix} {a_{11}}&{a_{12}}&{\cdots}&{a_{1n}}\\ {a_{21}}&{a_{22}}&{\cdots}&{a_{2n}}\\ {\vdots}&{\vdots}&{\ddots}&{\vdots}\\ {a_{m1}}&{a_{m2}}&{\cdots}&{a_{mn}}\\ \end{bmatrix}阵列 需要array环境：起始、结束处以{array}声明 对齐方式：在{array}后以{}逐行统一声明 左对齐：l；居中：c；右对齐：r 竖直线：在声明对齐方式时，插入|建立竖直线 插入水平线：\hline 举例 12345$$\begin&#123;array&#125;&#123;c|lll&#125;&#123;↓&#125;&amp;&#123;a&#125;&amp;&#123;b&#125;&amp;&#123;c&#125;\\&#123;R_1&#125;&amp;&#123;c&#125;&amp;&#123;b&#125;&amp;&#123;a&#125;\\&#123;R_2&#125;&amp;&#123;b&#125;&amp;&#123;c&#125;&amp;&#123;c&#125;\\\end&#123;array&#125;$$ 呈现为 \begin{array}{c|lll} {↓}&{a}&{b}&{c}\\ {R_1}&{c}&{b}&{a}\\ {R_2}&{b}&{c}&{c}\\ \end{array}方程组 需要cases环境：起始、结束处以{cases}声明 举例 123456$$\begin&#123;cases&#125;a_1x+b_1y+c_1z=d_1\\a_2x+b_2y+c_2z=d_2\\a_3x+b_3y+c_3z=d_3\\\end&#123;cases&#125;$$ 呈现为 \begin{cases} a_1x+b_1y+c_1z=d_1\\ a_2x+b_2y+c_2z=d_2\\ a_3x+b_3y+c_3z=d_3\\ \end{cases}]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>MathJax</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo Next优化&踩坑]]></title>
    <url>%2F2018%2F04%2F16%2Fhexo%2FHexo%20Next%E4%BC%98%E5%8C%96%26%E8%B8%A9%E5%9D%91%2F</url>
    <content type="text"><![CDATA[一、界面 篇1 添加动态背景修改配置文件在主题配置文件中找到canvas_nest: false，把它改为canvas_nest: true 修改_layout.swig打开 next/layout/_layout.swig在 &lt; /body&gt;之前添加代码 1234&#123;% if theme.canvas_nest %&#125;&lt;script type=&quot;text/javascript&quot;color=&quot;0,0,0&quot; opacity=&apos;0.5&apos; zIndex=&quot;-2&quot; count=&quot;50&quot; src=&quot;//cdn.bootcss.com/canvas-nest.js/1.0.0/canvas-nest.min.js&quot;&gt;&lt;/script&gt;&#123;% endif %&#125; 配置项说明 color ：线条颜色, 默认: &#39;0,0,0&#39;；三个数字分别为(R,G,B) opacity: 线条透明度（0~1）, 默认: 0.5 count: 线条的总数量, 默认: 150 zIndex: 背景的z-index属性，css属性用于控制所在层的位置, 默认: -1 2 直接展开文章全部目录搜索打开这个文件：sidebar-toc.styl 把下面的内容注释掉： 1234567//取消逐渐展开，改为直接展开所有TOC//.post-toc .nav .nav-child &#123; display: none; &#125;.post-toc .nav .active &gt; .nav-child &#123; display: block; &#125;.post-toc .nav .active-current &gt; .nav-child &#123; display: block; &amp; &gt; .nav-item &#123; display: block; &#125;&#125; 3 添加文章结束标记在 next\layout_macro\post.swig 中wechat-subscriber.swig 上面加入如下代码： 1234&lt;!-- 添加文章结束标记 --&gt;&#123;% if not is_index %&#125; &lt;div style=&quot;text-align:center;color: #000;font-size:14px;&quot;&gt;----------------- The End -----------------&lt;/div&gt;&#123;% endif %&#125; 4 实现主页文章预览效果进入hexo博客项目的themes/next目录用文本编辑器打开_config.yml文件搜索”auto_excerpt”,找到如下部分：12345# Automatically Excerpt. Not recommand.# Please use &lt;!-- more --&gt; in the post to control excerpt accurately.auto_excerpt: enable: true length: 150 把enable值设置为true，就可以控制文章在主页的显示了 5 添加MathJax数学公式支持在主题中开启mathjax开关如何使用了主题了，别忘了在主题（Theme）中开启mathjax开关，下面以next主题为例，介绍下如何打开mathjax开关。 进入到主题目录，找到_config.yml配置问题，把mathjax默认的false修改为true，具体如下： 1234# MathJax Supportmathjax: enable: true per_page: true 别着急，这样还不够，还需要在文章的Front-matter里打开mathjax开关，如下： 123456---title: index.htmldate: 2016-12-28 21:01:30tags:mathjax: true-- 更换Hexo的markdown渲染引擎hexo-renderer-kramed引擎是在默认的渲染引擎hexo-renderer-marked的基础上修改了一些bug，两者比较接近，也比较轻量级。 12npm uninstall hexo-renderer-marked --savenpm install hexo-renderer-kramed --save 执行上面的命令即可，先卸载原来的渲染引擎，再安装新的。 然后，跟换引擎后行间公式可以正确渲染了，但是这样还没有完全解决问题，行内公式的渲染还是有问题，因为hexo-renderer-kramed引擎也有语义冲突的问题。接下来到博客根目录下，找到node_modules\kramed\lib\rules\inline.js，把第11行的escape变量的值做相应的修改： 12// escape: /^\\([\\`*&#123;&#125;\[\]()#$+\-.!_&gt;])/, escape: /^\\([`*\[\]()#+\-.!_&gt;])/ 同时把第20行的em变量也要做相应的修改。 12// em: /^\b_((?:__|[\s\S])+?)_\b|^\*((?:\*\*|[\s\S])+?)\*(?!\*)/, em: /^\*((?:\*\*|[\s\S])+?)\*(?!\*)/ 重新启动hexo（先clean再generate）,问题完美解决。 6 调整页面CSS布局为了加宽文章页面显示，在下面两个文件中添加自定义代码在 themes\next\source\css_custom\custom.styl 中：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657// Custom styles.//边框效果/*// 最上面.content-wrap &#123; padding: 0 40px 40px 40px;&#125;.posts-expand &#123; padding-top: 0;&#125;// 文章.post &#123; margin-top: 60px; margin-bottom: 60px; padding: 25px; -webkit-box-shadow: 1px 1px 1px 1px rgba(202, 203, 203, .5); -moz-box-shadow: 1px 1px 1px 1px rgba(202, 203, 204, .5); &#125;// 右上.sidebar-position-right .header-inner &#123; -webkit-box-shadow: 1px 1px 1px 1px rgba(202, 203, 203, .5); -moz-box-shadow: 1px 1px 1px 1px rgba(202, 203, 204, .5);&#125;// 右下.sidebar .sidebar-inner &#123; -webkit-box-shadow: 1px 1px 1px 1px rgba(202, 203, 203, .5); -moz-box-shadow: 1px 1px 1px 1px rgba(202, 203, 204, .5);&#125;// 右上.sidebar-position-right .header-inner &#123; -webkit-box-shadow: 0 1px 0 0 #262a30; -moz-box-shadow: 0 1px 0 0 #262a30;&#125;*/// 最下面.sidebar-position-right .footer-inner &#123; padding-left: 40px; padding-right: 280px;&#125;// 首页文章添加分割线.posts-expand .post-eof &#123; display: block; margin: 80px auto 60px; width: 61.8%; height: 1px; background: #bbb; text-align: center;&#125;.sidebar-inner &#123; padding: 20px 0 0 0;&#125;.music163 &#123; margin: 20px 0 0 0;&#125; 在 D:\wxy555123.github.io\themes\next\source\css_variables\custom.styl 中：1234567891011121314151617181920// base.styl Layout sizes// --------------------------------------------------//$main-desktop = 960px $main-desktop = 1230px //new 主宽度，也调大防止sidebar遮挡$main-desktop-large = 1200px//$content-desktop = 700px$content-desktop = 990px //new 文章宽度调大$content-desktop-large = 900px$content-desktop-padding = 40px$content-tablet-padding = 10px$content-mobile-padding = 8px$sidebar-desktop = 240px$footer-height = 50px$gap-between-main-and-footer = 100px 7 添加 Gitment 评论系统简介本文介绍hexo next主题(5.1.2)集成giment评论系统的过程。所谓gitment就是把评论放到github的issues系统里，评论支持md，比较适合程序员. 一.注册OAuth Application点击https://github.com/settings/applications/new注册，注意Authorization callback URL填自己的网站urlhttp://yangq.me/.记下Client ID和Client Secret. 二.修改themes/next/_config.yml在其中添加: 123456789# Gitment# Introduction: https://imsun.net/posts/gitment-introduction/gitment: enable: true githubID: yourid repo: yourrepo ClientID: yourid ClientSecret: yoursecret lazy: true123456789 注意:格式要正确，该空格的一定要空格。所有的yourXXX都换成自己的. 三.修改gitment.swig在主题下layout/_third-party/comments/目录下中修改文件gitment.swig使得能够正确初始化： 修改红框标记的id字段，用日期时间戳代替，使得id不会超过50个字符 8 博文压缩博文压缩用于加快网站访问速度 step1:在根目录新建 gulpfile.js 文件 step2:方法一，使用下面代码和命令：123456789101112131415161718192021222324252627282930313233var gulp = require('gulp');var minifycss = require('gulp-minify-css');var uglify = require('gulp-uglify');var htmlmin = require('gulp-htmlmin');var htmlclean = require('gulp-htmlclean');// 压缩 public 目录 cssgulp.task('minify-css', function() &#123; return gulp.src('./public/**/*.css') .pipe(minifycss()) .pipe(gulp.dest('./public'));&#125;);// 压缩 public 目录 htmlgulp.task('minify-html', function() &#123; return gulp.src('./public/**/*.html') .pipe(htmlclean()) .pipe(htmlmin(&#123; removeComments: true, minifyJS: true, minifyCSS: true, minifyURLs: true, &#125;)) .pipe(gulp.dest('./public'))&#125;);// 压缩 public/js 目录 jsgulp.task('minify-js', function() &#123; return gulp.src('./public/**/*.js') .pipe(uglify()) .pipe(gulp.dest('./public'));&#125;);// 执行 gulp 命令时执行的任务gulp.task('default', [ 'minify-html','minify-css','minify-js']); 命令为 hexo g&amp;&amp;gulp方法二，使用下面代码和命令：1234567891011121314151617181920212223242526272829303132333435363738var gulp = require('gulp'), uglify = require('gulp-uglify'), rename = require('gulp-rename'), cssmin = require('gulp-minify-css'), imagemin = require('gulp-imagemin');//JS压缩gulp.task('uglify', function() &#123; return gulp.src('././public/js/*.js') .pipe(uglify()) .pipe(gulp.dest('././public/js/'));&#125;);//public-fancybox-js压缩gulp.task('fancybox:js', function() &#123; return gulp.src('././public/fancybox/jquery.fancybox.js') .pipe(uglify()) .pipe(gulp.dest('././public/fancybox/'));&#125;);//public-fancybox-css压缩gulp.task('fancybox:css', function() &#123; return gulp.src('././public/fancybox/jquery.fancybox.css') .pipe(cssmin()) .pipe(gulp.dest('././public/fancybox/'));&#125;);//CSS压缩gulp.task('cssmin', function() &#123; return gulp.src('././public/css/style.css') .pipe(cssmin()) .pipe(gulp.dest('././public/css/'));&#125;);//图片压缩gulp.task('images', function() &#123; gulp.src('././public/img/*.*') .pipe(imagemin(&#123; progressive: false &#125;)) .pipe(gulp.dest('././public/img/'));&#125;);gulp.task('build', ['uglify', 'cssmin', 'images', 'fancybox:js', 'fancybox:css']); 命令为 hexo g&amp;&amp;gulp build 二、操作 篇1 Hexo命令 安装主题：用git clone到themes文件夹中 生成静态文件：hexo g 启动本地服务器：hexo s 发布到远程网站：hexo d （hexo d -g 生成的后自动发布） ​ 创建文章：hexo new “标题” （默认就在“post”目录里） 创建草稿：hexo new draft “标题” 把草稿转到“post”目录：hexo publish “标题” ​ （注：中间的命令可以用哦个首字母简写） 2 Git命令 清空你的 github.io 仓库项目中所有文件进入到.deploy_git 文件夹下123git rm -rf *git commit -m &apos;clean all file&apos;git push 3 修改Hexo生成文件模版可在根目录 scaffolds 文件夹下修改3类文章模版 4 添加创建文件后，用vscode自动打开脚本在根目录下新建 scripts 文件夹，里面新建 js 文件 名字随意，代码如下： 12345var exec = require("child_process").exec;hexo.on("new", function(data) &#123; exec("Code.exe " + [data.path]);&#125;); 以后每次执行 hexo n 新建文件后都会自动运行 vscode 打开编辑 5修改node.js默认启动路径右键node.js快捷方式，把“起始位置”属性修改为以下内容：1D:\wxy555123.github.io\]]></content>
      <categories>
        <category>hexo</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[数据的标准化（归一化）]]></title>
    <url>%2F2018%2F04%2F13%2Fmachine_learning_theory%2F%E6%95%B0%E6%8D%AE%E7%9A%84%E6%A0%87%E5%87%86%E5%8C%96%EF%BC%88%E5%BD%92%E4%B8%80%E5%8C%96%EF%BC%89%2F</url>
    <content type="text"><![CDATA[几种归一化方式数据的标准化（normalization）是将数据按比例缩放，使之落入一个小的特定区间。在某些比较和评价的指标处理中经常会用到，去除数据的单位限制，将其转化为无量纲的纯数值，便于不同单位或量级的指标能够进行比较和加权。 其中最典型的就是数据的归一化处理，即将数据统一映射到[0,1]区间上，常见的数据归一化的方法有： 1）min-max标准化(Min-max normalization) 也叫离差标准化，是对原始数据的线性变换，使结果落到[0,1]区间，转换函数如下： 其中max为样本数据的最大值，min为样本数据的最小值。这种方法有一个缺陷就是当有新数据加入时，可能导致max和min的变化，需要重新定义。 2）log函数转换 通过以10为底的log函数转换的方法同样可以实现归一下，具体方法如下： 看了下网上很多介绍都是x*=log10(x)，其实是有问题的，这个结果并非一定落到[0,1]区间上，应该还要除以log10(max)，max为样本数据最大值，并且所有的数据都要大于等于1。 3）atan函数转换 用反正切函数也可以实现数据的归一化： 使用这个方法需要注意的是如果想映射的区间为[0,1]，则数据都应该大于等于0，小于0的数据将被映射到[-1,0]区间上。 而并非所有数据标准化的结果都映射到[0,1]区间上，其中最常见的标准化方法就是Z标准化，也是SPSS中最为常用的标准化方法： 4）z-score 标准化(zero-mean normalization) 也叫标准差标准化，经过处理的数据符合标准正态分布，即均值为0，标准差为1，其转化函数为： 其中μ为所有样本数据的均值，σ为所有样本数据的标准差。]]></content>
      <categories>
        <category>machine_learning_theory</category>
      </categories>
  </entry>
</search>
