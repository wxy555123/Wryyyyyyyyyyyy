<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Numpy常用功能、函数]]></title>
    <url>%2F2018%2F04%2F19%2Fnumpy%2FNumpy%E5%B8%B8%E7%94%A8%E5%8A%9F%E8%83%BD%E3%80%81%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[Numpynp.matrix()：矩阵标准写法1234567891011&gt;&gt;&gt; a = np.matrix('1 2 7; 3 4 8; 5 6 9') #单行写法&gt;&gt;&gt; a #矩阵的换行必须是用分号(;)隔开，内部数据必须为字符串形式(“ ”)，矩matrix([[1, 2, 7], #阵的元素之间必须以空格隔开。[3, 4, 8],[5, 6, 9]])&gt;&gt;&gt; b=np.array([[1,5],[3,2]])&gt;&gt;&gt; x=np.matrix(b) #矩阵中的data可以为数组对象。&gt;&gt;&gt; xmatrix([[1, 5],[3, 2]]) np.inf：无穷大np.set_printoptions(suppress=True)不用科学记数法输出 np.multiply()：矩阵对应元素相乘numpy.linalg：矩阵运算，中的常用函数diag 以一维数组的形式返回方阵的对角线元素dot 矩阵乘法det 计算矩阵行列式eig 计算方阵的特征值和特征向量inv 计算方阵的逆lstsq 计算Ax=b的最小二乘解norm 计算范数（ord=指定范数），默认为２范数pinv 计算矩阵的Moore-Penrose伪逆qr 计算qr分解svd 计算奇异值分解solve 解线性方程组Ax=b，其中A为一个方阵trace 计算对角线元素的和]]></content>
      <categories>
        <category>numpy</category>
      </categories>
      <tags>
        <tag>numpy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pandas常用功能、函数]]></title>
    <url>%2F2018%2F04%2F19%2Fpandas%2FPandas%E5%B8%B8%E7%94%A8%E5%8A%9F%E8%83%BD%E3%80%81%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"></content>
      <categories>
        <category>pandas</category>
      </categories>
      <tags>
        <tag>pandas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PCA 手写主成分分析]]></title>
    <url>%2F2018%2F04%2F19%2Fmachine_learning_in_action%2FPCA%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[PCA 手写主成分分析1234import numpy as npimport pandas as pddf = pd.read_csv('iris.data')df.head() .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } 5.1 3.5 1.4 0.2 Iris-setosa 0 4.9 3.0 1.4 0.2 Iris-setosa 1 4.7 3.2 1.3 0.2 Iris-setosa 2 4.6 3.1 1.5 0.2 Iris-setosa 3 5.0 3.6 1.4 0.2 Iris-setosa 4 5.4 3.9 1.7 0.4 Iris-setosa 1 数据预处理123#加上列名df.columns=['sepal_len', 'sepal_wid', 'petal_len', 'petal_wid', 'class']df.head() .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } sepal_len sepal_wid petal_len petal_wid class 0 4.9 3.0 1.4 0.2 Iris-setosa 1 4.7 3.2 1.3 0.2 Iris-setosa 2 4.6 3.1 1.5 0.2 Iris-setosa 3 5.0 3.6 1.4 0.2 Iris-setosa 4 5.4 3.9 1.7 0.4 Iris-setosa 2 画图，进行降维特征分析1234# split data table into data X and class labels yX = df.iloc[:,0:4].valuesy = df.iloc[:,4].values 123456789101112131415161718192021222324252627# 把每个特征用于分类的结果，都画成条形图，观察哪个特征更容易划分种类from matplotlib import pyplot as pltimport mathlabel_dict = &#123;1: 'Iris-Setosa', 2: 'Iris-Versicolor', 3: 'Iris-Virgnica'&#125;feature_dict = &#123;0: 'sepal length [cm]', 1: 'sepal width [cm]', 2: 'petal length [cm]', 3: 'petal width [cm]'&#125;plt.figure(figsize=(8, 6))for cnt in range(4): plt.subplot(2, 2, cnt+1) for lab in ('Iris-setosa', 'Iris-versicolor', 'Iris-virginica'): plt.hist(X[y==lab, cnt], label=lab, bins=10, alpha=0.3,) plt.xlabel(feature_dict[cnt]) plt.legend(loc='upper right', fancybox=True, fontsize=8)plt.tight_layout()plt.show() 123# 特征 归一化from sklearn.preprocessing import StandardScalerX_std = StandardScaler().fit_transform(X) 3 开始降维（发现有2个有用特征，决定从4维降到2维）1 计算样本X的 协方差矩阵（有4个特征，所以是4x4）1234# 自己算 协方差矩阵mean_vec = np.mean(X_std, axis=0)cov_mat = (X_std - mean_vec).T.dot((X_std - mean_vec)) / (X_std.shape[0]-1)print('Covariance matrix \n%s' %cov_mat) Covariance matrix [[ 1.00675676 -0.10448539 0.87716999 0.82249094] [-0.10448539 1.00675676 -0.41802325 -0.35310295] [ 0.87716999 -0.41802325 1.00675676 0.96881642] [ 0.82249094 -0.35310295 0.96881642 1.00675676]] 12# numpy算 协方差矩阵print('NumPy covariance matrix: \n%s' %np.cov(X_std.T)) NumPy covariance matrix: [[ 1.00675676 -0.10448539 0.87716999 0.82249094] [-0.10448539 1.00675676 -0.41802325 -0.35310295] [ 0.87716999 -0.41802325 1.00675676 0.96881642] [ 0.82249094 -0.35310295 0.96881642 1.00675676]] 2 对协方差矩阵进行 特征值分解123456cov_mat = np.cov(X_std.T)eig_vals, eig_vecs = np.linalg.eig(cov_mat)print('Eigenvectors \n%s' %eig_vecs)print('\nEigenvalues \n%s' %eig_vals) Eigenvectors [[ 0.52308496 -0.36956962 -0.72154279 0.26301409] [-0.25956935 -0.92681168 0.2411952 -0.12437342] [ 0.58184289 -0.01912775 0.13962963 -0.80099722] [ 0.56609604 -0.06381646 0.63380158 0.52321917]] Eigenvalues [ 2.92442837 0.93215233 0.14946373 0.02098259] 3 把特征值从大到小排列，并配对特征向量1234567891011# Make a list of (eigenvalue, eigenvector) tupleseig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:,i]) for i in range(len(eig_vals))]print (eig_pairs)print ('----------')# Sort the (eigenvalue, eigenvector) tuples from high to loweig_pairs.sort(key=lambda x: x[0], reverse=True)# Visually confirm that the list is correctly sorted by decreasing eigenvaluesprint('Eigenvalues in descending order:')for i in eig_pairs: print(i[0],"对应",i[1]) [(2.9244283691111144, array([ 0.52308496, -0.25956935, 0.58184289, 0.56609604])), (0.93215233025350641, array([-0.36956962, -0.92681168, -0.01912775, -0.06381646])), (0.14946373489813314, array([-0.72154279, 0.2411952 , 0.13962963, 0.63380158])), (0.020982592764270606, array([ 0.26301409, -0.12437342, -0.80099722, 0.52321917]))] ---------- Eigenvalues in descending order: 2.92442836911 对应 [ 0.52308496 -0.25956935 0.58184289 0.56609604] 0.932152330254 对应 [-0.36956962 -0.92681168 -0.01912775 -0.06381646] 0.149463734898 对应 [-0.72154279 0.2411952 0.13962963 0.63380158] 0.0209825927643 对应 [ 0.26301409 -0.12437342 -0.80099722 0.52321917] 4 通过前面特征值累加所占比重 的图像，判断取前多少特征值合适，组成投影矩阵W12345tot = sum(eig_vals)var_exp = [(i / tot)*100 for i in sorted(eig_vals, reverse=True)]print (var_exp)cum_var_exp = np.cumsum(var_exp)cum_var_exp [72.620033326920336, 23.147406858644135, 3.7115155645845164, 0.52104424985101538] array([ 72.62003333, 95.76744019, 99.47895575, 100. ]) 1234a = np.array([1,2,3,4])print (a)print ('-----------')print (np.cumsum(a)) [1 2 3 4] ----------- [ 1 3 6 10] 123456789101112plt.figure(figsize=(6, 4))plt.bar(range(4), var_exp, alpha=0.5, align='center', label='individual explained variance')plt.step(range(4), cum_var_exp, where='mid', label='cumulative explained variance')plt.ylabel('Explained variance ratio')plt.xlabel('Principal components')plt.legend(loc='best')plt.tight_layout()plt.show() 1234matrix_w = np.hstack((eig_pairs[0][1].reshape(4,1), eig_pairs[1][1].reshape(4,1)))print('Matrix W:\n', matrix_w) Matrix W: [[ 0.52308496 -0.36956962] [-0.25956935 -0.92681168] [ 0.58184289 -0.01912775] [ 0.56609604 -0.06381646]] 5 用投影矩阵降维样本矩阵X1Y = X_std.dot(matrix_w) 4 画图观察 降维前 和 降维后的样本分布123456789101112plt.figure(figsize=(6, 4))for lab, col in zip(('Iris-setosa', 'Iris-versicolor', 'Iris-virginica'), ('blue', 'red', 'green')): plt.scatter(X[y==lab, 0], X[y==lab, 1], label=lab, c=col)plt.xlabel('sepal_len')plt.ylabel('sepal_wid')plt.legend(loc='best')plt.tight_layout()plt.show() 123456789101112plt.figure(figsize=(6, 4))for lab, col in zip(('Iris-setosa', 'Iris-versicolor', 'Iris-virginica'), ('blue', 'red', 'green')): plt.scatter(Y[y==lab, 0], Y[y==lab, 1], label=lab, c=col)plt.xlabel('Principal Component 1')plt.ylabel('Principal Component 2')plt.legend(loc='lower center')plt.tight_layout()plt.show() 12 12 12 12]]></content>
      <categories>
        <category>machine_learning_in_action</category>
      </categories>
      <tags>
        <tag>PCA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[备忘记录]]></title>
    <url>%2F2018%2F04%2F19%2FOTHERS%2F%E5%A4%87%E5%BF%98%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[变量命名标准整型：语义名数组：语义名+S 或 语义名+Arr一般：语义名+数据结构名+其他特征 比如：classArrTry 标签tags设置标准文章内所包含的 技术点名称]]></content>
      <categories>
        <category>其他</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Python常用功能、函数]]></title>
    <url>%2F2018%2F04%2F19%2Fpython%2FPython%E5%B8%B8%E7%94%A8%E5%8A%9F%E8%83%BD%E3%80%81%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[open()1简单的文件读写 123456789f = open("123.txt", "r");content = f.read();print(content);f.close();f = open("123.txt", "a+");f.write("\n");f.write("222222222222");f.close(); 2读取数据到矩阵 123456789101112#读取txt二维数据到矩阵def loadDataSet(fileName): dataMat = np.mat([0,0]) f = open(fileName) for line in f.readlines(): curLine = line.strip().split('\t') if len(curLine)==1 : continue curLineMat = np.mat(curLine) dataMat = np.vstack((dataMat, curLineMat)) #拼接矩阵 dataMat = dataMat[1:,:].astype(float) #不要第一行；转为纯数字 return dataMat set()将一个字符串拆成 单个字符 组成的字符集，可进行关系测试，删除重复数据，还可以计算交集、差集、并集等。 1234567&gt;&gt;&gt; x = set('runoob') &gt;&gt;&gt; y = set('google') &gt;&gt;&gt; x, y (set(['b', 'r', 'u', 'o', 'n']), set(['e', 'o', 'g', 'l'])) # 重复的被删除 &gt;&gt;&gt; x &amp; y # 交集 set(['o']) &gt;&gt;&gt; x | y # 并集 set(['b', 'e', 'g', 'l', 'o', 'n', 'r', 'u']) &gt;&gt;&gt; x - y # 差集 set(['r', 'b', 'u', 'n']) &gt;&gt;&gt; copy() = 赋值 传引用 =》内存不独立 =》 同步跟随变化 copy 浅拷贝 只拷贝父对象 =》父对象内存独立 =》只有子对象跟随变化 deepcopy 深拷贝 拷贝对象及其子对象 =》全部内存独立 =》 不跟随变化（深拷贝 和 浅拷贝——只对数组结构有用，int之类的没用）123456789101112131415161718a = [1, 2, 3, 4, ['a', 'b']] #原始对象 b = a #赋值，传对象的引用c = copy.copy(a) #对象拷贝，浅拷贝d = copy.deepcopy(a) #对象拷贝，深拷贝 a.append(5) #修改对象aa[4].append('c') #修改对象a中的['a', 'b']数组对象 print 'a = ', aprint 'b = ', bprint 'c = ', cprint 'd = ', d输出结果：a = [1, 2, 3, 4, ['a', 'b', 'c'], 5]b = [1, 2, 3, 4, ['a', 'b', 'c'], 5]c = [1, 2, 3, 4, ['a', 'b', 'c']]d = [1, 2, 3, 4, ['a', 'b']] zip()zip函数接受任意多个序列作为参数，返回一个tuple列表123456789101112print(zip(range(3),range(5)))[(0, 0), (1, 1), (2, 2)]for i,j in zip(range(3),range(5)): print(i) print(j)001122]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[1向量、矩阵、向量范数、矩阵范数]]></title>
    <url>%2F2018%2F04%2F19%2Fmath%2Flinear_algebra%2F1%E5%90%91%E9%87%8F%E3%80%81%E7%9F%A9%E9%98%B5%E3%80%81%E5%90%91%E9%87%8F%E8%8C%83%E6%95%B0%E3%80%81%E7%9F%A9%E9%98%B5%E8%8C%83%E6%95%B0%2F</url>
    <content type="text"><![CDATA[向量向量内积 和 投影内积：1用点乘：a•b2用转置乘：a^T b3向量的模是范数的一种4 wT w《=》向量自己做内积 = 自身长度（模）²，因为投影结果还是w向量本身 投影：1 b在a上的投影 = |b|cosθ2 内积 = 向量1在向量2上的投影 * 他的长度（模，绝对值符号）；3 由2又有：b在a上的投影 = $\frac {a^T b} {||a||}$ ★ 向量外积向量范数 向量范数的定义和性质： 齐次性：数乘以后会放大相应的倍数 1-范数、2-范数、无穷范数： 稀疏性 和 0-范数：稀疏性用到的范数：特殊的0-范数，他不满足齐次性；所以需要1-范数来辅助解决； 范数的几何意义： 向量组 初始单位向两组 向量组等价 线性相关 和 线性无关 施密特正交化由施密特正交化生成的 正交向量组 和 之前的线性无关向量组 可以互相线性表示 矩阵 O矩阵所有元素都为0的矩阵 一阶矩阵(a)等同于数a 矩阵的内积内积 A·B &lt;=&gt; ${A^T}B$矩阵范数矩阵的范数比向量的范数多一条相容性]]></content>
      <categories>
        <category>math</category>
        <category>linear_algebra</category>
      </categories>
      <tags>
        <tag>范数</tag>
        <tag>外积</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2矩阵的运算、行列式]]></title>
    <url>%2F2018%2F04%2F19%2Fmath%2Flinear_algebra%2F2%E7%9F%A9%E9%98%B5%E7%9A%84%E8%BF%90%E7%AE%97%E3%80%81%E8%A1%8C%E5%88%97%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[矩阵的运算1 矩阵乘法的具体应用总结：A中每个元素和B中每个元素相乘是有意义的；B矩阵和最终C矩阵指标数相等，相当于对应指标类元素的求和 几种特殊的矩阵1.对角矩阵2.数量矩阵★3.单矩阵4.三角矩阵5.对称矩阵 分块矩阵和其运算1 简介：2 分块矩阵相加和相乘A+B 和 AB相加：要求每个子块矩阵有相同的行数和列数相乘：要求A的列 = B的行 行列式1 矩阵的行列式和他的转置的行列式相等]]></content>
      <categories>
        <category>math</category>
        <category>linear_algebra</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[数据挖掘——信用卡欺诈检测]]></title>
    <url>%2F2018%2F04%2F19%2Fmachine_learning_in_action%2FCreditCard%2F</url>
    <content type="text"><![CDATA[案例：用 逻辑回归 预测 信用卡欺诈12345import pandas as pdimport matplotlib.pyplot as pltimport numpy as np%matplotlib inline 12data = pd.read_csv("creditcard.csv")data.head() .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } Time V1 V2 V3 V4 V5 V6 V7 V8 V9 ... V21 V22 V23 V24 V25 V26 V27 V28 Amount Class 0 0.0 -1.359807 -0.072781 2.536347 1.378155 -0.338321 0.462388 0.239599 0.098698 0.363787 ... -0.018307 0.277838 -0.110474 0.066928 0.128539 -0.189115 0.133558 -0.021053 149.62 0 1 0.0 1.191857 0.266151 0.166480 0.448154 0.060018 -0.082361 -0.078803 0.085102 -0.255425 ... -0.225775 -0.638672 0.101288 -0.339846 0.167170 0.125895 -0.008983 0.014724 2.69 0 2 1.0 -1.358354 -1.340163 1.773209 0.379780 -0.503198 1.800499 0.791461 0.247676 -1.514654 ... 0.247998 0.771679 0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752 378.66 0 3 1.0 -0.966272 -0.185226 1.792993 -0.863291 -0.010309 1.247203 0.237609 0.377436 -1.387024 ... -0.108300 0.005274 -0.190321 -1.175575 0.647376 -0.221929 0.062723 0.061458 123.50 0 4 2.0 -1.158233 0.877737 1.548718 0.403034 -0.407193 0.095921 0.592941 -0.270533 0.817739 ... -0.009431 0.798278 -0.137458 0.141267 -0.206010 0.502292 0.219422 0.215153 69.99 0 5 rows × 31 columns 1数据预处理——归一化、去掉不用的列123456789count_classes = pd.value_counts(data['Class'], sort = True).sort_index()count_classes.plot(kind = 'bar')plt.title("Fraud class histogram")plt.xlabel("Class")plt.ylabel("Frequency")#发现样本分布十分不均衡#策略：统一不同类别样本总数 #1）oversample——过采样，把少的增多 #2) undersample——欠采样，把多的减少 &lt;matplotlib.text.Text at 0x5d32f27ef0&gt; 123456#用sklearn 函数来进行归一化(自带合并到原dataframe功能)from sklearn.preprocessing import StandardScaler data['normAmount'] = StandardScaler().fit_transform(data['Amount'].values.reshape(-1, 1))data = data.drop(['Time','Amount'],axis=1)data.head() .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } V1 V2 V3 V4 V5 V6 V7 V8 V9 V10 ... V21 V22 V23 V24 V25 V26 V27 V28 Class normAmount 0 -1.359807 -0.072781 2.536347 1.378155 -0.338321 0.462388 0.239599 0.098698 0.363787 0.090794 ... -0.018307 0.277838 -0.110474 0.066928 0.128539 -0.189115 0.133558 -0.021053 0 0.244964 1 1.191857 0.266151 0.166480 0.448154 0.060018 -0.082361 -0.078803 0.085102 -0.255425 -0.166974 ... -0.225775 -0.638672 0.101288 -0.339846 0.167170 0.125895 -0.008983 0.014724 0 -0.342475 2 -1.358354 -1.340163 1.773209 0.379780 -0.503198 1.800499 0.791461 0.247676 -1.514654 0.207643 ... 0.247998 0.771679 0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752 0 1.160686 3 -0.966272 -0.185226 1.792993 -0.863291 -0.010309 1.247203 0.237609 0.377436 -1.387024 -0.054952 ... -0.108300 0.005274 -0.190321 -1.175575 0.647376 -0.221929 0.062723 0.061458 0 0.140534 4 -1.158233 0.877737 1.548718 0.403034 -0.407193 0.095921 0.592941 -0.270533 0.817739 0.753074 ... -0.009431 0.798278 -0.137458 0.141267 -0.206010 0.502292 0.219422 0.215153 0 -0.073403 5 rows × 30 columns 1数据预处理——解决样本分布不均衡问题之undersample1234567891011121314151617181920212223242526272829303132# 方式一：采用“undersample”构建模型#把数据集切分为 样本 和 标记 存变量X = data.loc[:, data.columns != 'Class']y = data.loc[:, data.columns == 'Class']#计算欺诈样本总数number_records_fraud = len(data[data.Class == 1]) #取得欺诈行为的样本indexfraud_indices = np.array(data[data.Class == 1].index) #取得正常的样本indexnormal_indices = data[data.Class == 0].index # 随机选出 和 欺诈类数量相同的 正常Indexrandom_normal_indices = np.random.choice(normal_indices, number_records_fraud, replace = False)random_normal_indices = np.array(random_normal_indices)# 合并取得的两组index，作为欠采样indexunder_sample_indices = np.concatenate([fraud_indices,random_normal_indices])# 取得欠采样datasetunder_sample_data = data.iloc[under_sample_indices,:]#把undersample数据集切分为 样本 和 标记 存变量X_undersample = under_sample_data.loc[:, under_sample_data.columns != 'Class']y_undersample = under_sample_data.loc[:, under_sample_data.columns == 'Class']# 显示处理结果print("Percentage of normal transactions: ", len(under_sample_data[under_sample_data.Class == 0])/len(under_sample_data))print("Percentage of fraud transactions: ", len(under_sample_data[under_sample_data.Class == 1])/len(under_sample_data))print("Total number of transactions in resampled data: ", len(under_sample_data)) Percentage of normal transactions: 0.5 Percentage of fraud transactions: 0.5 Total number of transactions in resampled data: 984 1数据预处理——划分训练和测试集1234567891011121314151617181920#引入数据集切分函数from sklearn.cross_validation import train_test_split# Whole dataset 划分全部数据X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.3, random_state = 0)print("Number transactions train dataset: ", len(X_train))print("Number transactions test dataset: ", len(X_test))print("Total number of transactions: ", len(X_train)+len(X_test))# Undersampled dataset 划分欠采样数据X_train_undersample, X_test_undersample, y_train_undersample, y_test_undersample = train_test_split(X_undersample ,y_undersample ,test_size = 0.3 ,random_state = 0)print("")print("Number transactions train dataset: ", len(X_train_undersample))print("Number transactions test dataset: ", len(X_test_undersample))print("Total number of transactions: ", len(X_train_undersample)+len(X_test_undersample)) Number transactions train dataset: 199364 Number transactions test dataset: 85443 Total number of transactions: 284807 Number transactions train dataset: 688 Number transactions test dataset: 296 Total number of transactions: 984 2 交叉验证——在训练集上做，找最好的逻辑回归正则惩罚系数C1234#Recall = TP/(TP+FN) 这里适用召回率来检测from sklearn.linear_model import LogisticRegressionfrom sklearn.cross_validation import KFold, cross_val_scorefrom sklearn.metrics import confusion_matrix,recall_score,classification_report 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657# 自己实现召回率的 K=5的交叉验证函数（注意：此处是在训练集上的交叉验证）def printing_Kfold_scores(x_train_data,y_train_data): fold = KFold(len(y_train_data),5,shuffle=False) # Different C parameters #在sklearn里面，惩罚系数是倒数，比如100其实是0.01 #每个都试一遍，看哪个模型最好 c_param_range = [0.01,0.1,1,10,100] results_table = pd.DataFrame(index = range(len(c_param_range),2), columns = ['C_parameter','Mean recall score']) results_table['C_parameter'] = c_param_range # the k-fold will give 2 lists: train_indices = indices[0], test_indices = indices[1] j = 0 for c_param in c_param_range: print('-------------------------------------------') print('C parameter: ', c_param) print('-------------------------------------------') print('') recall_accs = [] # iteration：迭代轮数1-5 # indices：[0]表示训练集索引集合，[1]表示测试集索引集合 for iteration, indices in enumerate(fold,start=1): # Call the logistic regression model with a certain C parameter # C：指定惩罚项的参数 # penalty：指定惩罚项的算法 lr = LogisticRegression(C = c_param, penalty = 'l1') # Use the training data to fit the model. In this case, we use the portion of the fold to train the model # with indices[0]. We then predict on the portion assigned as the 'test cross validation' with indices[1] lr.fit(x_train_data.iloc[indices[0],:],y_train_data.iloc[indices[0],:].values.ravel()) # Predict values using the test indices in the training data y_pred_undersample = lr.predict(x_train_data.iloc[indices[1],:].values) # Calculate the recall score and append it to a list for recall scores representing the current c_parameter recall_acc = recall_score(y_train_data.iloc[indices[1],:].values,y_pred_undersample) recall_accs.append(recall_acc) print('Iteration ', iteration,': recall score = ', recall_acc) # The mean value of those recall scores is the metric we want to save and get hold of. results_table.loc[j,'Mean recall score'] = np.mean(recall_accs) j += 1 print('') print('Mean recall score ', np.mean(recall_accs)) print('') best_c = results_table.loc[results_table['Mean recall score'].idxmax()]['C_parameter'] # Finally, we can check which C parameter is the best amongst the chosen. print('*********************************************************************************') print('Best model to choose from cross validation is with C parameter = ', best_c) print('*********************************************************************************') return best_c 1best_c = printing_Kfold_scores(X_train_undersample,y_train_undersample) ------------------------------------------- C parameter: 0.01 ------------------------------------------- Iteration 1 : recall score = 0.931506849315 Iteration 2 : recall score = 0.931506849315 Iteration 3 : recall score = 1.0 Iteration 4 : recall score = 0.972972972973 Iteration 5 : recall score = 0.969696969697 Mean recall score 0.96113672826 ------------------------------------------- C parameter: 0.1 ------------------------------------------- Iteration 1 : recall score = 0.849315068493 Iteration 2 : recall score = 0.86301369863 Iteration 3 : recall score = 0.932203389831 Iteration 4 : recall score = 0.945945945946 Iteration 5 : recall score = 0.893939393939 Mean recall score 0.896883499368 ------------------------------------------- C parameter: 1 ------------------------------------------- Iteration 1 : recall score = 0.86301369863 Iteration 2 : recall score = 0.890410958904 Iteration 3 : recall score = 0.983050847458 Iteration 4 : recall score = 0.945945945946 Iteration 5 : recall score = 0.909090909091 Mean recall score 0.918302472006 ------------------------------------------- C parameter: 10 ------------------------------------------- Iteration 1 : recall score = 0.86301369863 Iteration 2 : recall score = 0.904109589041 Iteration 3 : recall score = 0.983050847458 Iteration 4 : recall score = 0.945945945946 Iteration 5 : recall score = 0.909090909091 Mean recall score 0.921042198033 ------------------------------------------- C parameter: 100 ------------------------------------------- Iteration 1 : recall score = 0.876712328767 Iteration 2 : recall score = 0.890410958904 Iteration 3 : recall score = 0.983050847458 Iteration 4 : recall score = 0.945945945946 Iteration 5 : recall score = 0.909090909091 Mean recall score 0.921042198033 ********************************************************************************* Best model to choose from cross validation is with C parameter = 0.01 ********************************************************************************* 3训练 + 测试——用best_C在训练集上重新训练一遍，再在undersample测试集上预测用 混淆矩阵 计算recall值1234567891011121314151617181920212223import itertoolsdef plot_confusion_matrix(cm, classes, title='Confusion matrix', cmap=plt.cm.Blues): """ This function prints and plots the confusion matrix. """ plt.imshow(cm, interpolation='nearest', cmap=cmap) plt.title(title) plt.colorbar() tick_marks = np.arange(len(classes)) plt.xticks(tick_marks, classes, rotation=0) plt.yticks(tick_marks, classes) thresh = cm.max() / 2. for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])): plt.text(j, i, cm[i, j], horizontalalignment="center", color="white" if cm[i, j] &gt; thresh else "black") plt.tight_layout() plt.ylabel('True label') plt.xlabel('Predicted label') 123456789101112131415161718lr = LogisticRegression(C = best_c, penalty = 'l1')lr.fit(X_train_undersample,y_train_undersample.values.ravel())y_pred_undersample = lr.predict(X_test_undersample.values)# Compute confusion matrixcnf_matrix = confusion_matrix(y_test_undersample,y_pred_undersample)np.set_printoptions(precision=2)print("Recall metric in the testing dataset: ", cnf_matrix[1,1]/(cnf_matrix[1,0]+cnf_matrix[1,1]))# Plot non-normalized confusion matrixclass_names = [0,1]plt.figure()plot_confusion_matrix(cnf_matrix , classes=class_names , title='Confusion matrix')plt.show()#混淆矩阵 显示模型分类效果 Recall metric in the testing dataset: 0.931972789116 3训练 + 测试——用best_C在训练集上重新训练一遍，再在 完整 测试集上预测用 混淆矩阵 计算recall值1234567891011121314151617lr = LogisticRegression(C = best_c, penalty = 'l1')lr.fit(X_train_undersample,y_train_undersample.values.ravel())y_pred = lr.predict(X_test.values)# Compute confusion matrixcnf_matrix = confusion_matrix(y_test,y_pred)np.set_printoptions(precision=2)print("Recall metric in the testing dataset: ", cnf_matrix[1,1]/(cnf_matrix[1,0]+cnf_matrix[1,1]))# Plot non-normalized confusion matrixclass_names = [0,1]plt.figure()plot_confusion_matrix(cnf_matrix , classes=class_names , title='Confusion matrix')plt.show() Recall metric in the testing dataset: 0.918367346939 12这里发现虽然recall值还可以，但是误伤了8581个（被检测成欺诈了），也就是精度accuracy有点低。故要权衡两者，都要较高才行 这里展示的是：不做样本平衡处理，直接把所有样本做交叉验证，发现效果很差1best_c = printing_Kfold_scores(X_train,y_train) ------------------------------------------- C parameter: 0.01 ------------------------------------------- Iteration 1 : recall score = 0.492537313433 Iteration 2 : recall score = 0.602739726027 Iteration 3 : recall score = 0.683333333333 Iteration 4 : recall score = 0.569230769231 Iteration 5 : recall score = 0.45 Mean recall score 0.559568228405 ------------------------------------------- C parameter: 0.1 ------------------------------------------- Iteration 1 : recall score = 0.567164179104 Iteration 2 : recall score = 0.616438356164 Iteration 3 : recall score = 0.683333333333 Iteration 4 : recall score = 0.584615384615 Iteration 5 : recall score = 0.525 Mean recall score 0.595310250644 ------------------------------------------- C parameter: 1 ------------------------------------------- Iteration 1 : recall score = 0.55223880597 Iteration 2 : recall score = 0.616438356164 Iteration 3 : recall score = 0.716666666667 Iteration 4 : recall score = 0.615384615385 Iteration 5 : recall score = 0.5625 Mean recall score 0.612645688837 ------------------------------------------- C parameter: 10 ------------------------------------------- Iteration 1 : recall score = 0.55223880597 Iteration 2 : recall score = 0.616438356164 Iteration 3 : recall score = 0.733333333333 Iteration 4 : recall score = 0.615384615385 Iteration 5 : recall score = 0.575 Mean recall score 0.61847902217 ------------------------------------------- C parameter: 100 ------------------------------------------- Iteration 1 : recall score = 0.55223880597 Iteration 2 : recall score = 0.616438356164 Iteration 3 : recall score = 0.733333333333 Iteration 4 : recall score = 0.615384615385 Iteration 5 : recall score = 0.575 Mean recall score 0.61847902217 ********************************************************************************* Best model to choose from cross validation is with C parameter = 10.0 ********************************************************************************* 1234567891011121314151617lr = LogisticRegression(C = best_c, penalty = 'l1')lr.fit(X_train,y_train.values.ravel())y_pred_undersample = lr.predict(X_test.values)# Compute confusion matrixcnf_matrix = confusion_matrix(y_test,y_pred_undersample)np.set_printoptions(precision=2)print("Recall metric in the testing dataset: ", cnf_matrix[1,1]/(cnf_matrix[1,0]+cnf_matrix[1,1]))# Plot non-normalized confusion matrixclass_names = [0,1]plt.figure()plot_confusion_matrix(cnf_matrix , classes=class_names , title='Confusion matrix')plt.show() Recall metric in the testing dataset: 0.619047619048 4 用predict_proba来测试 最好的逻辑回归 阈值1234567891011121314151617181920212223242526lr = LogisticRegression(C = 0.01, penalty = 'l1')lr.fit(X_train_undersample,y_train_undersample.values.ravel())y_pred_undersample_proba = lr.predict_proba(X_test_undersample.values)thresholds = [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]plt.figure(figsize=(10,10))j = 1for threshold in thresholds: y_test_predictions_high_recall = y_pred_undersample_proba[:,1] &gt; threshold plt.subplot(3,3,j) j += 1 # Compute confusion matrix cnf_matrix = confusion_matrix(y_test_undersample,y_test_predictions_high_recall) np.set_printoptions(precision=2) print("Recall metric in the testing dataset: ", cnf_matrix[1,1]/(cnf_matrix[1,0]+cnf_matrix[1,1])) # Plot non-normalized confusion matrix class_names = [0,1] plot_confusion_matrix(cnf_matrix , classes=class_names , title='Threshold &gt;= %s'%threshold) Recall metric in the testing dataset: 1.0 Recall metric in the testing dataset: 1.0 Recall metric in the testing dataset: 1.0 Recall metric in the testing dataset: 0.993197278912 Recall metric in the testing dataset: 0.931972789116 Recall metric in the testing dataset: 0.884353741497 Recall metric in the testing dataset: 0.843537414966 Recall metric in the testing dataset: 0.748299319728 Recall metric in the testing dataset: 0.578231292517 1数据预处理——解决样本分布不均衡问题之oversample123456import pandas as pd# 安装命令：pip install imblearnfrom imblearn.over_sampling import SMOTEfrom sklearn.ensemble import RandomForestClassifierfrom sklearn.metrics import confusion_matrixfrom sklearn.model_selection import train_test_split 12345678credit_cards=pd.read_csv('creditcard.csv')columns=credit_cards.columns# The labels are in the last column ('Class'). Simply remove it to obtain features columnsfeatures_columns=columns.delete(len(columns)-1)features=credit_cards[features_columns]labels=credit_cards['Class'] 12345#划分数据集features_train, features_test, labels_train, labels_test = train_test_split(features, labels, test_size=0.2, random_state=0) 123#★SMOTE算法通过给定的训练集，生成新的随机扩充训练集oversampler=SMOTE(random_state=0)os_features,os_labels=oversampler.fit_sample(features_train,labels_train) 12345#生成前，label=0print(len(labels_train[labels_train == 0]))#生成以后，label=1的变成和=0的一样多print(len(os_labels[os_labels == 1])) 227454 227454 123os_features = pd.DataFrame(os_features)os_labels = pd.DataFrame(os_labels)best_c = printing_Kfold_scores(os_features,os_labels) ------------------------------------------- C parameter: 0.01 ------------------------------------------- Iteration 1 : recall score = 0.890322580645 Iteration 2 : recall score = 0.894736842105 Iteration 3 : recall score = 0.968617904172 Iteration 4 : recall score = 0.944471922709 Iteration 5 : recall score = 0.958397907255 Mean recall score 0.931309431377 ------------------------------------------- C parameter: 0.1 ------------------------------------------- Iteration 1 : recall score = 0.890322580645 Iteration 2 : recall score = 0.894736842105 Iteration 3 : recall score = 0.970255615802 Iteration 4 : recall score = 0.959991646608 Iteration 5 : recall score = 0.96051922929 Mean recall score 0.93516518289 ------------------------------------------- C parameter: 1 ------------------------------------------- Iteration 1 : recall score = 0.890322580645 Iteration 2 : recall score = 0.894736842105 Iteration 3 : recall score = 0.970211353325 Iteration 4 : recall score = 0.960134533584 Iteration 5 : recall score = 0.960442290148 Mean recall score 0.935169519962 ------------------------------------------- C parameter: 10 ------------------------------------------- Iteration 1 : recall score = 0.890322580645 Iteration 2 : recall score = 0.894736842105 Iteration 3 : recall score = 0.970322009516 Iteration 4 : recall score = 0.95977182049 Iteration 5 : recall score = 0.960783020631 Mean recall score 0.935187254678 ------------------------------------------- C parameter: 100 ------------------------------------------- Iteration 1 : recall score = 0.890322580645 Iteration 2 : recall score = 0.894736842105 Iteration 3 : recall score = 0.969635941131 Iteration 4 : recall score = 0.960255437949 Iteration 5 : recall score = 0.960398324925 Mean recall score 0.935069825351 ********************************************************************************* Best model to choose from cross validation is with C parameter = 10.0 ********************************************************************************* 1234567891011121314151617lr = LogisticRegression(C = best_c, penalty = 'l1')lr.fit(os_features,os_labels.values.ravel())y_pred = lr.predict(features_test.values)# Compute confusion matrixcnf_matrix = confusion_matrix(labels_test,y_pred)np.set_printoptions(precision=2)print("Recall metric in the testing dataset: ", cnf_matrix[1,1]/(cnf_matrix[1,0]+cnf_matrix[1,1]))# Plot non-normalized confusion matrixclass_names = [0,1]plt.figure()plot_confusion_matrix(cnf_matrix , classes=class_names , title='Confusion matrix')plt.show() Recall metric in the testing dataset: 0.910891089109 12 12 12]]></content>
      <categories>
        <category>machine_learning_in_action</category>
      </categories>
      <tags>
        <tag>usersample</tag>
        <tag>oversample</tag>
        <tag>K折交叉验证</tag>
        <tag>混淆矩阵</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo Next优化&踩坑]]></title>
    <url>%2F2018%2F04%2F16%2Fhexo%2FHexo%20Next%E4%BC%98%E5%8C%96%26%E8%B8%A9%E5%9D%91%2F</url>
    <content type="text"><![CDATA[一、界面 篇1 添加动态背景修改配置文件在主题配置文件中找到canvas_nest: false，把它改为canvas_nest: true 修改_layout.swig打开 next/layout/_layout.swig在 &lt; /body&gt;之前添加代码 1234&#123;% if theme.canvas_nest %&#125;&lt;script type=&quot;text/javascript&quot;color=&quot;0,0,0&quot; opacity=&apos;0.5&apos; zIndex=&quot;-2&quot; count=&quot;50&quot; src=&quot;//cdn.bootcss.com/canvas-nest.js/1.0.0/canvas-nest.min.js&quot;&gt;&lt;/script&gt;&#123;% endif %&#125; 配置项说明 color ：线条颜色, 默认: &#39;0,0,0&#39;；三个数字分别为(R,G,B) opacity: 线条透明度（0~1）, 默认: 0.5 count: 线条的总数量, 默认: 150 zIndex: 背景的z-index属性，css属性用于控制所在层的位置, 默认: -1 2 直接展开文章全部目录搜索打开这个文件：sidebar-toc.styl 把下面的内容注释掉： 1234567//取消逐渐展开，改为直接展开所有TOC//.post-toc .nav .nav-child &#123; display: none; &#125;.post-toc .nav .active &gt; .nav-child &#123; display: block; &#125;.post-toc .nav .active-current &gt; .nav-child &#123; display: block; &amp; &gt; .nav-item &#123; display: block; &#125;&#125; 3 添加文章结束标记在 next\layout_macro\post.swig 中wechat-subscriber.swig 上面加入如下代码： 1234&lt;!-- 添加文章结束标记 --&gt;&#123;% if not is_index %&#125; &lt;div style=&quot;text-align:center;color: #000;font-size:14px;&quot;&gt;----------------- The End -----------------&lt;/div&gt;&#123;% endif %&#125; 4 实现主页文章预览效果进入hexo博客项目的themes/next目录用文本编辑器打开_config.yml文件搜索”auto_excerpt”,找到如下部分：12345# Automatically Excerpt. Not recommand.# Please use &lt;!-- more --&gt; in the post to control excerpt accurately.auto_excerpt: enable: true length: 150 把enable值设置为true，就可以控制文章在主页的显示了 5 添加MathJax数学公式支持在主题中开启mathjax开关如何使用了主题了，别忘了在主题（Theme）中开启mathjax开关，下面以next主题为例，介绍下如何打开mathjax开关。 进入到主题目录，找到_config.yml配置问题，把mathjax默认的false修改为true，具体如下： 1234# MathJax Supportmathjax: enable: true per_page: true 别着急，这样还不够，还需要在文章的Front-matter里打开mathjax开关，如下： 123456---title: index.htmldate: 2016-12-28 21:01:30tags:mathjax: true-- 更换Hexo的markdown渲染引擎hexo-renderer-kramed引擎是在默认的渲染引擎hexo-renderer-marked的基础上修改了一些bug，两者比较接近，也比较轻量级。 12npm uninstall hexo-renderer-marked --savenpm install hexo-renderer-kramed --save 执行上面的命令即可，先卸载原来的渲染引擎，再安装新的。 然后，跟换引擎后行间公式可以正确渲染了，但是这样还没有完全解决问题，行内公式的渲染还是有问题，因为hexo-renderer-kramed引擎也有语义冲突的问题。接下来到博客根目录下，找到node_modules\kramed\lib\rules\inline.js，把第11行的escape变量的值做相应的修改： 12// escape: /^\\([\\`*&#123;&#125;\[\]()#$+\-.!_&gt;])/, escape: /^\\([`*\[\]()#+\-.!_&gt;])/ 同时把第20行的em变量也要做相应的修改。 12// em: /^\b_((?:__|[\s\S])+?)_\b|^\*((?:\*\*|[\s\S])+?)\*(?!\*)/, em: /^\*((?:\*\*|[\s\S])+?)\*(?!\*)/ 重新启动hexo（先clean再generate）,问题完美解决。 6 调整页面CSS布局为了加宽文章页面显示，在下面两个文件中添加自定义代码在 themes\next\source\css_custom\custom.styl 中：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657// Custom styles.//边框效果/*// 最上面.content-wrap &#123; padding: 0 40px 40px 40px;&#125;.posts-expand &#123; padding-top: 0;&#125;// 文章.post &#123; margin-top: 60px; margin-bottom: 60px; padding: 25px; -webkit-box-shadow: 1px 1px 1px 1px rgba(202, 203, 203, .5); -moz-box-shadow: 1px 1px 1px 1px rgba(202, 203, 204, .5); &#125;// 右上.sidebar-position-right .header-inner &#123; -webkit-box-shadow: 1px 1px 1px 1px rgba(202, 203, 203, .5); -moz-box-shadow: 1px 1px 1px 1px rgba(202, 203, 204, .5);&#125;// 右下.sidebar .sidebar-inner &#123; -webkit-box-shadow: 1px 1px 1px 1px rgba(202, 203, 203, .5); -moz-box-shadow: 1px 1px 1px 1px rgba(202, 203, 204, .5);&#125;// 右上.sidebar-position-right .header-inner &#123; -webkit-box-shadow: 0 1px 0 0 #262a30; -moz-box-shadow: 0 1px 0 0 #262a30;&#125;*/// 最下面.sidebar-position-right .footer-inner &#123; padding-left: 40px; padding-right: 280px;&#125;// 首页文章添加分割线.posts-expand .post-eof &#123; display: block; margin: 80px auto 60px; width: 61.8%; height: 1px; background: #bbb; text-align: center;&#125;.sidebar-inner &#123; padding: 20px 0 0 0;&#125;.music163 &#123; margin: 20px 0 0 0;&#125; 在 D:\wxy555123.github.io\themes\next\source\css_variables\custom.styl 中：1234567891011121314151617181920// base.styl Layout sizes// --------------------------------------------------//$main-desktop = 960px $main-desktop = 1230px //new 主宽度，也调大防止sidebar遮挡$main-desktop-large = 1200px//$content-desktop = 700px$content-desktop = 990px //new 文章宽度调大$content-desktop-large = 900px$content-desktop-padding = 40px$content-tablet-padding = 10px$content-mobile-padding = 8px$sidebar-desktop = 240px$footer-height = 50px$gap-between-main-and-footer = 100px 7 添加 Gitment 评论系统简介本文介绍hexo next主题(5.1.2)集成giment评论系统的过程。所谓gitment就是把评论放到github的issues系统里，评论支持md，比较适合程序员. 一.注册OAuth Application点击https://github.com/settings/applications/new注册，注意Authorization callback URL填自己的网站urlhttp://yangq.me/.记下Client ID和Client Secret. 二.修改themes/next/_config.yml在其中添加: 123456789# Gitment# Introduction: https://imsun.net/posts/gitment-introduction/gitment: enable: true githubID: yourid repo: yourrepo ClientID: yourid ClientSecret: yoursecret lazy: true123456789 注意:格式要正确，该空格的一定要空格。所有的yourXXX都换成自己的. 三.修改gitment.swig在主题下layout/_third-party/comments/目录下中修改文件gitment.swig使得能够正确初始化： 修改红框标记的id字段，用日期时间戳代替，使得id不会超过50个字符 二、操作 篇1 Hexo命令 安装主题：用git clone到themes文件夹中 生成静态文件：hexo g 启动本地服务器：hexo s 发布到远程网站：hexo d （hexo d -g 生成的后自动发布） ​ 创建文章：hexo new “标题” （默认就在“post”目录里） 创建草稿：hexo new draft “标题” 把草稿转到“post”目录：hexo publish “标题” ​ （注：中间的命令可以用哦个首字母简写） 2 Git命令 清空你的 github.io 仓库项目中所有文件进入到.deploy_git 文件夹下123git rm -rf *git commit -m &apos;clean all file&apos;git push 3 修改Hexo生成文件模版可在根目录 scaffolds 文件夹下修改3类文章模版 4 添加创建文件后，用vscode自动打开脚本在根目录下新建 scripts 文件夹，里面新建 js 文件 名字随意，代码如下： 12345var exec = require("child_process").exec;hexo.on("new", function(data) &#123; exec("Code.exe " + [data.path]);&#125;); 以后每次执行 hexo n 新建文件后都会自动运行 vscode 打开编辑]]></content>
      <categories>
        <category>hexo</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[MathJax数学公式语法]]></title>
    <url>%2F2018%2F04%2F16%2Fhexo%2FMathJax%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F%E8%AF%AD%E6%B3%95%2F</url>
    <content type="text"><![CDATA[概述在Markdown中输入数学公式需要LaTeX语法的支持。 基本语法呈现位置 正文(inline)中的LaTeX公式用\$…$定义 语句为\sum_{i=0}^N\int_{a}^{b}g(t,i)\text{d}t 显示为 $\sum_{i=0}^N\int_{a}^{b}g(t,i)\text{d}t$ 单独显示(display)的LaTeX公式用\$\$…$$定义，此时公式居中并放大显示 语句为\sum_{i=0}^N\int_{a}^{b}g(t,i)\text{d}t 显示为 ​ \sum_{i=0}^N\int_{a}^{b}g(t,i)\text{d}t 下列描述语句中若非特别指出均省略\$…$ 希腊字母 显示 命令 显示 命令 α \alpha β \beta γ \gamma δ \delta ε \epsilon ζ \zeta η \eta θ \theta ι \iota κ \kappa λ \lambda μ \mu ν \nu ξ \xi π \pi ρ \rho σ \sigma τ \tau υ \upsilon φ \phi χ \chi ψ \psi ω \omega - 若需要大写希腊字母，将命令首字母大写即可。 \Gamma呈现为 $\Gamma$- 若需要斜体希腊字母，将命令前加上var前缀即可。 \varGamma呈现为 $\varGamma$ 字母修饰上下标 上标：^ 下标：_ 举例：C_n^2呈现为 $C_n^2$ 矢量 \vec a呈现为 $\vec a$ \overrightarrow{xy}呈现为 $\overrightarrow{xy}$ 字体 Typewriter：\mathtt{A}呈现为 $\mathtt{A}$ $\mathtt{ABCDEFGHIJKLMNOPQRSTUVWXYZABCDEFGHIJKLMNOPQRSTUVWXYZ}$ Blackboard Bold：\mathbb{A}呈现为 $\mathbb{A}$ $\mathbb{ABCDEFGHIJKLMNOPQRSTUVWXYZABCDEFGHIJKLMNOPQRSTUVWXYZ}$ Sans Serif：\mathsf{A}呈现为 $\mathsf{A}$ $\mathsf{ABCDEFGHIJKLMNOPQRSTUVWXYZABCDEFGHIJKLMNOPQRSTUVWXYZ}$ 分组 使用{}将具有相同等级的内容扩入其中，成组处理 举例：10^{10}呈现为 $10^{10}$，而10^10呈现为 $10^10$ 括号 小括号：()呈现为() 中括号：[]呈现为[] 尖括号：\langle,\rangle呈现为⟨⟩ 此处为与分组符号{}相区别，使用转义字符\ 使用\left(或\right)使符号大小与邻近的公式相适应；该语句适用于所有括号类型 (\frac{x}{y})呈现为$(\frac{x}{y})$ 而\left(\frac{x}{y}\right)呈现为$\left(\frac{x}{y}\right)$ 求和、极限与积分 求和：\sum 举例：`\sum_{i=1}^n{a_i}`呈现为$\sum_{i=1}^n{a_i}$ 极限：\lim_{x\to 0}呈现为 $\lim_{x\to 0}$ 积分：\int 举例：`\int_0^\infty{fxdx}`呈现为 $\int_0^\infty{fxdx}$ 分式与根式 分式(fractions)：\frac{公式1}{公式2}呈现为 $\frac{a+b}{a-b}$ 根式：\sqrt[x]{y}呈现为$\sqrt[x]{y}$ 特殊函数 \函数名 举例：\sin x，\ln x，\max(A,B,C)呈现为 $\sin x ,\ln x, \max(A,B,C)$ 特殊符号 显示 命令 ∞ \infty ∪ \cup ∩ \cap ⊂ \subset ⊆ \subseteq ⊃ \supset ∈ \in ∉ \notin ∅ \varnothing ∀ \forall ∃ \exists ¬ \lnot ∇ \nabla ∂ \partial 空格 LaTeX语法本身会忽略空格的存在 小空格：a\ b呈现为 $a\ b$ 4格空格：a\quad b呈现为 $a\quad b$ 矩阵基本语法 起始标记\begin{matrix}``，结束标记``\end{matrix} 每一行末尾标记\\，行间元素之间以&amp;分隔 举例 12345$$\begin&#123;matrix&#125;1&amp;0&amp;0\\0&amp;1&amp;0\\0&amp;0&amp;1\\\end&#123;matrix&#125;$$ 呈现为 ​ \begin{matrix}1&0&0\\0&1&0\\0&0&1\\\end{matrix} 矩阵边框 在起始、结束标记处用下列词替换matrix pmatrix：小括号边框 bmatrix：中括号边框 Bmatrix：大括号边框 vmatrix：单竖线边框 Vmatrix：双竖线边框 省略元素 横省略号：\cdots 竖省略号：\vdots 斜省略号：\ddots 举例 123456$$\begin&#123;bmatrix&#125;&#123;a_&#123;11&#125;&#125;&amp;&#123;a_&#123;12&#125;&#125;&amp;&#123;\cdots&#125;&amp;&#123;a_&#123;1n&#125;&#125;\\&#123;a_&#123;21&#125;&#125;&amp;&#123;a_&#123;22&#125;&#125;&amp;&#123;\cdots&#125;&amp;&#123;a_&#123;2n&#125;&#125;\\&#123;\vdots&#125;&amp;&#123;\vdots&#125;&amp;&#123;\ddots&#125;&amp;&#123;\vdots&#125;\\&#123;a_&#123;m1&#125;&#125;&amp;&#123;a_&#123;m2&#125;&#125;&amp;&#123;\cdots&#125;&amp;&#123;a_&#123;mn&#125;&#125;\\\end&#123;bmatrix&#125;$$ 呈现为 \begin{bmatrix} {a_{11}}&{a_{12}}&{\cdots}&{a_{1n}}\\ {a_{21}}&{a_{22}}&{\cdots}&{a_{2n}}\\ {\vdots}&{\vdots}&{\ddots}&{\vdots}\\ {a_{m1}}&{a_{m2}}&{\cdots}&{a_{mn}}\\ \end{bmatrix}阵列 需要array环境：起始、结束处以{array}声明 对齐方式：在{array}后以{}逐行统一声明 左对齐：l；居中：c；右对齐：r 竖直线：在声明对齐方式时，插入|建立竖直线 插入水平线：\hline 举例 12345$$\begin&#123;array&#125;&#123;c|lll&#125;&#123;↓&#125;&amp;&#123;a&#125;&amp;&#123;b&#125;&amp;&#123;c&#125;\\&#123;R_1&#125;&amp;&#123;c&#125;&amp;&#123;b&#125;&amp;&#123;a&#125;\\&#123;R_2&#125;&amp;&#123;b&#125;&amp;&#123;c&#125;&amp;&#123;c&#125;\\\end&#123;array&#125;$$ 呈现为 \begin{array}{c|lll} {↓}&{a}&{b}&{c}\\ {R_1}&{c}&{b}&{a}\\ {R_2}&{b}&{c}&{c}\\ \end{array}方程组 需要cases环境：起始、结束处以{cases}声明 举例 123456$$\begin&#123;cases&#125;a_1x+b_1y+c_1z=d_1\\a_2x+b_2y+c_2z=d_2\\a_3x+b_3y+c_3z=d_3\\\end&#123;cases&#125;$$ 呈现为 \begin{cases} a_1x+b_1y+c_1z=d_1\\ a_2x+b_2y+c_2z=d_2\\ a_3x+b_3y+c_3z=d_3\\ \end{cases}]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>MathJax</tag>
      </tags>
  </entry>
</search>
